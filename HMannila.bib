@inproceedings{DBLP:conf/sosa/KaskiMM25,
  author       = {Petteri Kaski and
                  Heikki Mannila and
                  Antonis Matakos},
  editor       = {Ioana Oriana Bercea and
                  Rasmus Pagh},
  title        = {A Multilinear Johnson-Lindenstrauss Transform},
  booktitle    = {2025 Symposium on Simplicity in Algorithms, {SOSA} 2025, New Orleans,
                  LA, USA, January 13-15, 2025},
  pages        = {108--118},
  publisher    = {{SIAM}},
  year         = {2025},
  url          = {https://doi.org/10.1137/1.9781611978315.8},
  doi          = {10.1137/1.9781611978315.8},
  timestamp    = {Mon, 10 Mar 2025 16:06:30 +0100},
  biburl       = {https://dblp.org/rec/conf/sosa/KaskiMM25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The Johnson-Lindenstrauss family of transforms constitutes a key algorithmic tool for reducing the dimensionality of a Euclidean space with low distortion of distances. Rephrased from geometry to linear algebra, one seeks to reduce the dimension of a vector space while approximately preserving inner products. We present a multilinear generalization of this bilinear (inner product) setting that admits both an elementary randomized algorithm as well as a short proof of correctness using Orlicz quasinorms.}
}

@article{DBLP:journals/corr/abs-2503-21563,
  author       = {Antonis Matakos and
                  Martino Ciaperoni and
                  Heikki Mannila},
  title        = {Consistent Multigroup Low-Rank Approximation},
  journal      = {CoRR},
  volume       = {abs/2503.21563},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2503.21563},
  doi          = {10.48550/ARXIV.2503.21563},
  eprinttype    = {arXiv},
  eprint       = {2503.21563},
  timestamp    = {Sat, 19 Apr 2025 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2503-21563.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The Min-Max FAIR-PCA problem seeks a low-rank representation of multigroup data such that the approximation error is as balanced as possible across groups. Existing approaches to this problem return a rank-d fair subspace, but lack the fundamental containment property of standard PCA: each rank-d PCA subspace should contain all lower-rank PCA subspaces. To fill this gap, we define fair principal components as directions that minimize the maximum group-wise reconstruction error, subject to orthogonality with previously selected components, and we introduce an iterative method to compute them. This approach preserves the containment property of standard PCA, and reduces to standard PCA for data with a single group. We analyze the theoretical properties of our method and show empirically that it outperforms existing approaches to Min-Max FAIR-PCA.}
}

@article{DBLP:journals/datamine/CiaperoniGM24,
  author       = {Martino Ciaperoni and
                  Aristides Gionis and
                  Heikki Mannila},
  title        = {The Hadamard decomposition problem},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {38},
  number       = {4},
  pages        = {2306--2347},
  year         = {2024},
  url          = {https://doi.org/10.1007/s10618-024-01033-y},
  doi          = {10.1007/S10618-024-01033-Y},
  timestamp    = {Tue, 06 Aug 2024 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/datamine/CiaperoniGM24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We introduce the Hadamard decomposition problem in the context of data analysis. The problem is to represent exactly or approximately a given matrix as the Hadamard (or element-wise) product of two or more low-rank matrices. The motivation for this problem comes from situations where the input matrix has a multiplicative structure. The Hadamard decomposition has potential for giving more succint but equally accurate representations of matrices when compared with the gold-standard of singular value decomposition (SVD). Namely, the Hadamard product of two rank-matrices can have rank as high as h^2. We study the computational properties of the Hadamard decomposition problem and give gradient-based algorithms for solving it approximately. We also introduce a mixed model that combines SVD and Hadamard decomposition. We present extensive empirical results comparing the approximation accuracy of the Hadamard decomposition with that of the SVD using the same number of basis vectors. The results demonstrate that the Hadamard decomposition is competitive with the SVD and, for some datasets, it yields a clearly higher approximation accuracy, indicating the presence of multiplicative structure in the data.}
}

@article{DBLP:journals/lalc/LijffijtNSPPM16,
  author       = {Jefrey Lijffijt and
                  Terttu Nevalainen and
                  Tanja S{\"{a}}ily and
                  Panagiotis Papapetrou and
                  Kai Puolam{\"{a}}ki and
                  Heikki Mannila},
  title        = {Significance testing of word frequencies in corpora},
  journal      = {Digit. Scholarsh. Humanit.},
  volume       = {31},
  number       = {2},
  pages        = {374--397},
  year         = {2016},
  url          = {https://doi.org/10.1093/llc/fqu064},
  doi          = {10.1093/LLC/FQU064},
  timestamp    = {Thu, 05 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/lalc/LijffijtNSPPM16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Finding out whether a word occurs significantly more often in one text or corpus than in another is an important question in analysing corpora. As noted by Kilgarriff (Language is never, ever, ever, random, Corpus Linguistics and Linguistic Theory, 2005; 1(2): 263–76.), the use of the 2 and log-likelihood ratio tests is problematic in this context, as they are based on the assumption that all samples are statistically independent of each other. However, words within a text are not independent. As pointed out in Kilgarriff (Comparing corpora, International Journal of Corpus Linguistics, 2001; 6(1): 1–37) and Paquot and Bestgen (Distinctive words in academic writing: a comparison of three statistical tests for keyword extraction. In Jucker, A., Schreier, D., and Hundt, M. (eds), Corpora: Pragmatics and Discourse. Amsterdam: Rodopi, 2009, pp. 247–69), it is possible to represent the data differently and employ other tests, such that we assume independence at the level of texts rather than individual words. This allows us to account for the distribution of words within a corpus. In this article we compare the significance estimates of various statistical tests in a controlled resampling experiment and in a practical setting, studying differences between texts produced by male and female fiction writers in the British National Corpus. We find that the choice of the test, and hence data representation, matters. We conclude that significance testing can be used to find consequential differences between corpora, but that assuming independence between all words may lead to overestimating the significance of the observed differences, especially for poorly dispersed words. We recommend the use of the t-test, Wilcoxon rank-sum test, or bootstrap test for comparing word frequencies across corpora.}
}

@article{DBLP:journals/bmcbi/KallioVOHM11,
  author       = {Aleksi Kallio and
                  Niko Vuokko and
                  Markus Ojala and
                  Niina Haiminen and
                  Heikki Mannila},
  title        = {Randomization techniques for assessing the significance of gene periodicity results},
  journal      = {{BMC} Bioinform.},
  volume       = {12},
  pages        = {330},
  year         = {2011},
  url          = {https://doi.org/10.1186/1471-2105-12-330},
  doi          = {10.1186/1471-2105-12-330},
  timestamp    = {Fri, 27 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/bmcbi/KallioVOHM11.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Modern high-throughput measurement technologies such as DNA microarrays and next generation sequencers produce extensive datasets. With large datasets the emphasis has been moving from traditional statistical tests to new data mining methods that are capable of detecting complex patterns, such as clusters, regulatory networks, or time series periodicity. Study of periodic gene expression is an interesting research question that also is a good example of challenges involved in the analysis of high-throughput data in general. Unlike for classical statistical tests, the distribution of test statistic for data mining methods cannot be derived analytically. We describe the randomization based approach to significance testing, and show how it can be applied to detect periodically expressed genes. We present four randomization methods, three of which have previously been used for gene cycle data. We propose a new method for testing significance of periodicity in gene expression short time series data, such as from gene cycle and circadian clock studies. We argue that the underlying assumptions behind existing significance testing approaches are problematic and some of them unrealistic. We analyze the theoretical properties of the existing and proposed methods, showing how our method can be robustly used to detect genes with exceptionally high periodicity. We also demonstrate the large differences in the number of significant results depending on the chosen randomization methods and parameters of the testing framework. By reanalyzing gene cycle data from various sources, we show how previous estimates on the number of gene cycle controlled genes are not supported by the data. Our randomization approach combined with widely adopted Benjamini-Hochberg multiple testing method yields better predictive power and produces more accurate null distributions than previous methods. Existing methods for testing significance of periodic gene expression patterns are simplistic and optimistic. Our testing framework allows strict levels of statistical significance with more realistic underlying assumptions, without losing predictive power. As DNA microarrays have now become mainstream and new high-throughput methods are rapidly being adopted, we argue that not only there will be need for data mining methods capable of coping with immense datasets, but there will also be need for solid methods for significance testing.}
}

@article{DBLP:journals/kais/GarrigaJM11,
  author       = {Gemma C. Garriga and
                  Esa Junttila and
                  Heikki Mannila},
  title        = {Banded structure in binary matrices},
  journal      = {Knowl. Inf. Syst.},
  volume       = {28},
  number       = {1},
  pages        = {197--226},
  year         = {2011},
  url          = {https://doi.org/10.1007/s10115-010-0319-7},
  doi          = {10.1007/S10115-010-0319-7},
  timestamp    = {Sun, 28 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/kais/GarrigaJM11.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A binary matrix has a banded structure if both rows and columns can be permuted so that the non-zero entries exhibit a staircase pattern of overlapping rows. The concept of banded matrices has its origins in numerical analysis, where entries can be viewed as descriptions between the problem variables; the bandedness corresponds to variables that are coupled over short distances. Banded data occurs also in other applications, for example in the physical mapping problem of the human genome, in paleontological data, in network data and in the discovery of overlapping communities without cycles. We study the banded structure of binary matrices, give a formal definition of the concept and discuss its theoretical properties. We consider the algorithmic problems of computing how far a matrix is from being banded, and of finding a good submatrix of the original data that exhibits approximate bandedness. Finally, we show by experiments on real data from ecology and other applications the usefulness of the concept. Our results reveal that bands exist in real datasets and that the final obtained orderings of rows and columns have natural interpretations.}
}

@inproceedings{DBLP:conf/pkdd/Mannila11,
  author       = {Heikki Mannila},
  editor       = {Dimitrios Gunopulos and
                  Thomas Hofmann and
                  Donato Malerba and
                  Michalis Vazirgiannis},
  title        = {Permutation Structure in 0-1 Data},
  booktitle    = {Machine Learning and Knowledge Discovery in Databases - European Conference,
                  {ECML} {PKDD} 2011, Athens, Greece, September 5-9, 2011. Proceedings,
                  Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {6911},
  pages        = {7},
  publisher    = {Springer},
  year         = {2011},
  url          = {https://doi.org/10.1007/978-3-642-23780-5\_6},
  doi          = {10.1007/978-3-642-23780-5\_6},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/Mannila11.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Multidimensional 0-1 data occurs in many domains. Typically one assumes that the order of rows and columns has no importance. However, in some applications, e.g., in ecology, there is structure in the data that becomes visible only when the rows and columns are permuted in a certain way. Examples of such structure are different forms of nestedness and bandedness. I review some of the applications, intuitions, results, and open problems in this area.}
}

@inproceedings{DBLP:conf/pkdd/LijffijtPPM11,
  author       = {Jefrey Lijffijt and
                  Panagiotis Papapetrou and
                  Kai Puolam{\"{a}}ki and
                  Heikki Mannila},
  editor       = {Dimitrios Gunopulos and
                  Thomas Hofmann and
                  Donato Malerba and
                  Michalis Vazirgiannis},
  title        = {Analyzing Word Frequencies in Large Text Corpora Using Inter-arrival
                  Times and Bootstrapping},
  booktitle    = {Machine Learning and Knowledge Discovery in Databases - European Conference,
                  {ECML} {PKDD} 2011, Athens, Greece, September 5-9, 2011, Proceedings,
                  Part {II}},
  series       = {Lecture Notes in Computer Science},
  volume       = {6912},
  pages        = {341--357},
  publisher    = {Springer},
  year         = {2011},
  url          = {https://doi.org/10.1007/978-3-642-23783-6\_22},
  doi          = {10.1007/978-3-642-23783-6\_22},
  timestamp    = {Sat, 19 Oct 2019 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/LijffijtPPM11.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Comparing frequency counts over texts or corpora is an important task in many applications and scientific disciplines. Given a text corpus, we want to test a hypothesis, such as “word X is frequent”, “word X has become more frequent over time”, or “word X is more frequent in male than in female speech”. For this purpose we need a null model of word frequencies. The commonly used bag-of-words model, which corresponds to a Bernoulli process with fixed parameter, does not account for any structure present in natural languages. Using this model for word frequencies results in large numbers of words being reported as unexpectedly frequent. We address how to take into account the inherent occurrence patterns of words in significance testing of word frequencies. Based on studies of words in two large corpora, we propose two methods for modeling word frequencies that both take into account the occurrence patterns of words and go beyond the bag-of-words assumption. The first method models word frequencies based on the spatial distribution of individual words in the language. The second method is based on bootstrapping and takes into account only word frequency at the text level. The proposed methods are compared to the current gold standard in a series of experiments on both corpora. We find that words obey different spatial patterns in the language, ranging from bursty to non-bursty/uniform, independent of their frequency, showing that the traditional approach leads to many false positives.}
}

@inproceedings{DBLP:conf/pkdd/PapapetrouGM11,
  author       = {Panagiotis Papapetrou and
                  Aristides Gionis and
                  Heikki Mannila},
  editor       = {Dimitrios Gunopulos and
                  Thomas Hofmann and
                  Donato Malerba and
                  Michalis Vazirgiannis},
  title        = {A Shapley Value Approach for Influence Attribution},
  booktitle    = {Machine Learning and Knowledge Discovery in Databases - European Conference,
                  {ECML} {PKDD} 2011, Athens, Greece, September 5-9, 2011, Proceedings,
                  Part {II}},
  series       = {Lecture Notes in Computer Science},
  volume       = {6912},
  pages        = {549--564},
  publisher    = {Springer},
  year         = {2011},
  url          = {https://doi.org/10.1007/978-3-642-23783-6\_35},
  doi          = {10.1007/978-3-642-23783-6\_35},
  timestamp    = {Tue, 26 Jun 2018 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/PapapetrouGM11.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Finding who and what is “important” is an ever-occurring question. Many methods that aim at characterizing important items or influential individuals have been developed in areas such as, bibliometrics, social-network analysis, link analysis, and web search. In this paper we study the problem of attributing influence scores to individuals who accomplish tasks in a collaborative manner. We assume that individuals build small teams, in different and diverse ways, in order to accomplish atomic tasks. For each task we are given an assessment of success or importance score, and the goal is to attribute those team-wise scores to the individuals. The challenge we face is that individuals in strong coalitions are favored against individuals in weaker coalitions, so the objective is to find fair attributions that account for such biasing. We propose an iterative algorithm for solving this problem that is based on the concept of Shapley value. The proposed method is applicable to a variety of scenarios, for example, attributing influence scores to scientists who collaborate in published articles, or employees of a company who participate in projects. Our method is evaluated on two real datasets: ISI Web of Science publication data and the Internet Movie Database.}
}

@article{DBLP:journals/ijdmb/HaiminenM10,
  author       = {Niina Haiminen and
                  Heikki Mannila},
  title        = {Evaluation of {BIC} and Cross Validation for model selection on sequence
                  segmentations},
  journal      = {Int. J. Data Min. Bioinform.},
  volume       = {4},
  number       = {6},
  pages        = {675--700},
  year         = {2010},
  url          = {https://doi.org/10.1504/IJDMB.2010.037547},
  doi          = {10.1504/IJDMB.2010.037547},
  timestamp    = {Mon, 11 May 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ijdmb/HaiminenM10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Segmentation is a general data mining technique for summarising and analysing sequential data. Segmentation can be applied, e.g., when studying large-scale genomic structures such as isochores. Choosing the number of segments remains a challenging question. We present extensive experimental studies on model selection techniques, Bayesian Information Criterion (BIC) and Cross Validation (CV). We successfully identify segments with different means or variances, and demonstrate the effect of linear trends and outliers, frequently occurring in real data. Results are given for real DNA sequences with respect to changes in their codon, G + C, and bigram frequencies, and copy-number variation from CGH data.}
}

@inproceedings{DBLP:conf/dis/LuostoKM10,
  author       = {Panu Luosto and
                  Jyrki Kivinen and
                  Heikki Mannila},
  editor       = {Bernhard Pfahringer and
                  Geoffrey Holmes and
                  Achim G. Hoffmann},
  title        = {Gaussian Clusters and Noise: An Approach Based on the Minimum Description
                  Length Principle},
  booktitle    = {Discovery Science - 13th International Conference, {DS} 2010, Canberra,
                  Australia, October 6-8, 2010. Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {6332},
  pages        = {251--265},
  publisher    = {Springer},
  year         = {2010},
  url          = {https://doi.org/10.1007/978-3-642-16184-1\_18},
  doi          = {10.1007/978-3-642-16184-1\_18},
  timestamp    = {Thu, 19 Mar 2020 15:28:05 +0100},
  biburl       = {https://dblp.org/rec/conf/dis/LuostoKM10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We introduce a well-grounded minimum description length (MDL) based quality measure for a clustering consisting of either spherical or axis-aligned normally distributed clusters and a cluster with a uniform distribution in an axis-aligned rectangular box. The uniform component extends the practical usability of the model e.g. in the presence of noise, and using the MDL principle for the model selection makes comparing the quality of clusterings with a different number of clusters possible. We also introduce a novel search heuristic for finding the best clustering with an unknown number of clusters. The heuristic is based on the idea of moving points from the Gaussian clusters to the uniform one and using MDL for determining the optimal amount of noise. Tests with synthetic data having a clear cluster structure imply that the search method is effective in finding the intuitively correct clustering.}
}

@inproceedings{DBLP:conf/kdd/LappasTGM10,
  author       = {Theodoros Lappas and
                  Evimaria Terzi and
                  Dimitrios Gunopulos and
                  Heikki Mannila},
  editor       = {Bharat Rao and
                  Balaji Krishnapuram and
                  Andrew Tomkins and
                  Qiang Yang},
  title        = {Finding effectors in social networks},
  booktitle    = {Proceedings of the 16th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Washington, DC, USA, July 25-28,
                  2010},
  pages        = {1059--1068},
  publisher    = {{ACM}},
  year         = {2010},
  url          = {https://doi.org/10.1145/1835804.1835937},
  doi          = {10.1145/1835804.1835937},
  timestamp    = {Sat, 09 Apr 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/LappasTGM10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Assume a network (V,E) where a subset of the nodes in V are active. We consider the problem of selecting a set of k active nodes that best explain the observed activation state, under a given information-propagation model. We call these nodes effectors. We formally define the k-Effectors problem and study its complexity for different types of graphs. We show that for arbitrary graphs the problem is not only NP-hard to solve optimally, but also NP-hard to approximate. We also show that, for some special cases, the problem can be solved optimally in polynomial time using a dynamic-programming algorithm. To the best of our knowledge, this is the first work to consider the k-Effectors problem in networks. We experimentally evaluate our algorithms using the DBLP co-authorship graph, where we search for effectors of topics that appear in research papers.}
}

@inproceedings{DBLP:conf/sdm/OjalaGGM10,
  author       = {Markus Ojala and
                  Gemma C. Garriga and
                  Aristides Gionis and
                  Heikki Mannila},
  title        = {Evaluating Query Result Significance in Databases via Randomizations},
  booktitle    = {Proceedings of the {SIAM} International Conference on Data Mining,
                  {SDM} 2010, April 29 - May 1, 2010, Columbus, Ohio, {USA}},
  pages        = {906--917},
  publisher    = {{SIAM}},
  year         = {2010},
  url          = {https://doi.org/10.1137/1.9781611972801.79},
  doi          = {10.1137/1.9781611972801.79},
  timestamp    = {Wed, 17 May 2017 14:24:53 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/OjalaGGM10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Many sorts of structured data are commonly stored in a multi-relational format of interrelated tables. Under this relational model, exploratory data analysis can be done by using relational queries. As an example, in the Internet Movie Database (IMDb) a query can be used to check whether the average rank of action movies is higher than the average rank of drama movies. We consider the problem of assessing whether the results returned by such a query are statistically significant or just a random artifact of the structure in the data. Our approach is based on randomizing the tables occurring in the queries and repeating the original query on the randomized tables. It turns out that there is no unique way of randomizing in multi-relational data. We propose several randomization techniques, study their properties, and show how to find out which queries or hypotheses about our data result in statistically significant information and which tables in the database convey most of the structure in the query. We give results on real and generated data and show how the significance of some queries vary between different randomizations.}
}

@incollection{DBLP:books/daglib/p/RaedtJLM10,
  author       = {Luc De Raedt and
                  Manfred Jaeger and
                  Sau Dan Lee and
                  Heikki Mannila},
  editor       = {Saso Dzeroski and
                  Bart Goethals and
                  Pance Panov},
  title        = {A Theory of Inductive Query Answering},
  booktitle    = {Inductive Databases and Constraint-Based Data Mining},
  pages        = {79--103},
  publisher    = {Springer},
  year         = {2010},
  url          = {https://doi.org/10.1007/978-1-4419-7738-0\_4},
  doi          = {10.1007/978-1-4419-7738-0\_4},
  timestamp    = {Sat, 09 Apr 2022 12:20:03 +0200},
  biburl       = {https://dblp.org/rec/books/daglib/p/RaedtJLM10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We introduce the Boolean inductive query evaluation problem, which is concerned with answering inductive queries that are arbitrary Boolean expressions over monotonic and anti-monotonic predicates. Boolean inductive queries can be used to address many problems in data mining and machine learning, such as local pattern mining and concept-learning, and actually provides a unifying view on many machine learning and data mining tasks. Secondly, we develop a decomposition theory for inductive query evaluation in which a Boolean query Q is reformulated into k sub-queries  that are the conjunction of a monotonic and an anti-monotonic predicate. The solution to each sub-query can be represented using a version space.We investigate how the number of version spaces k needed to answer the query can be minimized and define this as the dimension of the solution space and query. Thirdly, we generalize the notion of version spaces to cover Boolean queries, so that the solution sets form a closed Boolean-algebraic space under the usual set operations. The effects of these set operations on the dimension of the involved queries are studied.}
}

@article{DBLP:journals/csda/BinghamM09,
  author       = {Ella Bingham and
                  Heikki Mannila},
  title        = {Complexity control in a mixture model by the Hardy-Weinberg equilibrium},
  journal      = {Comput. Stat. Data Anal.},
  volume       = {53},
  number       = {5},
  pages        = {1711--1719},
  year         = {2009},
  url          = {https://doi.org/10.1016/j.csda.2008.07.023},
  doi          = {10.1016/J.CSDA.2008.07.023},
  timestamp    = {Tue, 18 Feb 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/csda/BinghamM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A method of complexity control in multinomial mixture modeling of multiple-marker genotype data, imposing the Hardy–Weinberg equilibrium (HWE) between the genotype values, is studied. This is a very natural restriction, and known to hold at population level under modest assumptions. The hypothesis under study is that imposing this restriction will prevent overfitting and lead to a better model. This is shown to indeed be case. Experimental results on chromosomes 1 and 17 of the HapMap data demonstrate that the restricted model generalizes better to unseen data, and also finds clusters that correspond better to the ethnic groups of the HapMap, when compared with a model without the HWE restriction.}
}

@article{DBLP:journals/ipl/UkkonenPGM09,
  author       = {Antti Ukkonen and
                  Kai Puolam{\"{a}}ki and
                  Aristides Gionis and
                  Heikki Mannila},
  title        = {A randomized approximation algorithm for computing bucket orders},
  journal      = {Inf. Process. Lett.},
  volume       = {109},
  number       = {7},
  pages        = {356--359},
  year         = {2009},
  url          = {https://doi.org/10.1016/j.ipl.2008.12.003},
  doi          = {10.1016/J.IPL.2008.12.003},
  timestamp    = {Sat, 19 Oct 2019 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ipl/UkkonenPGM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We show that a simple randomized algorithm has an expected constant factor approximation guarantee for fitting bucket orders to a set of pairwise preferences.}
}

@article{DBLP:journals/ipl/FederMT09,
  author       = {Tom{\'{a}}s Feder and
                  Heikki Mannila and
                  Evimaria Terzi},
  title        = {Approximating the Minimum Chain Completion problem},
  journal      = {Inf. Process. Lett.},
  volume       = {109},
  number       = {17},
  pages        = {980--985},
  year         = {2009},
  url          = {https://doi.org/10.1016/j.ipl.2009.05.006},
  doi          = {10.1016/J.IPL.2009.05.006},
  timestamp    = {Fri, 26 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ipl/FederMT09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A bipartite graph G is a chain graph [M. Yannakakis, Computing the minimum fill-in is NP-complete, SIAM J. Algebraic Discrete Methods 2 (1) (1981) 77–79] if there is a bijection such that , where Γ is a function that maps a node to its neighbors. We give approximation algorithms for two variants of the Minimum Chain Completion problem, where we are given a bipartite graph , and the goal is find the minimum set of edges F that need to be added to G such that the bipartite graph  () is a chain graph.}
}

@article{DBLP:journals/sadm/OjalaVKHM09,
  author       = {Markus Ojala and
                  Niko Vuokko and
                  Aleksi Kallio and
                  Niina Haiminen and
                  Heikki Mannila},
  title        = {Randomization methods for assessing data analysis results on real-valued matrices},
  journal      = {Stat. Anal. Data Min.},
  volume       = {2},
  number       = {4},
  pages        = {209--230},
  year         = {2009},
  url          = {https://doi.org/10.1002/sam.10042},
  doi          = {10.1002/SAM.10042},
  timestamp    = {Thu, 01 Oct 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/sadm/OjalaVKHM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Randomization is an important technique for assessing the significance of data analysis results. Given an input dataset, a randomization method samples at random from some class of datasets that share certain characteristics with the original data. The measure of interest on the original data is then compared to the measure on the samples to assess its significance. For certain types of data, e.g., gene expression matrices, it is useful to be able to sample datasets that have the same row and column distributions of values as the original dataset. Testing whether the results of a data mining algorithm on such randomized datasets differ from the results on the true dataset tells us whether the results on the true data were an artifact of the row and column statistics, or due to some more interesting phenomena in the data. We study the problem of generating such randomized datasets. We describe methods based on local transformations and Metropolis sampling, and show that the methods are efficient and usable in practice. We evaluate the performance of the methods both on real and generated data. We also show how our methods can be applied to a real data analysis scenario on DNA microarray data. The results indicate that the methods work efficiently and are usable in significance testing of data mining results on real-valued matrices.}
}

@article{DBLP:journals/tkde/MiahDHM09,
  author       = {Muhammed Miah and
                  Gautam Das and
                  Vagelis Hristidis and
                  Heikki Mannila},
  title        = {Determining Attributes to Maximize Visibility of Objects},
  journal      = {{IEEE} Trans. Knowl. Data Eng.},
  volume       = {21},
  number       = {7},
  pages        = {959--973},
  year         = {2009},
  url          = {https://doi.org/10.1109/TKDE.2009.72},
  doi          = {10.1109/TKDE.2009.72},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/tkde/MiahDHM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In recent years, there has been significant interest in the development of ranking functions and efficient top-k retrieval algorithms to help users in ad hoc search and retrieval in databases (e.g., buyers searching for products in a catalog). We introduce a complementary problem: How to guide a seller in selecting the best attributes of a new tuple (e.g., a new product) to highlight so that it stands out in the crowd of existing competitive products and is widely visible to the pool of potential buyers. We develop several formulations of this problem. Although the problems are NP-complete, we give several exact and approximation algorithms that work well in practice. One type of exact algorithms is based on integer programming (IP) formulations of the problems. Another class of exact methods is based on maximal frequent item set mining algorithms. The approximation algorithms are based on greedy heuristics. A detailed performance study illustrates the benefits of our methods on real and synthetic data.}
}

@inproceedings{DBLP:conf/ismis/Mannila09,
  author       = {Heikki Mannila},
  editor       = {Jan Rauch and
                  Zbigniew W. Ras and
                  Petr Berka and
                  Tapio Elomaa},
  title        = {Randomization Methods for Assessing the Significance of Data Mining Results},
  booktitle    = {Foundations of Intelligent Systems, 18th International Symposium,
                  {ISMIS} 2009, Prague, Czech Republic, September 14-17, 2009. Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {5722},
  pages        = {1},
  publisher    = {Springer},
  year         = {2009},
  url          = {https://doi.org/10.1007/978-3-642-04125-9\_1},
  doi          = {10.1007/978-3-642-04125-9\_1},
  timestamp    = {Tue, 14 May 2019 10:00:41 +0200},
  biburl       = {https://dblp.org/rec/conf/ismis/Mannila09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining research has developed many algorithms for various analysis tasks on large and complex datasets. However, assessing the significance of data mining results has received less attention. Analytical methods are rarely available, and hence one has to use computationally intensive methods. Randomization approaches based on null models provide, at least in principle, a general approach that can be used to obtain empirical p-values for various types of data mining approaches. I review some of the recent work in this area, outlining some of the open questions and problems.}
}

@inproceedings{DBLP:conf/kdd/Mannila09,
  author       = {Heikki Mannila},
  editor       = {John F. Elder IV and
                  Fran{\c{c}}oise Fogelman{-}Souli{\'{e}} and
                  Peter A. Flach and
                  Mohammed Javeed Zaki},
  title        = {Randomization methods in data mining},
  booktitle    = {Proceedings of the 15th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Paris, France, June 28 - July
                  1, 2009},
  pages        = {5--6},
  publisher    = {{ACM}},
  year         = {2009},
  url          = {https://doi.org/10.1145/1557019.1557023},
  doi          = {10.1145/1557019.1557023},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/Mannila09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining research has developed many algorithms for various analysis tasks on large and
  complex datasets. However, assessing the significance of data mining results has received less
  attention. Analytical methods are rarely available, and hence one has to use computationally
  intensive methods. Randomization approaches based on null models provide, at least in principle, a
  general approach that can be used to obtain empirical p-values for various types of data mining
  approaches. I review some of the recent work in this area, outlining some of the open questions
  and problems.}
}

@inproceedings{DBLP:conf/kdd/HanhijarviOVPTM09,
  author       = {Sami Hanhij{\"{a}}rvi and
                  Markus Ojala and
                  Niko Vuokko and
                  Kai Puolam{\"{a}}ki and
                  Nikolaj Tatti and
                  Heikki Mannila},
  editor       = {John F. Elder IV and
                  Fran{\c{c}}oise Fogelman{-}Souli{\'{e}} and
                  Peter A. Flach and
                  Mohammed Javeed Zaki},
  title        = {Tell me something {I} don't know: randomization strategies for iterative data mining},
  booktitle    = {Proceedings of the 15th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Paris, France, June 28 - July
                  1, 2009},
  pages        = {379--388},
  publisher    = {{ACM}},
  year         = {2009},
  url          = {https://doi.org/10.1145/1557019.1557065},
  doi          = {10.1145/1557019.1557065},
  timestamp    = {Sat, 19 Oct 2019 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/HanhijarviOVPTM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {There is a wide variety of data mining methods available, and it is generally useful in exploratory data analysis to use
  many diﬀerent methods for the same dataset. This, however, leads to the problem of whether the results found by
  one method are a reflection of the phenomenon shown by the results of another method, or whether the results de-
  pict in some sense unrelated properties of the data. For example, using clustering can give indication of a clear clus-
  ter structure, and computing correlations between variables can show that there are many significant correlations in the
  data. However, it can be the case that the correlations are actually determined by the cluster structure.
  In this paper, we consider the problem of randomizing data so that previously discovered patterns or models are
  taken into account. The randomization methods can be used in iterative data mining. At each step in the data mining
  process, the randomization produces random samples from the set of data matrices satisfying the already discovered
  patterns or models. That is, given a data set and some statistics (e.g., cluster centers or co-occurrence counts) of the
  data, the randomization methods sample data sets having similar values of the given statistics as the original data set.
  We use Metropolis sampling based on local swaps to achieve this. We describe experiments on real data that demonstrate
  the usefulness of our approach. Our results indicate that in many cases, the results of, e.g., clustering actually imply the
  results of, say, frequent pattern discovery.}
}

@inproceedings{DBLP:conf/pkdd/HakkoymazCGM09,
  author       = {Huseyin Hakkoymaz and
                  Georgios Chatzimilioudis and
                  Dimitrios Gunopulos and
                  Heikki Mannila},
  editor       = {Wray L. Buntine and
                  Marko Grobelnik and
                  Dunja Mladenic and
                  John Shawe{-}Taylor},
  title        = {Applying Electromagnetic Field Theory Concepts to Clustering with Constraints},
  booktitle    = {Machine Learning and Knowledge Discovery in Databases, European Conference,
                  {ECML} {PKDD} 2009, Bled, Slovenia, September 7-11, 2009, Proceedings,
                  Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {5781},
  pages        = {485--500},
  publisher    = {Springer},
  year         = {2009},
  url          = {https://doi.org/10.1007/978-3-642-04180-8\_49},
  doi          = {10.1007/978-3-642-04180-8\_49},
  timestamp    = {Thu, 14 Oct 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/HakkoymazCGM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {This work shows how concepts from the electromagnetic field theory can be efficiently used in clustering with constraints. The proposed framework transforms vector data into a fully connected graph, or just works straight on the given graph data. User constraints are represented by electromagnetic fields that affect the weight of the graph’s edges. A clustering algorithm is then applied on the adjusted graph, using k-distinct shortest paths as the distance measure. Our framework provides better accuracy compared to MPCK-Means, SS-Kernel-KMeans and Kmeans+Diagonal Metric even when very few constraints are used, significantly improves clustering performance on some datasets that other methods fail to partition successfully, and can cluster both vector and graph datasets. All these advantages are demonstrated through thorough experimental evaluation.}
}

@inproceedings{DBLP:conf/sdm/HeikinheimoVSM09,
  author       = {Hannes Heikinheimo and
                  Jilles Vreeken and
                  Arno Siebes and
                  Heikki Mannila},
  title        = {Low-Entropy Set Selection},
  booktitle    = {Proceedings of the {SIAM} International Conference on Data Mining,
                  {SDM} 2009, April 30 - May 2, 2009, Sparks, Nevada, {USA}},
  pages        = {569--580},
  publisher    = {{SIAM}},
  year         = {2009},
  url          = {https://doi.org/10.1137/1.9781611972795.49},
  doi          = {10.1137/1.9781611972795.49},
  timestamp    = {Wed, 17 May 2017 14:24:53 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/HeikinheimoVSM09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Most pattern discovery algorithms easily generate very large
  numbers of patterns, making the results impossible to un-
  derstand and hard to use. Recently, the problem of instead
  selecting a small subset of informative patterns from a large
  collection of patterns has attracted a lot of interest. In this
  paper we present a succinct way of representing data on the
  basis of itemsets that identify strong interactions.
  This new approach, LESS, provides a more powerful
  and more general technique to data description than exist-
  ing approaches. Low-entropy sets consider the data sym-
  metrically and as such identify strong interactions between
  attributes, not just between items that are present. Selec-
  tion of these patterns is executed through the MDL-criterion.
  This results in only a handful of sets that together form a
  compact lossless description of the data.
  By using entropy-based elements for the data descrip-
  tion, we can successfully apply the maximum likelihood
  principle to locally cover the data optimally. Further, it al-
  lows for a fast, natural and well performing heuristic. Based
  on these approaches we present two algorithms that provide
  high-quality descriptions of the data in terms of strongly in-
  teracting variables.
  Experiments on these methods show that high-quality
  results are mined: very small pattern sets are returned that
  are easily interpretable and understandable descriptions of
  the data, and can be straightforwardly visualized. Swap
  randomization experiments and high compression ratios
  show that they capture the structure of the data well.}
}

@inproceedings{DBLP:conf/sdm/Terzi09,
  author       = {Heikki Mannila and
                  Evimaria Terzi},
  title        = {Finding Links and Initiators: {A} Graph-Reconstruction Problem},
  booktitle    = {Proceedings of the {SIAM} International Conference on Data Mining,
                  {SDM} 2009, April 30 - May 2, 2009, Sparks, Nevada, {USA}},
  pages        = {1209--1219},
  publisher    = {{SIAM}},
  year         = {2009},
  url          = {https://doi.org/10.1137/1.9781611972795.103},
  doi          = {10.1137/1.9781611972795.103},
  timestamp    = {Wed, 17 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/Terzi09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Consider a 0–1 observation matrix M, where rows correspond to entities and columns correspond to signals; a value of 1 (or 0) in cell (i, j) of M indicates that signal j has been observed (or not observed) in entity i. Given such a matrix we study the problem of inferring the underlying directed links between entities (rows) and finding which entries in the matrix are initiators.
  We formally define this problem and propose an MCMC framework for estimating the links and the initiators given the matrix of observations M. We also show how this framework can be extended to incorporate a temporal aspect; instead of considering a single observation matrix M we consider a sequence of observation matrices M1, …, Mt over time.
  We show the connection between our problem and several problems studied in the field of social-network analysis. We apply our method to paleontological and ecological data and show that our algorithms work well in practice and give reasonable results.}
}

@article{DBLP:journals/bmcbi/HaiminenMT08,
  author       = {Niina Haiminen and
                  Heikki Mannila and
                  Evimaria Terzi},
  title        = {Determining significance of pairwise co-occurrences of events in bursty sequences},
  journal      = {{BMC} Bioinform.},
  volume       = {9},
  year         = {2008},
  url          = {https://doi.org/10.1186/1471-2105-9-336},
  doi          = {10.1186/1471-2105-9-336},
  timestamp    = {Fri, 27 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/bmcbi/HaiminenMT08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Event sequences where different types of events often occur close together arise, e.g., when studying potential transcription factor binding sites (TFBS, events) of certain transcription factors (TF, types) in a DNA sequence. These events tend to occur in bursts: in some genomic regions there are more genes and therefore potentially more binding sites, while in some, possibly very long regions, hardly any events occur. Also some types of events may occur in the sequence more often than others.
  Tendencies of co-occurrence of binding sites of two or more TFs are interesting, as they may imply a co-operative role between the TFs in regulatory processes. Determining a numerical value to summarize the tendency for co-occurrence between two TFs can be done in a number of ways. However, testing for the significance of such values should be done with respect to a relevant null model that takes into account the global sequence structure.
  We extend the existing techniques that have been considered for determining the significance of co-occurrence patterns between a pair of event types under different null models. These models range from very simple ones to more complex models that take the burstiness of sequences into account. We evaluate the models and techniques on synthetic event sequences, and on real data consisting of potential transcription factor binding sites.
  We show that simple null models are poorly suited for bursty data, and they yield many false positives. More sophisticated models give better results in our experiments. We also demonstrate the effect of the window size, i.e., maximum co-occurrence distance, on the significance results.}
}

@article{DBLP:journals/kais/GwaderaGM08,
  author       = {Robert Gwadera and
                  Aristides Gionis and
                  Heikki Mannila},
  title        = {Optimal segmentation using tree models},
  journal      = {Knowl. Inf. Syst.},
  volume       = {15},
  number       = {3},
  pages        = {259--283},
  year         = {2008},
  url          = {https://doi.org/10.1007/s10115-007-0091-5},
  doi          = {10.1007/S10115-007-0091-5},
  timestamp    = {Tue, 01 Jun 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/kais/GwaderaGM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequence data are abundant in application areas such as computational biology, environmental sciences, and telecommunications. Many real-life sequences have a strong segmental structure, with segments of different complexities. In this paper we study the description of sequence segments using variable length Markov chains (VLMCs), also known as tree models. We discover the segment boundaries of a sequence and at the same time we compute a VLMC for each segment. We use the Bayesian information criterion (BIC) and a variant of the minimum description length (MDL) principle that uses the Krichevsky-Trofimov (KT) code length to select the number of segments of a sequence. On DNA data the method selects segments that closely correspond to the annotated regions of the genes.}
}

@article{DBLP:journals/tkde/MiettinenMGDM08,
  author       = {Pauli Miettinen and
                  Taneli Mielik{\"{a}}inen and
                  Aristides Gionis and
                  Gautam Das and
                  Heikki Mannila},
  title        = {The Discrete Basis Problem},
  journal      = {{IEEE} Trans. Knowl. Data Eng.},
  volume       = {20},
  number       = {10},
  pages        = {1348--1362},
  year         = {2008},
  url          = {https://doi.org/10.1109/TKDE.2008.53},
  doi          = {10.1109/TKDE.2008.53},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/tkde/MiettinenMGDM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Matrix decomposition methods represent a data matrix as a product of two factor matrices: one containing basis vectors that represent meaningful concepts in the data, and another describing how the observed data can be expressed as combinations of the basis vectors. Decomposition methods have been studied extensively, but many methods return real-valued matrices. Interpreting real-valued factor matrices is hard if the original data is Boolean. In this paper, we describe a matrix decomposition formulation for Boolean data, the Discrete Basis Problem. The problem seeks for a Boolean decomposition of a binary matrix, thus allowing the user to easily interpret the basis vectors. We also describe a variation of the problem, the Discrete Basis Partitioning Problem. We show that both problems are NP-hard. For the Discrete Basis Problem, we give a simple greedy algorithm for solving it; for the Discrete Basis Partitioning Problem we show how it can be solved using existing methods. We present experimental results for the greedy algorithm and compare it against other, well known methods. Our algorithm gives intuitive basis vectors, but its reconstruction error is usually larger than with the real-valued methods. We discuss about the reasons for this behavior.}
}

@inproceedings{DBLP:conf/adbis/Mannila08,
  author       = {Heikki Mannila},
  editor       = {Paolo Atzeni and
                  Albertas Caplinskas and
                  Hannu Jaakkola},
  title        = {Randomization Techniques for Data Mining Methods},
  booktitle    = {Advances in Databases and Information Systems, 12th East European
                  Conference, {ADBIS} 2008, Pori, Finland, September 5-9, 2008. Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {5207},
  pages        = {1},
  publisher    = {Springer},
  year         = {2008},
  url          = {https://doi.org/10.1007/978-3-540-85713-6\_1},
  doi          = {10.1007/978-3-540-85713-6\_1},
  timestamp    = {Tue, 14 May 2019 10:00:53 +0200},
  biburl       = {https://dblp.org/rec/conf/adbis/Mannila08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining research has concentrated on inventing novel methods for finding interesting information from large masses of data. This has indeed led to many new computational tasks and some interesting algorithmic developments. However, there has been less emphasis on issues of significance testing of the discovered patterns or models. We discuss the issues in testing the results of data mining methods, and review some of the recent work in the development of scalable algorithmic techniques for randomization tests for data mining methods. We consider suitable null models and generation algorithms for randomization of 0-1 -matrices, arbitrary real valued matrices, and segmentations. We also discuss randomization for database queries.}
}

@inproceedings{DBLP:conf/dis/Mannila08,
  author       = {Heikki Mannila},
  editor       = {Jean{-}Fran{\c{c}}ois Boulicaut and
                  Michael R. Berthold and
                  Tam{\'{a}}s Horv{\'{a}}th},
  title        = {Finding Total and Partial Orders from Data for Seriation},
  booktitle    = {Discovery Science, 11th International Conference, {DS} 2008, Budapest,
                  Hungary, October 13-16, 2008. Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {5255},
  pages        = {16--25},
  publisher    = {Springer},
  year         = {2008},
  url          = {https://doi.org/10.1007/978-3-540-88411-8\_4},
  doi          = {10.1007/978-3-540-88411-8\_4},
  timestamp    = {Tue, 14 May 2019 10:00:46 +0200},
  biburl       = {https://dblp.org/rec/conf/dis/Mannila08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Ordering and ranking items of different types (observations, web pages, etc.) are important tasks in various applications, such as query processing and scientific data mining. We consider different problems of inferring total or partial orders from data, with special emphasis on applications to the seriation problem in paleontology. Seriation can be viewed as the task of ordering rows of a 0-1 matrix so that certain conditions hold. We review different approaches to this task, including spectral ordering methods, techniques for finding partial orders, and probabilistic models using MCMC methods.}
}

@inproceedings{DBLP:conf/dis/GarrigaUM08,
  author       = {Gemma C. Garriga and
                  Antti Ukkonen and
                  Heikki Mannila},
  editor       = {Jean{-}Fran{\c{c}}ois Boulicaut and
                  Michael R. Berthold and
                  Tam{\'{a}}s Horv{\'{a}}th},
  title        = {Feature Selection in Taxonomies with Applications to Paleontology},
  booktitle    = {Discovery Science, 11th International Conference, {DS} 2008, Budapest,
                  Hungary, October 13-16, 2008. Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {5255},
  pages        = {112--123},
  publisher    = {Springer},
  year         = {2008},
  url          = {https://doi.org/10.1007/978-3-540-88411-8\_13},
  doi          = {10.1007/978-3-540-88411-8\_13},
  timestamp    = {Tue, 23 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/dis/GarrigaUM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Taxonomies for a set of features occur in many real-world domains. An example is provided by paleontology, where the task is to determine the age of a fossil site on the basis of the taxa that have been found in it. As the fossil record is very noisy and there are lots of gaps in it, the challenge is to consider taxa at a suitable level of aggregation: species, genus, family, etc. For example, some species can be very suitable as features for the age prediction task, while for other parts of the taxonomy it would be better to use genus level or even higher levels of the hierarchy. A default choice is to select a fixed level (typically species or genus); this misses the potential gain of choosing the proper level for sets of species separately. Motivated by this application we study the problem of selecting an antichain from a taxonomy that covers all leaves and helps to predict better a specified target variable. Our experiments on paleontological data show that choosing antichains leads to better predictions than fixing specific levels of the taxonomy beforehand.}
}

@inproceedings{DBLP:conf/icde/MiahDHM08,
  author       = {Muhammed Miah and
                  Gautam Das and
                  Vagelis Hristidis and
                  Heikki Mannila},
  editor       = {Gustavo Alonso and
                  Jos{\'{e}} A. Blakeley and
                  Arbee L. P. Chen},
  title        = {Standing Out in a Crowd: Selecting Attributes for Maximum Visibility},
  booktitle    = {Proceedings of the 24th International Conference on Data Engineering,
                  {ICDE} 2008, April 7-12, 2008, Canc{\'{u}}n, Mexico},
  pages        = {356--365},
  publisher    = {{IEEE} Computer Society},
  year         = {2008},
  url          = {https://doi.org/10.1109/ICDE.2008.4497444},
  doi          = {10.1109/ICDE.2008.4497444},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icde/MiahDHM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In recent years, there has been significant interest in development of ranking functions and efficient top-k retrieval algorithms to help users in ad-hoc search and retrieval in databases (e.g., buyers searching for products in a catalog). In this paper we focus on a novel and complementary problem: how to guide a seller in selecting the best attributes of a new tuple (e.g., new product) to highlight such that it stands out in the crowd of existing competitive products and is widely visible to the pool of potential buyers. We develop several interesting formulations of this problem. Although these problems are NP-complete, we can give several exact algorithms as well as approximation heuristics that work well in practice. Our exact algorithms are based on integer programming (IP) formulations of the problems, as well as on adaptations of maximal frequent itemset mining algorithms, while our approximation algorithms are based on greedy heuristics. We conduct a performance study illustrating the benefits of our methods on real as well as synthetic data.}
}

@inproceedings{DBLP:conf/kdd/GarrigaJM08,
  author       = {Gemma C. Garriga and
                  Esa Junttila and
                  Heikki Mannila},
  editor       = {Ying Li and
                  Bing Liu and
                  Sunita Sarawagi},
  title        = {Banded structure in binary matrices},
  booktitle    = {Proceedings of the 14th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August
                  24-27, 2008},
  pages        = {292--300},
  publisher    = {{ACM}},
  year         = {2008},
  url          = {https://doi.org/10.1145/1401890.1401929},
  doi          = {10.1145/1401890.1401929},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/GarrigaJM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A 0--1 matrix has a banded structure if both rows and columns can be permuted so that the non-zero entries exhibit a staircase pattern of overlapping rows. The concept of banded matrices has its origins in numerical analysis, where entries can be viewed as descriptions between the problem variables; the bandedness corresponds to variables that are coupled over short distances. Banded data occurs also in other applications, for example in the physical mapping problem of the human genome, in paleontological data, in network data and in the discovery of overlapping communities without cycles.
  We study in this paper the banded structure of binary matrices, give a formal definition of the concept and discuss its theoretical properties. We consider the algorithmic problems of computing how far a matrix is from being banded, and of finding a good submatrix of the original data that exhibits approximate bandedness. Finally, we show by experiments on real data from ecology and other applications the usefulness of the concept. Our results reveal that bands exist in real datasets and that the final obtained ordering of rows and columns have natural interpretations.}
}

@inproceedings{DBLP:conf/sdm/GoethalsPM08,
  author       = {Bart Goethals and
                  Wim Le Page and
                  Heikki Mannila},
  title        = {Mining Association Rules of Simple Conjunctive Queries},
  booktitle    = {Proceedings of the {SIAM} International Conference on Data Mining,
                  {SDM} 2008, April 24-26, 2008, Atlanta, Georgia, {USA}},
  pages        = {96--107},
  publisher    = {{SIAM}},
  year         = {2008},
  url          = {https://doi.org/10.1137/1.9781611972788.9},
  doi          = {10.1137/1.9781611972788.9},
  timestamp    = {Mon, 05 Jun 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/GoethalsPM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We present an algorithm for mining association rules in arbitrary relational databases. We define association rules over a simple, but appealing subclass of conjunctive queries, and show that many interesting patterns can be found. We propose an efficient algorithm and a database-oriented implementation in SQL, together with several promising and convincing experimental results.}
}

@inproceedings{DBLP:conf/sdm/GalloMM08,
  author       = {Arianna Gallo and
                  Pauli Miettinen and
                  Heikki Mannila},
  title        = {Finding Subgroups having Several Descriptions: Algorithms for Redescription Mining},
  booktitle    = {Proceedings of the {SIAM} International Conference on Data Mining,
                  {SDM} 2008, April 24-26, 2008, Atlanta, Georgia, {USA}},
  pages        = {334--345},
  publisher    = {{SIAM}},
  year         = {2008},
  url          = {https://doi.org/10.1137/1.9781611972788.30},
  doi          = {10.1137/1.9781611972788.30},
  timestamp    = {Fri, 02 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/sdm/GalloMM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Given a 0–1 dataset, we consider the redescription mining task introduced by Ramakrishnan, Parida, and Zaki. The problem is to find subsets of the rows that can be (approximately) defined by at least two different Boolean formulae on the attributes. That is, we search for pairs (α, β) of Boolean formulae such that the implications α → β and β → α both hold with high accuracy. We require that the two descriptions α and β are syntactically sufficiently different. Such pairs of descriptions indicate that the subset has different definitions, a fact that gives useful information about the data. We give simple algorithms for this task, and evaluate their performance. The methods are based on pruning the search space of all possible pairs of formulae by different accuracy criteria. The significance of the findings is tested by using randomization methods. Experimental results on simulated and real data show that the methods work well: on simulated data they find the planted subsets, and on real data they produce small and understandable results.}
}

@inproceedings{DBLP:conf/sdm/OjalaVKHM08,
  author       = {Markus Ojala and
                  Niko Vuokko and
                  Aleksi Kallio and
                  Niina Haiminen and
                  Heikki Mannila},
  title        = {Randomization of real-valued matrices for assessing the significance of data mining results},
  booktitle    = {Proceedings of the {SIAM} International Conference on Data Mining,
                  {SDM} 2008, April 24-26, 2008, Atlanta, Georgia, {USA}},
  pages        = {494--505},
  publisher    = {{SIAM}},
  year         = {2008},
  url          = {https://doi.org/10.1137/1.9781611972788.45},
  doi          = {10.1137/1.9781611972788.45},
  timestamp    = {Fri, 27 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/sdm/OjalaVKHM08.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Randomization is an important technique for assessing the significance of data mining results. Given an input data set, a randomization method samples at random from some class of datasets that share certain characteristics with the original data. The measure of interest on the original data is then compared to the measure on the samples to assess its significance.
  For certain types of data, e.g., gene expression matrices, it is useful to be able to sample datasets that share row and column means and variances. Testing whether the results of a data mining algorithm on such randomized datasets differ from the results on the true dataset tells us whether the results on the true data were an artifact of the row and column means and variances, or due to some more interesting phenomena in the data.
  In this paper, we study the problem of generating such randomized datasets. We describe three alternative algorithms based on local transformations and Metropolis sampling, and show that the methods are efficient and usable in practice. We evaluate the performance of the methods both on real and generated data. The results indicate that the methods work efficiently and solve the defined problem.}
}

@article{DBLP:journals/bmcbi/HaiminenMT07,
  author       = {Niina Haiminen and
                  Heikki Mannila and
                  Evimaria Terzi},
  title        = {Comparing segmentations by applying randomization techniques},
  journal      = {{BMC} Bioinform.},
  volume       = {8},
  year         = {2007},
  url          = {https://doi.org/10.1186/1471-2105-8-171},
  doi          = {10.1186/1471-2105-8-171},
  timestamp    = {Fri, 09 Apr 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/bmcbi/HaiminenMT07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {There exist many segmentation techniques for genomic sequences, and the segmentations can also be based on many different biological features. We show how to evaluate and compare the quality of segmentations obtained by different techniques and alternative biological features.
  We apply randomization techniques for evaluating the quality of a given segmentation. Our example applications include isochore detection and the discovery of coding-noncoding structure. We obtain segmentations of relevant sequences by applying different techniques, and use alternative features to segment on. We show that some of the obtained segmentations are very similar to the underlying true segmentations, and this similarity is statistically significant. For some other segmentations, we show that equally good results are likely to appear by chance.
  We introduce a framework for evaluating segmentation quality, and demonstrate its use on two examples of segmental genomic structures. We transform the process of quality evaluation from simply viewing the segmentations, to obtaining p-values denoting significance of segmentation similarity.}
}

@article{DBLP:journals/bmcbi/LandwehrMETM07,
  author       = {Niels Landwehr and
                  Taneli Mielik{\"{a}}inen and
                  Lauri Eronen and
                  Hannu Toivonen and
                  Heikki Mannila},
  title        = {Constrained hidden Markov models for population-based haplotyping},
  journal      = {{BMC} Bioinform.},
  volume       = {8},
  number       = {{S-2}},
  year         = {2007},
  url          = {https://doi.org/10.1186/1471-2105-8-S2-S9},
  doi          = {10.1186/1471-2105-8-S2-S9},
  timestamp    = {Sun, 15 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/bmcbi/LandwehrMETM07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Haplotype Reconstruction is the problem of resolving the hidden phase information in genotype data obtained from laboratory measurements. Solving this problem is an important intermediate step in gene association studies, which seek to uncover the genetic basis of complex diseases. We propose a novel approach for haplotype reconstruction based on constrained hidden Markov models. Models are constructed by incrementally refining and regularizing the structure of a simple generative model for genotype data under Hardy-Weinberg equilibrium.
  The proposed method is evaluated on real-world and simulated population data. Results show that it is competitive with other recently proposed methods in terms of reconstruction accuracy, while offering a particularly good trade-off between computational costs and quality of results for large datasets.
  Relatively simple probabilistic approaches for haplotype reconstruction based on structured hidden Markov models are competitive with more complex, well-established techniques in this field.}
}

@article{DBLP:journals/lalc/HinneburgMKNR07,
  author       = {Alexander Hinneburg and
                  Heikki Mannila and
                  Samuli Kaislaniemi and
                  Terttu Nevalainen and
                  Helena Raumolin{-}Brunberg},
  title        = {How to Handle Small Samples: Bootstrap and Bayesian Methods in the Analysis of Linguistic Change},
  journal      = {Lit. Linguistic Comput.},
  volume       = {22},
  number       = {2},
  pages        = {137--150},
  year         = {2007},
  url          = {https://doi.org/10.1093/llc/fqm006},
  doi          = {10.1093/LLC/FQM006},
  timestamp    = {Wed, 20 May 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/lalc/HinneburgMKNR07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Estimating the relative frequencies of linguistic features is a fundamental task in linguistic computation. As the amount of text or speech that is available from a given user of the language typically varies greatly, and the sample sizes tend to be small, the most straightforward methods do not always give the most informative answers. Bootstrap and Bayesian methods provide techniques for handling the uncertainty in small samples. We describe these techniques for estimating frequencies from small samples, and show how they can be applied to the study of linguistic change. As a test case, we use the introduction of the pronoun you as subject in the data provided by the Corpus of Early English Correspondence (c. 1410–1681).}
}

@article{DBLP:journals/tkdd/GionisMT07,
  author       = {Aristides Gionis and
                  Heikki Mannila and
                  Panayiotis Tsaparas},
  title        = {Clustering aggregation},
  journal      = {{ACM} Trans. Knowl. Discov. Data},
  volume       = {1},
  number       = {1},
  pages        = {4},
  year         = {2007},
  url          = {https://doi.org/10.1145/1217299.1217303},
  doi          = {10.1145/1217299.1217303},
  timestamp    = {Thu, 23 Jun 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/tkdd/GionisMT07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the following problem: given a set of clusterings, find a single clustering that agrees as much as possible with the input clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the clustering aggregation problem; each categorical attribute can be viewed as a clustering of the input rows where rows are grouped together if they take the same value on that attribute. Clustering aggregation can also be used as a metaclustering method to improve the robustness of clustering by combining the output of multiple algorithms. Furthermore, the problem formulation does not require a priori information about the number of clusters; it is naturally determined by the optimization function.
  In this article, we give a formal statement of the clustering aggregation problem, and we propose a number of algorithms. Our algorithms make use of the connection between clustering aggregation and the problem of correlation clustering. Although the problems we consider are NP-hard, for several of our methods, we provide theoretical guarantees on the quality of the solutions. Our work provides the best deterministic approximation algorithm for the variation of the correlation clustering problem we consider. We also show how sampling can be used to scale the algorithms for large datasets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions.}
}

@article{DBLP:journals/tkdd/GionisMMT07,
  author       = {Aristides Gionis and
                  Heikki Mannila and
                  Taneli Mielik{\"{a}}inen and
                  Panayiotis Tsaparas},
  title        = {Assessing data mining results via swap randomization},
  journal      = {{ACM} Trans. Knowl. Discov. Data},
  volume       = {1},
  number       = {3},
  pages        = {14},
  year         = {2007},
  url          = {https://doi.org/10.1145/1297332.1297338},
  doi          = {10.1145/1297332.1297338},
  timestamp    = {Thu, 23 Jun 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/tkdd/GionisMMT07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The problem of assessing the significance of data mining results on high-dimensional 0--1 datasets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by standard statistical tests such as chi-square, or other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are difficult to apply to sets of patterns or other complex results of data mining algorithms. In this article, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins as the given dataset, computing the results of interest on the randomized instances and comparing them to the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and spectral analysis. To generate random datasets with given margins, we use variations of a Markov chain approach which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is expected, given the row and column margins of the datasets, while for other datasets the discovered structure conveys information that is not captured by the margin counts.}
}

@inproceedings{DBLP:conf/ida/HyvonenGM07,
  author       = {Saara Hyv{\"{o}}nen and
                  Aristides Gionis and
                  Heikki Mannila},
  editor       = {Michael R. Berthold and
                  John Shawe{-}Taylor and
                  Nada Lavrac},
  title        = {Recurrent Predictive Models for Sequence Segmentation},
  booktitle    = {Advances in Intelligent Data Analysis VII, 7th International Symposium
                  on Intelligent Data Analysis, {IDA} 2007, Ljubljana, Slovenia, September
                  6-8, 2007, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {4723},
  pages        = {195--206},
  publisher    = {Springer},
  year         = {2007},
  url          = {https://doi.org/10.1007/978-3-540-74825-0\_18},
  doi          = {10.1007/978-3-540-74825-0\_18},
  timestamp    = {Tue, 14 May 2019 10:00:49 +0200},
  biburl       = {https://dblp.org/rec/conf/ida/HyvonenGM07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Many sequential data sets have a segmental structure, and similar types of segments occur repeatedly. We consider sequences where the underlying phenomenon of interest is governed by a small set of models that change over time. Potential examples of such data are environmental, genomic, and economic sequences. Given a target sequence and a (possibly multivariate) sequence of observation values, we consider the problem of finding a small collection of models that can be used to explain the target phenomenon in a piecewise fashion using the observation values. We assume the same model will be used for multiple segments. We give an algorithm for this task based on first segmenting the sequence using dynamic programming, and then using k-median or facility location techniques to find the optimal set of models. We report on some experimental results.}
}

@inproceedings{DBLP:conf/kdd/HeikinheimoSHMM07,
  author       = {Hannes Heikinheimo and
                  Jouni K. Sepp{\"{a}}nen and
                  Eino Hinkkanen and
                  Heikki Mannila and
                  Taneli Mielik{\"{a}}inen},
  editor       = {Pavel Berkhin and
                  Rich Caruana and
                  Xindong Wu},
  title        = {Finding low-entropy sets and trees from binary data},
  booktitle    = {Proceedings of the 13th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, San Jose, California, USA, August
                  12-15, 2007},
  pages        = {350--359},
  publisher    = {{ACM}},
  year         = {2007},
  url          = {https://doi.org/10.1145/1281192.1281232},
  doi          = {10.1145/1281192.1281232},
  timestamp    = {Fri, 10 Mar 2023 14:55:31 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/HeikinheimoSHMM07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The discovery of subsets with special properties from binary data hasbeen one of the key themes in pattern discovery. Pattern classes suchas frequent itemsets stress the co-occurrence of the value 1 in the data. While this choice makes sense in the context of sparse binary data, it disregards potentially interesting subsets of attributes that have some other type of dependency structure.
  We consider the problem of finding all subsets of attributes that have low complexity. The complexity is measured by either the entropy of the projection of the data on the subset, or the entropy of the data for the subset when modeled using a Bayesian tree, with downward or upward pointing edges. We show that the entropy measure on sets has a monotonicity property, and thus a levelwise approach can find all low-entropy itemsets. We also show that the tree-based measures are bounded above by the entropy of the corresponding itemset, allowing similar algorithms to be used for finding low-entropy trees. We describe algorithms for finding all subsets satisfying an entropy condition. We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data. We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data.}
}

@inproceedings{DBLP:conf/kdd/MannilaT07,
  author       = {Heikki Mannila and
                  Evimaria Terzi},
  editor       = {Pavel Berkhin and
                  Rich Caruana and
                  Xindong Wu},
  title        = {Nestedness and segmented nestedness},
  booktitle    = {Proceedings of the 13th {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, San Jose, California, USA, August
                  12-15, 2007},
  pages        = {480--489},
  publisher    = {{ACM}},
  year         = {2007},
  url          = {https://doi.org/10.1145/1281192.1281245},
  doi          = {10.1145/1281192.1281245},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaT07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Consider each row of a 0-1 dataset as the subset of the columns for which the row has an 1. Then a dataset is nested, if for all pairs of rows one row is either a superset or subset of the other. The concept of nestedness has its origins in ecology, where approximate versions of it has been used to model the species distribution in different locations. We argue that nestedness and its extensions are interesting properties of datasets, and that they can be applied also to domains other than ecology.
  We first define natural measures of nestedness and study their properties. We then define the concept of k-nestedness: a dataset is (almost) k-nested if the set of columns can be partitioned to k parts so that each part is (almost) nested. We consider the algorithmic problems of computing how far a dataset is from being k-nested, and for finding a good partition of the columns into k parts. The algorithms are based on spectral partitioning, and scale to moderately large datasets. We apply the methods to real data from ecology and from other applications, and demonstrate the usefulness of the concept.}
}

@inproceedings{DBLP:conf/pkdd/UkkonenM07,
  author       = {Antti Ukkonen and
                  Heikki Mannila},
  editor       = {Joost N. Kok and
                  Jacek Koronacki and
                  Ram{\'{o}}n L{\'{o}}pez de M{\'{a}}ntaras and
                  Stan Matwin and
                  Dunja Mladenic and
                  Andrzej Skowron},
  title        = {Finding Outlying Items in Sets of Partial Rankings},
  booktitle    = {Knowledge Discovery in Databases: {PKDD} 2007, 11th European Conference
                  on Principles and Practice of Knowledge Discovery in Databases, Warsaw,
                  Poland, September 17-21, 2007, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {4702},
  pages        = {265--276},
  publisher    = {Springer},
  year         = {2007},
  url          = {https://doi.org/10.1007/978-3-540-74976-9\_26},
  doi          = {10.1007/978-3-540-74976-9\_26},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/UkkonenM07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Partial rankings are totally ordered subsets of a set of items. For example, the sequence in which a user browses through different parts of a website is a partial ranking. We consider the following problem. Given a set D of partial rankings, find items that have strongly different status in different parts of D. To do this, we first compute a clustering of D and then look at items whose average rank in the cluster substantially deviates from its average rank in D. Such items can be seen as those that contribute the most to the differences between the clusters. To test the statistical significance of the found items, we propose a method that is based on a MCMC algorithm for sampling random sets of partial rankings with exactly the same statistics as D. We also demonstrate the method on movie rankings and gene expression data.}
}

@inproceedings{DBLP:conf/sigmod/DasguptaDM07,
  author       = {Arjun Dasgupta and
                  Gautam Das and
                  Heikki Mannila},
  editor       = {Chee Yong Chan and
                  Beng Chin Ooi and
                  Aoying Zhou},
  title        = {A random walk approach to sampling hidden databases},
  booktitle    = {Proceedings of the {ACM} {SIGMOD} International Conference on Management
                  of Data, Beijing, China, June 12-14, 2007},
  pages        = {629--640},
  publisher    = {{ACM}},
  year         = {2007},
  url          = {https://doi.org/10.1145/1247480.1247550},
  doi          = {10.1145/1247480.1247550},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/sigmod/DasguptaDM07.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A large part of the data on the World Wide Web is hidden behind form-like interfaces. These interfaces interact with a hidden back-end database to provide answers to user queries. Generating a uniform random sample of this hidden database by using only the publicly available interface gives us access to the underlying data distribution. In this paper, we propose a random walk scheme over the query space provided by the interface to sample such databases. We discuss variants where the query space is visualized as a fixed and random ordering of attributes. We also propose techniques to further improve the sample quality by using a probabilistic rejection based approach. We conduct extensive experiments to illustrate the accuracy and efficiency of our techniques.}
}

@article{DBLP:journals/ploscb/PuolamakiFM06,
  author       = {Kai Puolam{\"{a}}ki and
                  Mikael Fortelius and
                  Heikki Mannila},
  title        = {Seriation in Paleontological Data Using Markov Chain Monte Carlo Methods},
  journal      = {PLoS Comput. Biol.},
  volume       = {2},
  number       = {2},
  year         = {2006},
  url          = {https://doi.org/10.1371/journal.pcbi.0020006},
  doi          = {10.1371/JOURNAL.PCBI.0020006},
  timestamp    = {Thu, 10 Sep 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ploscb/PuolamakiFM06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Given a collection of fossil sites with data about the taxa that occur in each site, the task in biochronology is to find good estimates for the ages or ordering of sites. We describe a full probabilistic model for fossil data. The parameters of the model are natural: the ordering of the sites, the origination and extinction times for each taxon, and the probabilities of different types of errors. We show that the posterior distributions of these parameters can be estimated reliably by using Markov chain Monte Carlo techniques. The posterior distributions of the model parameters can be used to answer many different questions about the data, including seriation (finding the best ordering of the sites) and outlier detection. We demonstrate the usefulness of the model and estimation method on synthetic data and on real data on large late Cenozoic mammals. As an example, for the sites with large number of occurrences of common genera, our methods give orderings, whose correlation with geochronologic ages is 0.95.}
}

@inproceedings{DBLP:conf/dis/RasinenHM06,
  author       = {Antti Rasinen and
                  Jaakko Hollm{\'{e}}n and
                  Heikki Mannila},
  editor       = {Ljupco Todorovski and
                  Nada Lavrac and
                  Klaus P. Jantke},
  title        = {Analysis of Linux Evolution Using Aligned Source Code Segments},
  booktitle    = {Discovery Science, 9th International Conference, {DS} 2006, Barcelona,
                  Spain, October 7-10, 2006, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {4265},
  pages        = {209--218},
  publisher    = {Springer},
  year         = {2006},
  url          = {https://doi.org/10.1007/11893318\_22},
  doi          = {10.1007/11893318\_22},
  timestamp    = {Tue, 14 May 2019 10:00:46 +0200},
  biburl       = {https://dblp.org/rec/conf/dis/RasinenHM06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The Linux operating system embodies a development history of 15 years and community effort of hundreds of voluntary developers. We examine the structure and evolution of the Linux kernel by considering the source code of the kernel as ordinary text without any regard to its semantics. After selecting three functionally central modules to study, we identified code segments using local alignments of source code from a reduced set of file comparisons. The further stages of the analyses take advantage of these identified alignments. We build module-specific visualizations, or descendant graphs, to visualize the overall code migration between versions and files. More detailed view can be achieved with chain graphs which show the time evolution of alignments between selected files. The methods used here may also prove useful in studying large collections of legacy code, whose original maintainers are not available.}
}

@inproceedings{DBLP:conf/f-egc/Mannila06,
  author       = {Heikki Mannila},
  editor       = {Gilbert Ritschard and
                  Chabane Djeraba},
  title        = {Finding fragments of orders and total orders from 0-1 data},
  booktitle    = {Extraction et gestion des connaissances (EGC'2006), Actes des sixi{\`{e}}mes
                  journ{\'{e}}es Extraction et Gestion des Connaissances, Lille, France,
                  17-20 janvier 2006, 2 Volumes},
  series       = {Revue des Nouvelles Technologies de l'Information},
  volume       = {{RNTI-E-6}},
  pages        = {1},
  publisher    = {C{\'{e}}padu{\`{e}}s-{\'{E}}ditions},
  year         = {2006},
  url          = {https://editions-rnti.fr/?inprocid=1000315},
  timestamp    = {Wed, 06 Nov 2024 17:10:35 +0100},
  biburl       = {https://dblp.org/rec/conf/f-egc/Mannila06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {High-dimensional collections of 0-1 data occur in many applications. The attributes in
  such data sets are typically considered to be unordered. However, in many cases there is a
  natural total or partial order underlying the variables of the data set. Examples of variables
  for which such orders exist include terms in documents and paleontological sites in fossil data
  collections. We describe methods for finding fragments of total orders from such data, based
  on finding frequently occurring patterns. We also discuss techniques for finding good total
  orderings (seriation) based on spectral ordering and MCMC methods.}
}

@inproceedings{DBLP:conf/icdm/GwaderaGM06,
  author       = {Robert Gwadera and
                  Aristides Gionis and
                  Heikki Mannila},
  title        = {Optimal Segmentation Using Tree Models},
  booktitle    = {Proceedings of the 6th {IEEE} International Conference on Data Mining
                  {(ICDM} 2006), 18-22 December 2006, Hong Kong, China},
  pages        = {244--253},
  publisher    = {{IEEE} Computer Society},
  year         = {2006},
  url          = {https://doi.org/10.1109/ICDM.2006.122},
  doi          = {10.1109/ICDM.2006.122},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icdm/GwaderaGM06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequence data are abundant in application areas such as computational biology, environmental sciences, and telecommunications. Many real-life sequences have a strong segmental structure, with segments of different complexities. In this paper we study the description of sequence segments using variable length Markov chains (VLMCs), also known as tree models. We discover the segment boundaries of a sequence and at the same time we compute a VLMC for each segment. We use the Bayesian information criterion (BIC) and a variant of the minimum description length (MDL) principle that uses the Krichevsky-Trofimov (KT) code length to select the number of segments of a sequence. On DNA data the method selects segments that closely correspond to the annotated regions of the genes.}
}

@inproceedings{DBLP:conf/icdm/TattiMGM06,
  author       = {Nikolaj Tatti and
                  Taneli Mielik{\"{a}}inen and
                  Aristides Gionis and
                  Heikki Mannila},
  title        = {What is the Dimension of Your Binary Data?},
  booktitle    = {Proceedings of the 6th {IEEE} International Conference on Data Mining
                  {(ICDM} 2006), 18-22 December 2006, Hong Kong, China},
  pages        = {603--612},
  publisher    = {{IEEE} Computer Society},
  year         = {2006},
  url          = {https://doi.org/10.1109/ICDM.2006.167},
  doi          = {10.1109/ICDM.2006.167},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icdm/TattiMGM06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Many 0/1 datasets have a very large number of variables; however, they are sparse and the dependency structure of the variables is simpler than the number of variables would suggest. Defining the effective dimensionality of such a dataset is a nontrivial problem. We consider the problem of defining a robust measure of dimension for 0/1 datasets, and show that the basic idea of fractal dimension can be adapted for binary data. However, as such the fractal dimension is difficult to interpret. Hence we introduce the concept of normalized fractal dimension. For a dataset D, its normalized fractal dimension counts the number of independent columns needed to achieve the unnormalized fractal dimension of D. The normalized fractal dimension measures the degree of dependency structure of the data. We study the properties of the normalized fractal dimension and discuss its computation. We give empirical results on the normalized fractal dimension, comparing it against PCA.}
}

@inproceedings{DBLP:conf/kdd/GionisMMT06,
  author       = {Aristides Gionis and
                  Heikki Mannila and
                  Taneli Mielik{\"{a}}inen and
                  Panayiotis Tsaparas},
  editor       = {Tina Eliassi{-}Rad and
                  Lyle H. Ungar and
                  Mark Craven and
                  Dimitrios Gunopulos},
  title        = {Assessing data mining results via swap randomization},
  booktitle    = {Proceedings of the Twelfth {ACM} {SIGKDD} International Conference
                  on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August
                  20-23, 2006},
  pages        = {167--176},
  publisher    = {{ACM}},
  year         = {2006},
  url          = {https://doi.org/10.1145/1150402.1150424},
  doi          = {10.1145/1150402.1150424},
  timestamp    = {Thu, 23 Jun 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/GionisMMT06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The problem of assessing the significance of data mining results on high-dimensional 0--1 datasets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by standard statistical tests such as chi-square, or other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are difficult to apply to sets of patterns or other complex results of data mining algorithms. In this article, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins as the given dataset, computing the results of interest on the randomized instances and comparing them to the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and spectral analysis. To generate random datasets with given margins, we use variations of a Markov chain approach which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is expected, given the row and column margins of the datasets, while for other datasets the discovered structure conveys information that is not captured by the margin counts.}
}

@inproceedings{DBLP:conf/kdd/GionisMPU06,
  author       = {Aristides Gionis and
                  Heikki Mannila and
                  Kai Puolam{\"{a}}ki and
                  Antti Ukkonen},
  editor       = {Tina Eliassi{-}Rad and
                  Lyle H. Ungar and
                  Mark Craven and
                  Dimitrios Gunopulos},
  title        = {Algorithms for discovering bucket orders from data},
  booktitle    = {Proceedings of the Twelfth {ACM} {SIGKDD} International Conference
                  on Knowledge Discovery and Data Mining, Philadelphia, PA, USA, August
                  20-23, 2006},
  pages        = {561--566},
  publisher    = {{ACM}},
  year         = {2006},
  url          = {https://doi.org/10.1145/1150402.1150468},
  doi          = {10.1145/1150402.1150468},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/GionisMPU06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Ordering and ranking items of different types are important tasks in various applications, such as query processing and scientific data mining. A total order for the items can be misleading, since there are groups of items that have practically equal ranks.We consider bucket orders, i.e., total orders with ties. They can be used to capture the essential order information without overfitting the data: they form a useful concept class between total orders and arbitrary partial orders. We address the question of finding a bucket order for a set of items, given pairwise precedence information between the items. We also discuss methods for computing the pairwise precedence data.We describe simple and efficient algorithms for finding good bucket orders. Several of the algorithms have a provable approximation guarantee, and they scale well to large datasets. We provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms.}
}

@inproceedings{DBLP:conf/pkdd/HeikinheimoMS06,
  author       = {Hannes Heikinheimo and
                  Heikki Mannila and
                  Jouni K. Sepp{\"{a}}nen},
  editor       = {Johannes F{\"{u}}rnkranz and
                  Tobias Scheffer and
                  Myra Spiliopoulou},
  title        = {Finding Trees from Unordered 0-1 Data},
  booktitle    = {Knowledge Discovery in Databases: {PKDD} 2006, 10th European Conference
                  on Principles and Practice of Knowledge Discovery in Databases, Berlin,
                  Germany, September 18-22, 2006, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {4213},
  pages        = {175--186},
  publisher    = {Springer},
  year         = {2006},
  url          = {https://doi.org/10.1007/11871637\_20},
  doi          = {10.1007/11871637\_20},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/HeikinheimoMS06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Tree structures are a natural way of describing occurrence relationships between attributes in a dataset. We define a new class of tree patterns for unordered 0–1 data and consider the problem of discovering frequently occurring members of this pattern class. Intuitively, a tree T occurs in a row u of the data, if the attributes of T that occur in u form a subtree of T containing the root. We show that this definition has advantageous properties: only shallow trees have a significant probability of occurring in random data, and the definition allows a simple levelwise algorithm for mining all frequently occurring trees. We demonstrate with empirical results that the method is feasible and that it discovers interesting trees in real data.}
}

@inproceedings{DBLP:conf/sdm/BinghamGHHMT06,
  author       = {Ella Bingham and
                  Aristides Gionis and
                  Niina Haiminen and
                  Heli Hiisil{\"{a}} and
                  Heikki Mannila and
                  Evimaria Terzi},
  editor       = {Joydeep Ghosh and
                  Diane Lambert and
                  David B. Skillicorn and
                  Jaideep Srivastava},
  title        = {Segmentation and dimensionality reduction},
  booktitle    = {Proceedings of the Sixth {SIAM} International Conference on Data Mining,
                  April 20-22, 2006, Bethesda, MD, {USA}},
  pages        = {372--383},
  publisher    = {{SIAM}},
  year         = {2006},
  url          = {https://doi.org/10.1137/1.9781611972764.33},
  doi          = {10.1137/1.9781611972764.33},
  timestamp    = {Fri, 27 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/sdm/BinghamGHHMT06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequence segmentation and dimensionality reduction have been used as methods for studying high-dimensional sequences — they both reduce the complexity of the representation of the original data. In this paper we study the interplay of these two techniques. We formulate the problem of segmenting a sequence while modeling it with a basis of small size, thus essentially reducing the dimension of the input sequence. We give three different algorithms for this problem: all combine existing methods for sequence segmentation and dimensionality reduction. For two of the proposed algorithms we prove guarantees for the quality of the solutions obtained. We describe experimental results on synthetic and real datasets, including data on exchange rates and genomic sequences. Our experiments show that the algorithms indeed discover underlying structure in the data, including both segmental structure and interdependencies between the dimensions.}
}

@article{DBLP:journals/kais/SalmenkiviM05,
  author       = {Marko Salmenkivi and
                  Heikki Mannila},
  title        = {Using Markov chain Monte Carlo and dynamic programming for event sequence data},
  journal      = {Knowl. Inf. Syst.},
  volume       = {7},
  number       = {3},
  pages        = {267--288},
  year         = {2005},
  url          = {https://doi.org/10.1007/s10115-004-0157-6},
  doi          = {10.1007/S10115-004-0157-6},
  timestamp    = {Fri, 12 Mar 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/kais/SalmenkiviM05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events are a common type of data in various scientific and business applications, e.g. telecommunication network management, study of web access logs, biostatistics and epidemiology. A natural approach to modelling event sequences is using time-dependent intensity functions, indicating the expected number of events per time unit. In Bayesian modelling, piecewise constant functions can be utilized to model continuous intensities, if the number of segments is a model parameter. The reversible jump Markov chain Monte Carlo (RJMCMC) methods can be exploited in the data analysis. With very large quantities, these approaches may be too slow. We study dynamic programming algorithms for finding the best fitting piecewise constant intensity function, given a number of pieces. We introduce simple heuristics for pruning the number of the potential change points of the functions. Empirical evidence from trials on real and artificial data sets is provided, showing that the developed methods yield high performance and they can be applied to very large data sets. We also compare the RJMCMC and dynamic programming approaches and show that the results correspond closely. The methods are applied to fault-alarm sequences produced by large telecommunication networks.}
}

@inproceedings{DBLP:conf/icdm/PapadimitriouGTVMF05,
  author       = {Spiros Papadimitriou and
                  Aristides Gionis and
                  Panayiotis Tsaparas and
                  Risto A. V{\"{a}}is{\"{a}}nen and
                  Heikki Mannila and
                  Christos Faloutsos},
  title        = {Parameter-Free Spatial Data Mining Using {MDL}},
  booktitle    = {Proceedings of the 5th {IEEE} International Conference on Data Mining
                  {(ICDM} 2005), 27-30 November 2005, Houston, Texas, {USA}},
  pages        = {346--353},
  publisher    = {{IEEE} Computer Society},
  year         = {2005},
  url          = {https://doi.org/10.1109/ICDM.2005.117},
  doi          = {10.1109/ICDM.2005.117},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icdm/PapadimitriouGTVMF05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Consider spatial data consisting of a set of binary features taking values over a collection of spatial extents (grid cells). We propose a method that simultaneously finds spatial correlation and feature co-occurrence patterns, without any parameters. In particular, we employ the minimum description length (MDL) principle coupled with a natural way of compressing regions. This defines what "good" means: a feature co-occurrence pattern is good, if it helps us better compress the set of locations for these features. Conversely, a spatial correlation is good, if it helps us better compress the set of features in the corresponding region. Our approach is scalable for large datasets (both number of locations and of features). We evaluate our method on both real and synthetic datasets.}
}

@inproceedings{DBLP:conf/icdm/AfratiDGMMT05,
  author       = {Foto N. Afrati and
                  Gautam Das and
                  Aristides Gionis and
                  Heikki Mannila and
                  Taneli Mielik{\"{a}}inen and
                  Panayiotis Tsaparas},
  title        = {Mining Chains of Relations},
  booktitle    = {Proceedings of the 5th {IEEE} International Conference on Data Mining
                  {(ICDM} 2005), 27-30 November 2005, Houston, Texas, {USA}},
  pages        = {553--556},
  publisher    = {{IEEE} Computer Society},
  year         = {2005},
  url          = {https://doi.org/10.1109/ICDM.2005.94},
  doi          = {10.1109/ICDM.2005.94},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icdm/AfratiDGMMT05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Traditional data mining applications consider the problem of mining a single relation between two attributes. For example, in a scientific bibliography database, authors are related to papers, and we may be interested in discovering association rules between authors. However, in real life, we often have multiple attributes related though chains of relations. For example, authors write papers, and papers concern one or more topics. Mining such relational chains poses additional challenges. In this paper we consider the following problem: given a chain of two relations R/sub 1/ (A, P) and R/sub 2/(P, T) we want to find selectors for the objects in T such that the projected relation between A and P satisfies a specific property. The motivation for our approach is that a given property might not hold on the whole dataset, but it might hold when projecting the data on a selector set. We discuss various algorithms and we examine the conditions under which the a priori technique can be used. We experimentally demonstrate the effectiveness of our methods.}
}

@inproceedings{DBLP:conf/kdd/UkkonenFM05,
  author       = {Antti Ukkonen and
                  Mikael Fortelius and
                  Heikki Mannila},
  editor       = {Robert Grossman and
                  Roberto J. Bayardo and
                  Kristin P. Bennett},
  title        = {Finding partial orders from unordered 0-1 data},
  booktitle    = {Proceedings of the Eleventh {ACM} {SIGKDD} International Conference
                  on Knowledge Discovery and Data Mining, Chicago, Illinois, USA, August
                  21-24, 2005},
  pages        = {285--293},
  publisher    = {{ACM}},
  year         = {2005},
  url          = {https://doi.org/10.1145/1081870.1081904},
  doi          = {10.1145/1081870.1081904},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/UkkonenFM05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In applications such as paleontology and medical genetics the 0-1 data has an underlying unknown order (the ages of the fossil sites, the locations of markers in the genome). The order might be total or partial: for example, two sites in different parts of the globe might be ecologically incomparable, or the ordering of certain markers might be different in different subgroups of the data. We consider the following problem. Given a table over a set of 0-1 variables, find a partial order for the rows minimizing a score function and being as specific as possible. The score function can be, e.g., the number of changes from 1 to 0 in a column (for paleontology) or the likelihood of the marker sequence (for genomic data). Our solution for this task first constructs small totally ordered fragments of the partial order, then finds good orientations for the fragments, and finally uses a simple and efficient heuristic method for finding a partial order that corresponds well with the collection of fragments. We describe the method, discuss its properties, and give empirical results on paleontological data demonstrating the usefulness of the method. In the application the use of the method highlighted some previously unknown properties of the data and pointed out probable errors in the data.}
}

@inproceedings{DBLP:conf/wabi/RastasKMU05,
  author       = {Pasi Rastas and
                  Mikko Koivisto and
                  Heikki Mannila and
                  Esko Ukkonen},
  editor       = {Rita Casadio and
                  Gene Myers},
  title        = {A Hidden Markov Technique for Haplotype Reconstruction},
  booktitle    = {Algorithms in Bioinformatics, 5th International Workshop, {WABI} 2005,
                  Mallorca, Spain, October 3-6, 2005, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {3692},
  pages        = {140--151},
  publisher    = {Springer},
  year         = {2005},
  url          = {https://doi.org/10.1007/11557067\_12},
  doi          = {10.1007/11557067\_12},
  timestamp    = {Tue, 14 May 2019 10:00:40 +0200},
  biburl       = {https://dblp.org/rec/conf/wabi/RastasKMU05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We give a new algorithm for the genotype phasing problem. Our solution is based on a hidden Markov model for haplotypes. The model has a uniform structure, unlike most solutions proposed so far that model recombinations using haplotype blocks. In our model, the haplotypes can be seen as a result of iterated recombinations applied on a few founder haplotypes. We find maximum likelihood model of this type by using the EM algorithm. We show how to solve the subtleties of the EM algorithm that arise when genotypes are generated using a haplotype model. We compare our method to the well-known currently available algorithms (PHASE, HAP, GERBIL) using some standard and new datasets. Our algorithm is relatively fast and gives results that are always best or second best among the methods compared.}
}

@incollection{DBLP:books/sp/wang2005/SalmenkiviM05,
  author       = {Marko Salmenkivi and
                  Heikki Mannila},
  editor       = {Jason Tsong{-}Li Wang and
                  Mohammed Javeed Zaki and
                  Hannu Toivonen and
                  Dennis E. Shasha},
  title        = {Piecewise Constant Modeling of Sequential Data Using Reversible Jump Markov Chain Monte Carlo},
  booktitle    = {Data Mining in Bioinformatics},
  pages        = {85--103},
  publisher    = {Springer},
  year         = {2005},
  timestamp    = {Thu, 11 Aug 2016 11:10:44 +0200},
  biburl       = {https://dblp.org/rec/books/sp/wang2005/SalmenkiviM05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We describe the use of reversible jump Markov chain Monte Carlo (RJMCMC) methods for finding piecewise constant descriptions of sequential data. The method provides posterior distributions on the number of segments in the data and thus gives a much broader view on the potential data than do methods (such as dynamic programming) that aim only at finding a single optimal solution. On the other hand, MCMC methods can be more difficult to implement than discrete optimization techniques, and monitoring convergence of the simulations is not trivial. We illustrate the methods by modeling the GC content and distribution of occurrences of ORFs and SNPs along the human genomes. We show how the simple models can be extended by modeling the influence of GC content on the intensity of ORF occurrence.}
}

@inproceedings{DBLP:conf/alt/KoivistoKMRU04,
  author       = {Mikko Koivisto and
                  Teemu Kivioja and
                  Heikki Mannila and
                  Pasi Rastas and
                  Esko Ukkonen},
  editor       = {Shai Ben{-}David and
                  John Case and
                  Akira Maruoka},
  title        = {Hidden Markov Modelling Techniques for Haplotype Analysis},
  booktitle    = {Algorithmic Learning Theory, 15th International Conference, {ALT}
                  2004, Padova, Italy, October 2-5, 2004, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {3244},
  pages        = {37--52},
  publisher    = {Springer},
  year         = {2004},
  url          = {https://doi.org/10.1007/978-3-540-30215-5\_4},
  doi          = {10.1007/978-3-540-30215-5\_4},
  timestamp    = {Tue, 14 May 2019 10:00:51 +0200},
  biburl       = {https://dblp.org/rec/conf/alt/KoivistoKMRU04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A hidden Markov model is introduced for descriptive modelling the mosaic–like structures of haplotypes, due to iterated recombinations within a population. Methods using the minimum description length principle are given for fitting such models to training data. Possible applications of the models are delineated, and some preliminary analysis results on real sets of haplotypes are reported, demonstrating the potential of our methods.}
}

@inproceedings{DBLP:conf/cinq/SeppanenM04,
  author       = {Jouni K. Sepp{\"{a}}nen and
                  Heikki Mannila},
  editor       = {Jean{-}Fran{\c{c}}ois Boulicaut and
                  Luc De Raedt and
                  Heikki Mannila},
  title        = {Boolean Formulas and Frequent Sets},
  booktitle    = {Constraint-Based Mining and Inductive Databases, European Workshop
                  on Inductive Databases and Constraint Based Mining, Hinterzarten,
                  Germany, March 11-13, 2004, Revised Selected Papers},
  series       = {Lecture Notes in Computer Science},
  volume       = {3848},
  pages        = {348--361},
  publisher    = {Springer},
  year         = {2004},
  url          = {https://doi.org/10.1007/11615576\_16},
  doi          = {10.1007/11615576\_16},
  timestamp    = {Sun, 25 Oct 2020 22:35:10 +0100},
  biburl       = {https://dblp.org/rec/conf/cinq/SeppanenM04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/kdd/AfratiGM04,
  author       = {Foto N. Afrati and
                  Aristides Gionis and
                  Heikki Mannila},
  editor       = {Won Kim and
                  Ron Kohavi and
                  Johannes Gehrke and
                  William DuMouchel},
  title        = {Approximating a collection of frequent sets},
  booktitle    = {Proceedings of the Tenth {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Seattle, Washington, USA, August
                  22-25, 2004},
  pages        = {12--19},
  publisher    = {{ACM}},
  year         = {2004},
  url          = {https://doi.org/10.1145/1014052.1014057},
  doi          = {10.1145/1014052.1014057},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/AfratiGM04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/kdd/SeppanenM04,
  author       = {Jouni K. Sepp{\"{a}}nen and
                  Heikki Mannila},
  editor       = {Won Kim and
                  Ron Kohavi and
                  Johannes Gehrke and
                  William DuMouchel},
  title        = {Dense itemsets},
  booktitle    = {Proceedings of the Tenth {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Seattle, Washington, USA, August
                  22-25, 2004},
  pages        = {683--688},
  publisher    = {{ACM}},
  year         = {2004},
  url          = {https://doi.org/10.1145/1014052.1014140},
  doi          = {10.1145/1014052.1014140},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/SeppanenM04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/pkdd/GionisMS04,
  author       = {Aristides Gionis and
                  Heikki Mannila and
                  Jouni K. Sepp{\"{a}}nen},
  editor       = {Jean{-}Fran{\c{c}}ois Boulicaut and
                  Floriana Esposito and
                  Fosca Giannotti and
                  Dino Pedreschi},
  title        = {Geometric and Combinatorial Tiles in 0-1 Data},
  booktitle    = {Knowledge Discovery in Databases: {PKDD} 2004, 8th European Conference
                  on Principles and Practice of Knowledge Discovery in Databases, Pisa,
                  Italy, September 20-24, 2004, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {3202},
  pages        = {173--184},
  publisher    = {Springer},
  year         = {2004},
  url          = {https://doi.org/10.1007/978-3-540-30116-5\_18},
  doi          = {10.1007/978-3-540-30116-5\_18},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/GionisMS04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/vldb/GeertsMT04,
  author       = {Floris Geerts and
                  Heikki Mannila and
                  Evimaria Terzi},
  editor       = {Mario A. Nascimento and
                  M. Tamer {\"{O}}zsu and
                  Donald Kossmann and
                  Ren{\'{e}}e J. Miller and
                  Jos{\'{e}} A. Blakeley and
                  K. Bernhard Schiefer},
  title        = {Relational link-based ranking},
  booktitle    = {(e)Proceedings of the Thirtieth International Conference on Very Large
                  Data Bases, {VLDB} 2004, Toronto, Canada, August 31 - September 3
                  2004},
  pages        = {552--563},
  publisher    = {Morgan Kaufmann},
  year         = {2004},
  url          = {http://www.vldb.org/conf/2004/RS15P1.PDF},
  doi          = {10.1016/B978-012088469-8.50050-4},
  timestamp    = {Fri, 07 Jun 2019 12:44:00 +0200},
  biburl       = {https://dblp.org/rec/conf/vldb/GeertsMT04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/dagstuhl/RamakrishnanAFB04,
  author       = {Raghu Ramakrishnan and
                  Rakesh Agrawal and
                  Johann{-}Christoph Freytag and
                  Toni Bollinger and
                  Christopher W. Clifton and
                  Saso Dzeroski and
                  Jochen Hipp and
                  Daniel A. Keim and
                  Stefan Kramer and
                  Hans{-}Peter Kriegel and
                  Ulf Leser and
                  Bing Liu and
                  Heikki Mannila and
                  Rosa Meo and
                  Shinichi Morishita and
                  Raymond T. Ng and
                  Jian Pei and
                  Prabhakar Raghavan and
                  Myra Spiliopoulou and
                  Jaideep Srivastava and
                  Vicen{\c{c}} Torra},
  editor       = {Rakesh Agrawal and
                  Johann{-}Christoph Freytag and
                  Raghu Ramakrishnan},
  title        = {Data Mining: The Next Generation},
  booktitle    = {Perspectives Workshop: Data Mining: The Next Generation, 11.07. -
                  16.07.2004},
  series       = {Dagstuhl Seminar Proceedings},
  volume       = {04292},
  publisher    = {Internationales Begegnungs- und Forschungszentrum f{\"{u}}r Informatik
                  (IBFI), Schloss Dagstuhl, Germany},
  year         = {2004},
  url          = {http://drops.dagstuhl.de/opus/volltexte/2005/270/},
  timestamp    = {Sat, 01 Mar 2025 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/dagstuhl/RamakrishnanAFB04.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/tkde/PavlovMS03,
  author       = {Dmitry Pavlov and
                  Heikki Mannila and
                  Padhraic Smyth},
  title        = {Beyond Independence: Probabilistic Models for Query Approximation
                  on Binary Transaction Data},
  journal      = {{IEEE} Trans. Knowl. Data Eng.},
  volume       = {15},
  number       = {6},
  pages        = {1409--1421},
  year         = {2003},
  url          = {https://doi.org/10.1109/TKDE.2003.1245281},
  doi          = {10.1109/TKDE.2003.1245281},
  timestamp    = {Tue, 29 Dec 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/tkde/PavlovMS03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/tods/GunopulosKMSTS03,
  author       = {Dimitrios Gunopulos and
                  Roni Khardon and
                  Heikki Mannila and
                  Sanjeev Saluja and
                  Hannu Toivonen and
                  Ram Sewak Sharm},
  title        = {Discovering all most specific sentences},
  journal      = {{ACM} Trans. Database Syst.},
  volume       = {28},
  number       = {2},
  pages        = {140--174},
  year         = {2003},
  url          = {https://doi.org/10.1145/777943.777945},
  doi          = {10.1145/777943.777945},
  timestamp    = {Thu, 14 Oct 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/tods/GunopulosKMSTS03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/kdd/GionisKM03,
  author       = {Aristides Gionis and
                  Teija Kujala and
                  Heikki Mannila},
  editor       = {Lise Getoor and
                  Ted E. Senator and
                  Pedro M. Domingos and
                  Christos Faloutsos},
  title        = {Fragments of order},
  booktitle    = {Proceedings of the Ninth {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, Washington, DC, USA, August 24
                  - 27, 2003},
  pages        = {129--136},
  publisher    = {{ACM}},
  year         = {2003},
  url          = {https://doi.org/10.1145/956750.956768},
  doi          = {10.1145/956750.956768},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/GionisKM03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/pkdd/LeinoMP03,
  author       = {Antti Leino and
                  Heikki Mannila and
                  Ritva Liisa Pitk{\"{a}}nen},
  editor       = {Nada Lavrac and
                  Dragan Gamberger and
                  Hendrik Blockeel and
                  Ljupco Todorovski},
  title        = {Rule Discovery and Probabilistic Modeling for Onomastic Data},
  booktitle    = {Knowledge Discovery in Databases: {PKDD} 2003, 7th European Conference
                  on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik,
                  Croatia, September 22-26, 2003, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2838},
  pages        = {291--302},
  publisher    = {Springer},
  year         = {2003},
  url          = {https://doi.org/10.1007/978-3-540-39804-2\_27},
  doi          = {10.1007/978-3-540-39804-2\_27},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/LeinoMP03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/pkdd/MielikainenM03,
  author       = {Taneli Mielik{\"{a}}inen and
                  Heikki Mannila},
  editor       = {Nada Lavrac and
                  Dragan Gamberger and
                  Hendrik Blockeel and
                  Ljupco Todorovski},
  title        = {The Pattern Ordering Problem},
  booktitle    = {Knowledge Discovery in Databases: {PKDD} 2003, 7th European Conference
                  on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik,
                  Croatia, September 22-26, 2003, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2838},
  pages        = {327--338},
  publisher    = {Springer},
  year         = {2003},
  url          = {https://doi.org/10.1007/978-3-540-39804-2\_30},
  doi          = {10.1007/978-3-540-39804-2\_30},
  timestamp    = {Mon, 22 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/MielikainenM03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/pkdd/SeppanenBM03,
  author       = {Jouni K. Sepp{\"{a}}nen and
                  Ella Bingham and
                  Heikki Mannila},
  editor       = {Nada Lavrac and
                  Dragan Gamberger and
                  Hendrik Blockeel and
                  Ljupco Todorovski},
  title        = {A Simple Algorithm for Topic Identification in 0-1 Data},
  booktitle    = {Knowledge Discovery in Databases: {PKDD} 2003, 7th European Conference
                  on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik,
                  Croatia, September 22-26, 2003, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2838},
  pages        = {423--434},
  publisher    = {Springer},
  year         = {2003},
  url          = {https://doi.org/10.1007/978-3-540-39804-2\_38},
  doi          = {10.1007/978-3-540-39804-2\_38},
  timestamp    = {Mon, 22 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/SeppanenBM03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/psb/KoivistoPVHELPUM03,
  author       = {Mikko Koivisto and
                  Markus Perola and
                  T. Varilo and
                  W. Hennah and
                  J. Ekelund and
                  Margus Lukk and
                  L. Peltonen and
                  Esko Ukkonen and
                  Heikki Mannila},
  editor       = {Russ B. Altman and
                  A. Keith Dunker and
                  Lawrence Hunter and
                  Teri E. Klein},
  title        = {An {MDL} Method for Finding Haplotype Blocks and for Estimating the Strength of Haplotype Block Boundaries},
  booktitle    = {Proceedings of the 8th Pacific Symposium on Biocomputing, {PSB} 2003,
                  Lihue, Hawaii, USA, January 3-7, 2003},
  pages        = {502--513},
  year         = {2003},
  url          = {http://psb.stanford.edu/psb-online/proceedings/psb03/koivisto.pdf},
  timestamp    = {Thu, 12 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/psb/KoivistoPVHELPUM03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We describe a new method for finding haplotype blocks based on the use of the minimum description length principle. We give a rigorous definition of the quality of a segmentation of a genomic region into blocks, and describe a dynamic programming algorithm for finding the optimal segmentation with respect to this measure. We also describe a method for finding the probability of a block boundary for each pair of adjacent markers: this gives a tool for evaluating the significance of each block boundary. We have applied the method to the published data of Daly et al.1 The results are in relatively good agreement with the published results, but also show clear differences in the predicted block boundaries and their strengths. We also give results on the block structure in population isolates.}
}

@inproceedings{DBLP:conf/recomb/GionisM03,
  author       = {Aristides Gionis and
                  Heikki Mannila},
  editor       = {Martin Vingron and
                  Sorin Istrail and
                  Pavel A. Pevzner and
                  Michael S. Waterman and
                  Webb Miller},
  title        = {Finding recurrent sources in sequences},
  booktitle    = {Proceedings of the Sventh Annual International Conference on Computational
                  Biology, {RECOMB} 2003, Berlin, Germany, April 10-13, 2003},
  pages        = {123--130},
  publisher    = {{ACM}},
  year         = {2003},
  url          = {https://doi.org/10.1145/640075.640091},
  doi          = {10.1145/640075.640091},
  timestamp    = {Mon, 13 May 2019 09:30:09 +0200},
  biburl       = {https://dblp.org/rec/conf/recomb/GionisM03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Many genomic sequences and, more generally, (multivariate) time series display tremendous variability. However, often it is reasonable to assume that the sequence is actually generated by or assembled from a small number of sources, each of which might contribute several segments to the sequence. That is, there are h hidden sources such that the sequence can be written as a concatenation of k > h pieces, each of which stems from one of the h sources. We define this (k,h)-segmentation problem and show that it is NP-hard in the general case. We give approximation algorithms achieving approximation ratios of 3 for the L1 error measure and sqrt(5) for the L2 error measure, and generalize the results to higher dimensions. We give empirical results on real (chromosome 22) and artificial data showing that the methods work well in practice.}
}

@inproceedings{DBLP:conf/sdm/HollmenSM03,
  author       = {Jaakko Hollm{\'{e}}n and
                  Jouni K. Sepp{\"{a}}nen and
                  Heikki Mannila},
  editor       = {Daniel Barbar{\'{a}} and
                  Chandrika Kamath},
  title        = {Mixture Models and Frequent Sets: Combining Global and Local Methods for 0-1 Data},
  booktitle    = {Proceedings of the Third {SIAM} International Conference on Data Mining,
                  San Francisco, CA, USA, May 1-3, 2003},
  pages        = {289--293},
  publisher    = {{SIAM}},
  year         = {2003},
  url          = {https://doi.org/10.1137/1.9781611972733.32},
  doi          = {10.1137/1.9781611972733.32},
  timestamp    = {Fri, 06 Oct 2023 11:45:43 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/HollmenSM03.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We study the interaction between global and local techniques in data mining. Specifically, we study the collections of frequent sets in clusters produced by a probabilistic clustering using mixtures of Bernoulli models. That is, we first analyze 0–1 datasets by a global technique (probabilistic clustering using the EM algorithm) and then do a local analysis (discovery of frequent sets) in each of the clusters. The results indicate that the use of clustering as a preliminary phase in finding frequent sets produces clusters that have significantly different collections of frequent sets. We also test the significance of the differences in the frequent set collections in the different clusters by obtaining estimates of the underlying joint density. To get from the local patterns in each cluster back to distributions, we use the maximum entropy technique [17] to obtain a local model for each cluster, and then combine these local models to get a mixture model. We obtain clear improvements to the approximation quality against the use of either the mixture model or the maximum entropy model.}
}

@article{DBLP:journals/bioinformatics/MannilaPSK02,
  author       = {Heikki Mannila and
                  Anne Patrikainen and
                  Jouni K. Sepp{\"{a}}nen and
                  Juha Kere},
  title        = {Long-range control of expression in yeast},
  journal      = {Bioinform.},
  volume       = {18},
  number       = {3},
  pages        = {482--483},
  year         = {2002},
  url          = {https://doi.org/10.1093/bioinformatics/18.3.482},
  doi          = {10.1093/BIOINFORMATICS/18.3.482},
  timestamp    = {Mon, 02 Mar 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/bioinformatics/MannilaPSK02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The mechanisms underlying gene expression are of fundamental importance in biology. We report on a whole-genome study on yeast (Saccharomyces cerevisiae), correlating gene expression profiles against their spatial location. Our results show that there is a small but highly significant dependence of gene expression level by relative position of gene pairs.}
}

@article{DBLP:journals/cacm/HanAKMP02,
  author       = {Jiawei Han and
                  Russ B. Altman and
                  Vipin Kumar and
                  Heikki Mannila and
                  Daryl Pregibon},
  title        = {Emerging scientific applications in data mining},
  journal      = {Commun. {ACM}},
  volume       = {45},
  number       = {8},
  pages        = {54--58},
  year         = {2002},
  url          = {https://doi.org/10.1145/545151.545179},
  doi          = {10.1145/545151.545179},
  timestamp    = {Fri, 29 Apr 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/cacm/HanAKMP02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Recent progress in scientific and engineering applications has accumulated huge volumes of high-dimensional data, stream data, unstructured and semi-structured data, and spatial and temporal data. Highly scalable and sophisticated data mining tools for such applications represent one of the most active research frontiers in data mining. The related challenges in several emerging domains are outlined. The long-terms significance of the new data of molecular biology is that it can be combined with clinical medical data to achieve a higher-resolution understanding of the causes for and treatment of disease. A major challenge for data mining in biomedicine is therefore the organization of molecular data, cellular data, and clinical data in ways allowing them to be integrated for the sake of knowledge extraction. Earth science data mining consists of 2 main components: the modeling of ecological data and the design of efficient algorithms for finding spatiotemporal patterns.}
}

@inproceedings{DBLP:conf/eccb/SalmenkiviKM02,
  author       = {Marko Salmenkivi and
                  Juha Kere and
                  Heikki Mannila},
  title        = {Genome segmentation using piecewise constant intensity models and reversible jump {MCMC}},
  booktitle    = {Proceedings of the European Conference on Computational Biology {(ECCB}
                  2002), October 6-9, 2002, Saarbr{\"{u}}cken, Germany},
  pages        = {211--218},
  year         = {2002},
  timestamp    = {Thu, 23 Jun 2016 15:53:27 +0200},
  biburl       = {https://dblp.org/rec/conf/eccb/SalmenkiviKM02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The existence of whole genome sequences makes it possible to search for global structure in the genome. We consider modeling the occurrence frequencies of discrete patterns (such as starting points of ORFs or other interesting phenomena) along the genome. We use piecewise constant intensity models with varying number of pieces, and show how a reversible jump Markov Chain Monte Carlo (RJMCMC) method can be used to obtain a posteriori distribution on the intensity of the patterns along the genome. We apply the method to modeling the occurrence of ORFs in the human genome. The results show that the chromosomes consist of 5–35 clearly distinct segments, and that the posteriori number and length of the segments shows significant variation. On the other hand, for the yeast genome the intensity of ORFs is nearly constant.}
}

@inproceedings{DBLP:conf/icalp/Mannila02,
  author       = {Heikki Mannila},
  editor       = {Peter Widmayer and
                  Francisco Triguero Ruiz and
                  Rafael Morales Bueno and
                  Matthew Hennessy and
                  Stephan J. Eidenbenz and
                  Ricardo Conejo},
  title        = {Local and Global Methods in Data Mining: Basic Techniques and Open Problems},
  booktitle    = {Automata, Languages and Programming, 29th International Colloquium,
                  {ICALP} 2002, Malaga, Spain, July 8-13, 2002, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2380},
  pages        = {57--68},
  publisher    = {Springer},
  year         = {2002},
  url          = {https://doi.org/10.1007/3-540-45465-9\_6},
  doi          = {10.1007/3-540-45465-9\_6},
  timestamp    = {Wed, 11 Sep 2019 13:15:53 +0200},
  biburl       = {https://dblp.org/rec/conf/icalp/Mannila02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining has in recent years emerged as an interesting area in the boundary between algorithms, probabilistic modeling, statistics, and databases. Data mining research can be divided into global approaches, which try to model the whole data, and local methods, which try to find useful patterns occurring in the data. We discuss briefly some simple local and global techniques, review two attempts at combining the approaches, and list open problems with an algorithmic flavor.}
}

@inproceedings{DBLP:conf/icde/LeungNM02,
  author       = {Carson Kai{-}Sang Leung and
                  Raymond T. Ng and
                  Heikki Mannila},
  editor       = {Rakesh Agrawal and
                  Klaus R. Dittrich},
  title        = {{OSSM:} {A} Segmentation Approach to Optimize Frequency Counting},
  booktitle    = {Proceedings of the 18th International Conference on Data Engineering,
                  San Jose, CA, USA, February 26 - March 1, 2002},
  pages        = {583--592},
  publisher    = {{IEEE} Computer Society},
  year         = {2002},
  url          = {https://doi.org/10.1109/ICDE.2002.994776},
  doi          = {10.1109/ICDE.2002.994776},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icde/LeungNM02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Computing the frequency of a pattern is one of the key operations in data mining algorithms. We describe a simple yet powerful way of speeding up any form of frequency counting satisfying the monotonicity condition. Our method, the optimized segment support map (OSSM), is a light-weight structure which partitions the collection of transactions into m segments, so as to reduce the number of candidate patterns that require frequency counting. We study the following problems: (1) what is the optimal number of segments to be used; and (2) given a user-determined m, what is the best segmentation/composition of the m segments? For Problem 1, we provide a thorough analysis and a theorem establishing the minimum value of m for which there is no accuracy lost in using the OSSM. For Problem 2, we develop various algorithms and heuristics, which efficiently generate OSSMs that are compact and effective, to help facilitate segmentation.}
}

@inproceedings{DBLP:conf/icdm/RaedtJLM02,
  author       = {Luc De Raedt and
                  Manfred Jaeger and
                  Sau Dan Lee and
                  Heikki Mannila},
  title        = {A Theory of Inductive Query Answering},
  booktitle    = {Proceedings of the 2002 {IEEE} International Conference on Data Mining
                  {(ICDM} 2002), 9-12 December 2002, Maebashi City, Japan},
  pages        = {123--130},
  publisher    = {{IEEE} Computer Society},
  year         = {2002},
  url          = {https://doi.org/10.1109/ICDM.2002.1183894},
  doi          = {10.1109/ICDM.2002.1183894},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icdm/RaedtJLM02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We introduce the Boolean inductive query evaluation problem, which is concerned with answering inductive queries that are arbitrary Boolean expressions over monotonic and anti-monotonic predicates. Boolean inductive queries can be used to address many problems in data mining and machine learning, such as local pattern mining and concept-learning, and actually provides a unifying view on many machine learning and data mining tasks. Secondly, we develop a decomposition theory for inductive query evaluation in which a Boolean query Q is reformulated into k sub-queries  that are the conjunction of a monotonic and an anti-monotonic predicate. The solution to each sub-query can be represented using a version space.We investigate how the number of version spaces k needed to answer the query can be minimized and define this as the dimension of the solution space and query. Thirdly, we generalize the notion of version spaces to cover Boolean queries, so that the solution sets form a closed Boolean-algebraic space under the usual set operations. The effects of these set operations on the dimension of the involved queries are studied.}
}

@inproceedings{DBLP:conf/kdd/BinghamMS02,
  author       = {Ella Bingham and
                  Heikki Mannila and
                  Jouni K. Sepp{\"{a}}nen},
  title        = {Topics in 0--1 data},
  booktitle    = {Proceedings of the Eighth {ACM} {SIGKDD} International Conference
                  on Knowledge Discovery and Data Mining, July 23-26, 2002, Edmonton,
                  Alberta, Canada},
  pages        = {450--455},
  publisher    = {{ACM}},
  year         = {2002},
  url          = {https://doi.org/10.1145/775047.775112},
  doi          = {10.1145/775047.775112},
  timestamp    = {Sat, 09 Apr 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/BinghamMS02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Large 0--1 datasets arise in various applications, such as market basket analysis and information retrieval. We concentrate on the study of topic models, aiming at results which indicate why certain methods succeed or fail. We describe simple algorithms for finding topic models from 0--1 data. We give theoretical results showing that the algorithms can discover the epsilon-separable topic models of Papadimitriou et al. We present empirical results showing that the algorithms find natural topics in real-world data sets. We also briefly discuss the connections to matrix approaches, including nonnegative matrix factorization and independent component analysis.}
}

@inproceedings{DBLP:conf/swat/Mannila02,
  author       = {Heikki Mannila},
  editor       = {Martti Penttonen and
                  Erik Meineche Schmidt},
  title        = {Combining Pattern Discovery and Probabilistic Modeling in Data Mining},
  booktitle    = {Algorithm Theory - {SWAT} 2002, 8th Scandinavian Workshop on Algorithm
                  Theory, Turku, Finland, July 3-5, 2002 Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2368},
  pages        = {19},
  publisher    = {Springer},
  year         = {2002},
  url          = {https://doi.org/10.1007/3-540-45471-3\_2},
  doi          = {10.1007/3-540-45471-3\_2},
  timestamp    = {Tue, 14 May 2019 10:00:39 +0200},
  biburl       = {https://dblp.org/rec/conf/swat/Mannila02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining has in recent years emerged as an interesting area in the boundary between algorithms, probabilistic modeling, statistics, and databases. Data mining research has come from two different traditions. The global approach aims at modeling the joint distribution of the data, while the local approach aims at efficient discovery of frequent patterns from the data. Among the global modeling techniques, mixture models have emerged as a strong unifying theme, and methods exist for fitting such models on large data sets. For pattern discovery, the methods for finding frequently occurring positive conjunctions have been applied in various domains. An interesting open issue is how to combine the two approaches, eg, by inferring joint distributions from pattern frequencies. Some promising results have been achieved using maximum entropy approaches. In the talk we describe some basic techniques in global and local approaches to data mining, and present a selection of open problems.}
}

@book{DBLP:books/mit/026208290,
  author       = {David J. Hand and
                  Heikki Mannila and
                  Padhraic Smyth},
  title        = {Principles of Data Mining},
  publisher    = {{MIT} Press},
  year         = {2001},
  url          = {https://mitpress.mit.edu/books/principles-data-mining},
  isbn         = {9780262082907},
  timestamp    = {Tue, 04 Apr 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/books/mit/026208290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The science of extracting useful information from large data sets or databases is known as data mining. It is a new discipline, lying at the intersection of statistics, machine learning, data management and databases, pattern recognition, artificial intelligence, and other areas. All of these are concerned with certain aspects of data analysis, so they have much in common—but each also has its own distinct flavor, emphasizing particular problems and types of solution. Because data mining encompasses a wide variety of topics in computer science and statistics it is impossible to cover all the potentially relevant material in a single text. Given this, we have focused on the topics that we believe are the most fundamental. From a teaching viewpoint the text is intended for undergraduate students at the senior (final year) level, or first or second-year graduate level, who wish to learn about the basic principles of data mining. The text should also be of value to researchers and practitioners who are interested in gaining a better understanding of data mining methods and techniques. A familiarity with the very basic concepts in probability, calculus, linear algebra, and optimization is assumed—in other words, an undergraduate background in any quantitative discipline such as engineering, computer science, mathematics, economics, etc., should provide a good background for reading and understanding this text.}
}

@article{DBLP:journals/njc/BollobasDGM01,
  author       = {B{\'{e}}la Bollob{\'{a}}s and
                  Gautam Das and
                  Dimitrios Gunopulos and
                  Heikki Mannila},
  title        = {Time-Series Similarity Problems and Well-Separated Geometric Sets},
  journal      = {Nord. J. Comput.},
  volume       = {8},
  number       = {4},
  pages        = {409--423},
  year         = {2001},
  url          = {http://www.cs.helsinki.fi/njc/References/bollobas2001:409.html},
  timestamp    = {Thu, 17 May 2018 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/njc/BollobasDGM01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Given a pair of nonidentical complex objects, defusing
  (and determining g) how similar they are toeach other is
  a nontrivial problem. In data mining applications, one
  frequently needs todetermine the similaritybetween two
  time series. We analyze a model of time-series similar-
  ity that allows outliers, and different scaling functions.
  We presentdeterministic and randomized algorithms for
  computing this notion of similarity. The algorithms are
  based on nontrivial tools and methods from computa-
  tional geometry. In particular, we use properties of fam-
  ilies of well-separated geometric sets. The randomized
  algorithm has provably good performanceand also works
  extremely efficiently in practice.}
}

@inproceedings{DBLP:conf/ecml/Mannila01,
  author       = {Heikki Mannila},
  editor       = {Luc De Raedt and
                  Peter A. Flach},
  title        = {Combining Discrete Algorithmic and Probabilistic Approaches in Data
                  Mining},
  booktitle    = {Machine Learning: {EMCL} 2001, 12th European Conference on Machine
                  Learning, Freiburg, Germany, September 5-7, 2001, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2167},
  pages        = {601},
  publisher    = {Springer},
  year         = {2001},
  url          = {https://doi.org/10.1007/3-540-44795-4\_52},
  doi          = {10.1007/3-540-44795-4\_52},
  timestamp    = {Sun, 25 Oct 2020 23:05:12 +0100},
  biburl       = {https://dblp.org/rec/conf/ecml/Mannila01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining research has approached the problems of analyzing large data sets in two ways. Simplifying a lot, the approaches can be characterized as follows. The database approach has concentrated on figuring out what types of summaries can be computed fast, and then finding ways of using those summaries. The model-based approach has focused on first finding useful model classes and then fast ways of fitting those models. In this talk I discuss some examples of both and describe some recent developments which try to combine the two approaches.}
}

@inproceedings{DBLP:conf/icdm/HimbergKMTT01,
  author       = {Johan Himberg and
                  Kalle Korpiaho and
                  Heikki Mannila and
                  Johanna Tikanm{\"{a}}ki and
                  Hannu Toivonen},
  editor       = {Nick Cercone and
                  Tsau Young Lin and
                  Xindong Wu},
  title        = {Time Series Segmentation for Context Recognition in Mobile Devices},
  booktitle    = {Proceedings of the 2001 {IEEE} International Conference on Data Mining,
                  29 November - 2 December 2001, San Jose, California, {USA}},
  pages        = {203--210},
  publisher    = {{IEEE} Computer Society},
  year         = {2001},
  url          = {https://doi.org/10.1109/ICDM.2001.989520},
  doi          = {10.1109/ICDM.2001.989520},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icdm/HimbergKMTT01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Recognizing the context of use is important in making mobile devices as simple to use as possible. Finding out what the user's situation is can help the device and underlying service in providing an adaptive and personalized user interface. The device can infer parts of the context of the user from sensor data: the mobile device can include sensors for acceleration, noise level, luminosity, humidity, etc. In this paper we consider context recognition by unsupervised segmentation of time series produced by sensors. Dynamic programming can be used to find segments that minimize the intra-segment variances. While this method produces optimal solutions, it is too slow for long sequences of data. We present and analyze randomized variations of the algorithm. One of them, global iterative replacement or GIR, gives approximately optimal results in a fraction of the time required by dynamic programming. We demonstrate the use of time series segmentation in context recognition for mobile phone applications.}
}

@inproceedings{DBLP:conf/kdd/CadezSM01,
  author       = {Igor V. Cadez and
                  Padhraic Smyth and
                  Heikki Mannila},
  editor       = {Doheon Lee and
                  Mario Schkolnick and
                  Foster J. Provost and
                  Ramakrishnan Srikant},
  title        = {Probabilistic modeling of transaction data with applications to profiling, visualization, and prediction},
  booktitle    = {Proceedings of the seventh {ACM} {SIGKDD} international conference
                  on Knowledge discovery and data mining, San Francisco, CA, USA, August
                  26-29, 2001},
  pages        = {37--46},
  publisher    = {{ACM}},
  year         = {2001},
  url          = {https://doi.org/10.1145/502512.502523},
  doi          = {10.1145/502512.502523},
  timestamp    = {Tue, 01 Jun 2021 15:21:34 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/CadezSM01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Transaction data is ubiquitous in data mining applications. Examples include market basket data in retail commerce, telephone call records in telecommunications, and Web logs of individual page-requests at Web sites. Profiling consists of using historical transaction data on individuals to construct a model of each individual's behavior. Simple profiling techniques such as histograms do not generalize well from sparse transaction data. In this paper we investigate the application of probabilistic mixture models to automatically generate profiles from large volumes of transaction data. In effect, the mixture model represents each individual's behavior as a linear combination of "basis transactions." We evaluate several variations of the model on a large retail transaction data set and show that the proposed model provides improved predictive power over simpler histogram-based techniques, as well as being relatively scalable, interpretable, and flexible. In addition we point to applications in outlier detection, customer ranking, interactive visualization, and so forth. The paper concludes by comparing and relating the proposed framework to other transaction-data modeling techniques such as association rules.}
}

@inproceedings{DBLP:conf/kdd/BinghamM01,
  author       = {Ella Bingham and
                  Heikki Mannila},
  editor       = {Doheon Lee and
                  Mario Schkolnick and
                  Foster J. Provost and
                  Ramakrishnan Srikant},
  title        = {Random projection in dimensionality reduction: applications to image and text data},
  booktitle    = {Proceedings of the seventh {ACM} {SIGKDD} international conference
                  on Knowledge discovery and data mining, San Francisco, CA, USA, August
                  26-29, 2001},
  pages        = {245--250},
  publisher    = {{ACM}},
  year         = {2001},
  url          = {https://doi.org/10.1145/502512.502546},
  doi          = {10.1145/502512.502546},
  timestamp    = {Sat, 09 Apr 2022 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/BinghamM01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.}
}

@inproceedings{DBLP:conf/kdd/MannilaS01,
  author       = {Heikki Mannila and
                  Marko Salmenkivi},
  editor       = {Doheon Lee and
                  Mario Schkolnick and
                  Foster J. Provost and
                  Ramakrishnan Srikant},
  title        = {Finding simple intensity descriptions from event sequence data},
  booktitle    = {Proceedings of the seventh {ACM} {SIGKDD} international conference
                  on Knowledge discovery and data mining, San Francisco, CA, USA, August
                  26-29, 2001},
  pages        = {341--346},
  publisher    = {{ACM}},
  year         = {2001},
  url          = {https://doi.org/10.1145/502512.502562},
  doi          = {10.1145/502512.502562},
  timestamp    = {Mon, 22 Mar 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaS01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events are an important type of data arising in various applications, including telecommunications, bio-statistics, web access analysis, etc. A basic approach to modeling such sequences is to find the underlying intensity functions describing the expected number of events per time unit. Typically, the intensity functions are assumed to be piecewise constant. We therefore consider different ways of fitting intensity models to event sequence data. We start by considering a Bayesian approach using Markov chain Monte Carlo (MCMC) methods with varying number of pieces. These methods can be used to produce posterior distributions on the intensity functions and they can also accomodate covariates. The drawback is that they are computationally intensive and thus are not very suitable for data mining applications in which large numbers of intensity functions have to be estimated. We consider dynamic programming approaches to finding the change points in the intensity functions. These methods can find the maximum likelihood intensity function in O(n2k) time for a sequence of n events and k different pieces of intensity. We show that simple heuristics can be used to prune the number of potential change points, yielding speedups of several orders of magnitude. The results of the improved dynamic programming method correspond very closely with the posterior averages produced by the MCMC methods.}
}

@inproceedings{DBLP:conf/pkdd/Mannila01,
  author       = {Heikki Mannila},
  editor       = {Luc De Raedt and
                  Arno Siebes},
  title        = {Combining Discrete Algorithmic and Probabilistic Approaches in Data Mining},
  booktitle    = {Principles of Data Mining and Knowledge Discovery, 5th European Conference,
                  {PKDD} 2001, Freiburg, Germany, September 3-5, 2001, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2168},
  pages        = {493},
  publisher    = {Springer},
  year         = {2001},
  url          = {https://doi.org/10.1007/3-540-44794-6\_42},
  doi          = {10.1007/3-540-44794-6\_42},
  timestamp    = {Sun, 25 Oct 2020 22:53:46 +0100},
  biburl       = {https://dblp.org/rec/conf/pkdd/Mannila01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining research has approached the problems of analyzing large data sets in two ways. Simplifying a lot, the approaches can be characterized as follows. The database approach has concentrated on figuring out what types of summaries can be computed fast, and then finding ways of using those summaries. The model-based approach has focused on first finding useful model classes and then fast ways of fitting those models. In this talk I discuss some examples of both and describe some recent developments which try to combine the two approaches.}
}

@inproceedings{DBLP:conf/sdm/MannilaR01,
  author       = {Heikki Mannila and
                  Dmitry Rusakov},
  editor       = {Vipin Kumar and
                  Robert L. Grossman},
  title        = {Decomposition of Event Sequences into Independent Components},
  booktitle    = {Proceedings of the First {SIAM} International Conference on Data Mining,
                  {SDM} 2001, Chicago, IL, USA, April 5-7, 2001},
  pages        = {1--17},
  publisher    = {{SIAM}},
  year         = {2001},
  url          = {https://doi.org/10.1137/1.9781611972719.2},
  doi          = {10.1137/1.9781611972719.2},
  timestamp    = {Wed, 17 May 2017 14:24:53 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/MannilaR01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Many real-world processes result in an extensive logs of sequences of events, i.e., events coupled with time of occurrence. Examples of such process logs include alarms produced by a large telecommunication network, web-access data, biostatistics, etc. In many cases, it is useful to decompose the incoming stream of events into the number of independent streams. Such decomposition may reveal valuable information about the event generating process, e.g. dependencies among alarms in the telecommunication network, relationships between web-users and relevant symptoms of the decease. It may, as well, facilitate further analysis of the data by working with independent components separately.}
}

@inproceedings{DBLP:conf/sdm/MannilaS01,
  author       = {Heikki Mannila and
                  Jouni K. Sepp{\"{a}}nen},
  editor       = {Vipin Kumar and
                  Robert L. Grossman},
  title        = {Finding similar situations in sequences of events via random projections},
  booktitle    = {Proceedings of the First {SIAM} International Conference on Data Mining,
                  {SDM} 2001, Chicago, IL, USA, April 5-7, 2001},
  pages        = {1--16},
  publisher    = {{SIAM}},
  year         = {2001},
  url          = {https://doi.org/10.1137/1.9781611972719.3},
  doi          = {10.1137/1.9781611972719.3},
  timestamp    = {Wed, 17 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/sdm/MannilaS01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events occur in several applications, such as mobile services (the re-
  quests made by each user), telecommunication network alarm handling, user inter-
  face studies, etc. Such a sequence can be denoted (⟨ei, ti⟩), where i = 1, . . . , n, and
  for each i, ei ∈ E is an eventtype and ti is the occurrencetime. In this paper we describe a simple and fast way of mapping a sequence of
  events into points in k-dimensional Euclidean space using a random function, and
  show how this mapping can be used as a preprocessing method for finding similar
  situations. We contrast the accuracy and performance of our method with the
  dynamic programming approach, but the real validation of the method is in the
  experimental results.}
}

@article{DBLP:journals/sigkdd/Mannila00,
  author       = {Heikki Mannila},
  title        = {Theoretical Frameworks for Data Mining},
  journal      = {{SIGKDD} Explor.},
  volume       = {1},
  number       = {2},
  pages        = {30--32},
  year         = {2000},
  url          = {https://doi.org/10.1145/846183.846191},
  doi          = {10.1145/846183.846191},
  timestamp    = {Tue, 29 Sep 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/sigkdd/Mannila00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Research in data mining and knowledge discovery in databases
  has mostly concentrated on developing good algorithms for
  various data mining tasks (see for example the recent proceedings
  of KDD conferences). Some parts of the research effort have gone
  to investigating data mining process, user interface issues,
  database topics, or visualization [7]. Relatively little has been
  published about the theoretical foundations of data mining. In this
  paper I present some possible theoretical approaches to data
  mining. The area is at its infancy, and there probably are more
  questions than answers in this paper.}
}

@inproceedings{DBLP:conf/bibe/ToivonenOVOSMK00,
  author       = {Hannu Toivonen and
                  Paivi Onkamo and
                  Kari Vasko and
                  Vesa Ollikainen and
                  Petteri Sevon and
                  Heikki Mannila and
                  Juha Kere},
  editor       = {Nikolaos G. Bourbakis},
  title        = {Gene Mapping by Haplotype Pattern Mining},
  booktitle    = {1st {IEEE} International Symposium on Bioinformatics and Biomedical
                  Engineering, Arlington, Virginia, USA, November 8-10, 2000, Proceedings},
  pages        = {99--108},
  publisher    = {{IEEE} Computer Society},
  year         = {2000},
  url          = {https://doi.org/10.1109/BIBE.2000.889596},
  doi          = {10.1109/BIBE.2000.889596},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/bibe/ToivonenOVOSMK00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Genetic markers are being increasingly utilized in gene mapping. The discovery of associations between markers and patient phenotypes - such as a disease status - enables the identification of potential disease gene loci. The rationale is that, in diseases with a reasonable genetic contribution, diseased individuals are more likely to have associated marker alleles near the disease susceptibility gene than control individuals. We describe a new gene mapping method-haplotype pattern mining (HPM) - that is based on discovering recurrent marker patterns. We define a class of useful haplotype patterns in genetic case-control data, give an algorithm for finding disease-associated haplotypes, and show how to use them to identify disease susceptibility loci. Experimental studies show that the method has good localization power in data sets with large degrees of phenocopies and with lots of missing and erroneous data. We also demonstrate how the method can be used to discover several genes simultaneously.}
}

@inproceedings{DBLP:conf/icde/MannilaS00,
  author       = {Heikki Mannila and
                  Padhraic Smyth},
  editor       = {David B. Lomet and
                  Gerhard Weikum},
  title        = {Approximate Query Answering with Frequent Sets and Maximum Entropy},
  booktitle    = {Proceedings of the 16th International Conference on Data Engineering,
                  San Diego, California, USA, February 28 - March 3, 2000},
  pages        = {309},
  publisher    = {{IEEE} Computer Society},
  year         = {2000},
  url          = {https://doi.org/10.1109/ICDE.2000.839426},
  doi          = {10.1109/ICDE.2000.839426},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icde/MannilaS00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The accumulation of high-throughput genomic and proteomic data allows for the reconstruction of the increasingly large and complex metabolic networks. In order to analyze accumulated data and reconstructed networks, it is critical to identify network patterns and evolutionary relations between metabolic networks. But even finding similar networks becomes computationally challenging. Alignment of the reconstructed networks can help to catch model inconsistencies and infer missing elements. We have formulated the network alignment problem which asks for the optimal vertex-to-vertex mapping allowing path contraction, vertex deletion, and vertex insertions. This paper gives the first efficient algorithm for optimal aligning of metabolic pathways with bounded tree width. In particular, the optimal alignment from pathway P to pathway T can be found in time O (| VP|| VT|^(a+ 1)) $, where VP and VT are the vertex sets of pathways and a is the tree width of P. This significantly improves alignment tools since the E. coli metabolic network has tree width 3 and more than 90% of pathways of several organisms are series-parallel. We have implemented the algorithm for alignment of metabolic pathways of tree width 2 with arbitrary metabolic networks. Our experiments show that allowing pattern vertex deletion significantly improves alignment. We also have applied the network alignment to identifying inconsistency, inferring missing enzymes, and finding potential candidates for filling the holes.}
}

@inproceedings{DBLP:conf/kdd/MannilaM00,
  author       = {Heikki Mannila and
                  Christopher Meek},
  editor       = {Raghu Ramakrishnan and
                  Salvatore J. Stolfo and
                  Roberto J. Bayardo and
                  Ismail Parsa},
  title        = {Global partial orders from sequential data},
  booktitle    = {Proceedings of the sixth {ACM} {SIGKDD} international conference on
                  Knowledge discovery and data mining, Boston, MA, USA, August 20-23,
                  2000},
  pages        = {161--168},
  publisher    = {{ACM}},
  year         = {2000},
  url          = {https://doi.org/10.1145/347090.347122},
  doi          = {10.1145/347090.347122},
  timestamp    = {Wed, 08 May 2024 08:22:41 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaM00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events arise in many applications, such as web browsing, e-commerce, and monitoring of processes. An important problem in mining sets of sequences of events is to get an overview of the ordering relationships in the data. We present a method for finding partial orders that describe the ordering relationships between the events in a collection of sequences. The method is based on viewing a partial order as a generative model for a set of sequences, and applying mixture modeling techniques to obtain a descriptive set of partial orders. Runtimes for our algorithm scale linearly in the number of sequences and polynomially in the number of different event types. Thus, the methods scales to handle large data sets and can be used for reasonable numbers of different types of events. We illustrate our technique by applying it to student enrollment data and web browsing data.}
}

@inproceedings{DBLP:conf/pkdd/DasM00,
  author       = {Gautam Das and
                  Heikki Mannila},
  editor       = {Djamel A. Zighed and
                  Henryk Jan Komorowski and
                  Jan M. Zytkow},
  title        = {Context-Based Similarity Measures for Categorical Databases},
  booktitle    = {Principles of Data Mining and Knowledge Discovery, 4th European Conference,
                  {PKDD} 2000, Lyon, France, September 13-16, 2000, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1910},
  pages        = {201--210},
  publisher    = {Springer},
  year         = {2000},
  url          = {https://doi.org/10.1007/3-540-45372-5\_20},
  doi          = {10.1007/3-540-45372-5\_20},
  timestamp    = {Mon, 05 Feb 2024 20:31:27 +0100},
  biburl       = {https://dblp.org/rec/conf/pkdd/DasM00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Similarity between complex data objects is one of the central notions in data mining. We propose certain similarity (or distance) measures between various components of a 0/1 relation. We define measures between attributes, between rows, and between subrelations of the database. They find important applications in clustering, classification, and several other data mining processes. Our measures are based on the contexts of individual components. For example, two products (i.e., attributes) are deemed similar if their respective sets of customers (i.e., subrelations) are similar. This reveals more subtle relationships between components, something that is usually missing in simpler measures. Our problem of finding distance measures can be formulated as a system of nonlinear equations. We present an iterative algorithm which, when seeded with random initial values, converges quickly to stable distances in practice (typically requiring less than five iterations). The algorithm requires only one database scan. Results on artificial and real data show that our method is efficient, and produces results with intuitive appeal.}
}

@inproceedings{DBLP:conf/uai/PavlovMS00,
  author       = {Dmitry Pavlov and
                  Heikki Mannila and
                  Padhraic Smyth},
  editor       = {Craig Boutilier and
                  Mois{\'{e}}s Goldszmidt},
  title        = {Probabilistic Models for Query Approximation with Large Sparse Binary Data Sets},
  booktitle    = {{UAI} '00: Proceedings of the 16th Conference in Uncertainty in Artificial
                  Intelligence, Stanford University, Stanford, California, USA, June
                  30 - July 3, 2000},
  pages        = {465--472},
  publisher    = {Morgan Kaufmann},
  year         = {2000},
  url          = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&\#38;smnu=2\&\#38;article\_id=54\&\#38;proceeding\_id=16},
  timestamp    = {Wed, 03 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/uai/PavlovMS00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Large sparse sets of binary transaction data with millions of records and thousands of attributes occur in various domains: customers purchasing products, users visiting web pages, and documents containing words are just three typical examples. Real-time query selectivity estimation (the problem of estimating the number of rows in the data satisfying a given predicate) is an important practical problem for such databases.
  We investigate the application of probabilistic models to this problem. In particular, we study a Markov random field (MRF) approach based on frequent sets and maximum entropy, and compare it to the independence model and the Chow-Liu tree model. We find that the MRF model provides substantially more accurate probability estimates than the other methods but is more expensive from a computational and memory viewpoint. To alleviate the computational requirements we show how one can apply bucket elimination and clique tree approaches to take advantage of structure in the models and in the queries. We provide experimental results on two large real-world transaction datasets.}
}

@article{DBLP:journals/acta/KhardonMR99,
  author       = {Roni Khardon and
                  Heikki Mannila and
                  Dan Roth},
  title        = {Reasoning with Examples: Propositional Formulae and Database Dependencies},
  journal      = {Acta Informatica},
  volume       = {36},
  number       = {4},
  pages        = {267--286},
  year         = {1999},
  url          = {https://doi.org/10.1007/s002360050161},
  doi          = {10.1007/S002360050161},
  timestamp    = {Sun, 21 Jun 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/acta/KhardonMR99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {For humans, looking at how concrete examples behave is an intuitive way of deriving conclusions. The drawback with this method is that it does not necessarily give the correct results. However, under certain conditions example-based deduction can be used to obtain a correct and complete inference procedure. This is the case for Boolean formulae (reasoning with models) and for certain types of database integrity constraints (the use of Armstrong relations). We show that these approaches are closely related, and use the relationship to prove new results about the existence and size of Armstrong relations for Boolean dependencies. Furthermore, we exhibit close relations between the questions of finding keys in relational databases and that of finding abductive explanations. Further applications of the correspondence between these two approaches are also discussed.}
}

@article{DBLP:journals/infsof/KlemettinenMT99,
  author       = {Mika Klemettinen and
                  Heikki Mannila and
                  Hannu Toivonen},
  title        = {Interactive exploration of interesting findings in the Telecommunication Network Alarm Sequence Analyzer {(TASA)}},
  journal      = {Inf. Softw. Technol.},
  volume       = {41},
  number       = {9},
  pages        = {557--567},
  year         = {1999},
  url          = {https://doi.org/10.1016/S0950-5849(99)00019-1},
  doi          = {10.1016/S0950-5849(99)00019-1},
  timestamp    = {Thu, 20 Feb 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/infsof/KlemettinenMT99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In this paper we describe the final version of a knowledge discovery system, Telecommunication Network Alarm Sequence Analyzer (TASA), for telecommunication networks alarm data analysis. The system is based on the discovery of recurrent, temporal patterns of alarms in databases; these patterns, episode rules, can be used in the construction of real-time alarm correlation systems. Also association rules are used for identifying relationships between alarm properties. TASA uses a methodology for knowledge discovery in databases (KDD) where one first discovers large collections of patterns at once, and then performs interactive retrievals from the collection of patterns. The proposed methodology suits very well such KDD formalisms as association and episode rules, where large collections of potentially interesting rules can be found efficiently. When searching for the most interesting rules, simple threshold-like restrictions, such as rule frequency and confidence may satisfy a large number of rules. In TASA, this problem can be alleviated by templates and pattern expressions that describe the form of rules that are to be selected or rejected. Using templates the user can flexibly specify the focus of interest, and also iteratively refine it. Different versions of TASA have been in prototype use in four telecommunication companies since the beginning of 1995. TASA has been found useful in, e.g. finding long-term, rather frequently occurring dependencies, creating an overview of a short-term alarm sequence, and evaluating the alarm data base consistency and correctness.}
}

@article{DBLP:journals/jiis/AumannFLM99,
  author       = {Yonatan Aumann and
                  Ronen Feldman and
                  Orly Liphstat and
                  Heikki Mannila},
  title        = {Borders: An Efficient Algorithm for Association Generation in Dynamic Databases},
  journal      = {J. Intell. Inf. Syst.},
  volume       = {12},
  number       = {1},
  pages        = {61--73},
  year         = {1999},
  url          = {https://doi.org/10.1023/A:1026482903537},
  doi          = {10.1023/A:1026482903537},
  timestamp    = {Fri, 26 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/jiis/AumannFLM99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the problem of finding association rules in a database with binary attributes. Most algorithms for finding such rules assume that all the data is available at the start of the data mining session. In practice, the data in the database may change over time, with records being added and deleted. At any given time, the rules for the current set of data are of interest. The naive, and highly inefficient, solution would be to rerun the association generation algorithm from scratch following the arrival of each new batch of data. This paper describes the Borders algorithm, which provides an efficient method for generating associations incrementally, from dynamically changing databases. Experimental results show an improved performance of the new algorithm when compared with previous solutions to the problem.}
}

@article{DBLP:journals/jnsm/KlemettinenMT99,
  author       = {Mika Klemettinen and
                  Heikki Mannila and
                  Hannu Toivonen},
  title        = {Rule Discovery in Telecommunication Alarm Data},
  journal      = {J. Netw. Syst. Manag.},
  volume       = {7},
  number       = {4},
  pages        = {395--423},
  year         = {1999},
  url          = {https://doi.org/10.1023/A:1018787815779},
  doi          = {10.1023/A:1018787815779},
  timestamp    = {Thu, 24 Sep 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/jnsm/KlemettinenMT99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Fault management is an important but difficultarea of telecommunication network management: networksproduce large amounts of alarm information which must beanalyzed and interpreted before faults can be located. So called alarm correlation is acentral technique in fault identification. While the useof alarm correlation systems is quite popular andmethods for expressing the correlations are maturing, acquiring all the knowledge necessary forconstructing an alarm correlation system for a networkand its elements is difficult. We describe a novelpartial solution to the task of knowledge acquisition for correlation systems. We present a methodand a tool for the discovery of recurrent patterns ofalarms in databases; these patterns, episode rules, canbe used in the construction of real-time alarm correlation systems. We also present tools withwhich network management experts can browse the largeamounts of rules produced. The construction ofcorrelation systems becomes easier with these tools, as the episode rules provide a wealth ofstatistical information about recurrent phenomena in thealarm stream. This methodology has been implemented ina research system called TASA, which is used by several telecommunication operators. We briefly discussexperiences in the use of TASA.}
}

@inproceedings{DBLP:conf/dawak/MannilaM99,
  author       = {Heikki Mannila and
                  Pirjo Moen},
  editor       = {Mukesh K. Mohania and
                  A Min Tjoa},
  title        = {Similarity between Event Types in Sequences},
  booktitle    = {Data Warehousing and Knowledge Discovery, First International Conference,
                  DaWaK '99, Florence, Italy, August 30 - September 1, 1999, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1676},
  pages        = {271--280},
  publisher    = {Springer},
  year         = {1999},
  url          = {https://doi.org/10.1007/3-540-48298-9\_29},
  doi          = {10.1007/3-540-48298-9\_29},
  timestamp    = {Tue, 29 Dec 2020 18:32:27 +0100},
  biburl       = {https://dblp.org/rec/conf/dawak/MannilaM99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Similarity or distance between objects is one of the central concepts in data mining. In this paper we consider the following problem: given a set of event sequences, define a useful notion of similarity between the different types of events occurring in the sequences. We approach the problem by considering two event types to be similar if they occur in similar contexts. The context of an occurrence of an event type is defined as the set of types of the events happening within a certain time limit before the occurrence. Then two event types are similar if their sets of contexts are similar. We quantify this by using a simple approach of computing centroids of sets of contexts and using the L1 distance. We present empirical results on telecommunications alarm sequences and student enrollment data, showing that the method produces intuitively appealing results.}
}

@inproceedings{DBLP:conf/dawak/BoulicautKM99,
  author       = {Jean{-}Fran{\c{c}}ois Boulicaut and
                  Mika Klemettinen and
                  Heikki Mannila},
  editor       = {Mukesh K. Mohania and
                  A Min Tjoa},
  title        = {Modeling {KDD} Processes within the Inductive Database Framework},
  booktitle    = {Data Warehousing and Knowledge Discovery, First International Conference,
                  DaWaK '99, Florence, Italy, August 30 - September 1, 1999, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1676},
  pages        = {293--302},
  publisher    = {Springer},
  year         = {1999},
  url          = {https://doi.org/10.1007/3-540-48298-9\_31},
  doi          = {10.1007/3-540-48298-9\_31},
  timestamp    = {Sun, 21 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/dawak/BoulicautKM99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {One of the most challenging problems in data manipulation in the future is to be able to efficiently handle very large databases but also multiple induced properties or generalizations in that data. Popular examples of useful properties are association rules, and inclusion and functional dependencies. Our view of a possible approach for this task is to specify and query inductive databases, which are databases that in addition to data also contain intensionally defined generalizations about the data. We formalize this concept and show how it can be used throughout the whole process of data mining due to the closure property of the framework. We show that simple query languages can be defined using normal database terminology. We demonstrate the use of this framework to model typical data mining processes. It is then possible to perform various tasks on these descriptions like, e.g., optimizing the selection of interesting properties or comparing two processes.}
}

@inproceedings{DBLP:conf/ilp/Mannila99,
  author       = {Heikki Mannila},
  editor       = {Saso Dzeroski and
                  Peter A. Flach},
  title        = {Inductive Databases},
  booktitle    = {Inductive Logic Programming, 9th International Workshop, ILP-99, Bled,
                  Slovenia, June 24-27, 1999, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1634},
  pages        = {14},
  publisher    = {Springer},
  year         = {1999},
%  url          = {https://doi.org/10.1007/3-540-48751-4\_2},
  doi          = {10.1007/3-540-48751-4\_2},
  timestamp    = {Tue, 14 May 2019 10:00:36 +0200},
  biburl       = {https://dblp.org/rec/conf/ilp/Mannila99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining aims at trying to locate interesting patterns or regularities from large masses of data. Data mining can be viewed as part of a data analysis or knowledge management. In data analysis tasks one can see a continuous spectrum of information needs, starting from very simple database queries ("what is the address of customer NN"), moving to more complex aggregate information ("what are the sales by product groups and regions") to data mining type of queries ("give me interesting trends on sales"). This suggests that it is useful to view data mining as querying the theory of the database, i.e., the set of sentences that are true in the database. An inductive database is a database that conceptually contains in addition to normal data also all the generalizations of the data from a given language of descriptors. Inductive databases can be viewed as analogues to deductive databases: deductive databases conceptually contain all the facts derivable from the data and the rules. In this talk I describe a formal framework for inductive databases and discuss some theoretical and practical problems in the area.}
}

@inproceedings{DBLP:conf/kdd/MannilaPS99,
  author       = {Heikki Mannila and
                  Dmitry Pavlov and
                  Padhraic Smyth},
  editor       = {Usama M. Fayyad and
                  Surajit Chaudhuri and
                  David Madigan},
  title        = {Prediction with Local Patterns using Cross-Entropy},
  booktitle    = {Proceedings of the Fifth {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, San Diego, CA, USA, August 15-18,
                  1999},
  pages        = {357--361},
  publisher    = {{ACM}},
  year         = {1999},
  url          = {https://doi.org/10.1145/312129.312281},
  doi          = {10.1145/312129.312281},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaPS99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sets of local patterns in the forms of rules and co-occurrence counts are produced by many data mining methods such as association rule algorithms. While such patterns can yield useful insights it is not obvious how to synthesize local sparse information into a coherent global predictive model. We study the use of a cross-entropy approach to combining local patterns. Each local pattern is viewed as a constraint on an appropriate high-order joint distribution of interest. Typically, a set of patterns returned by a data mining algorithm under-constrains the high-order model. The cross-entropy criterion is used to select a specific distribution in this constrained family relative to a prior. We review the iterative-scaling algorithm which is an iterative technique for finding a joint distribution given constraints. We then illustrate the application of this method to two specific problems. The first problem is combining information about frequent itemsets. We show that the cross-entropy approach can be used for query selectivity estimation for 0/1 data sets. The results show that we can accurately answer a large class of queries using just a small set of aggregate information. The second problem involves sequence modeling using historical rules, with an application to protein sequences. We conclude that viewing local patterns as constraints on a high-order probability model is a useful and principled framework for prediction based on large sets of mined patterns.}
}

@inproceedings{DBLP:conf/pkdd/KlemettinenMV99,
  author       = {Mika Klemettinen and
                  Heikki Mannila and
                  A. Inkeri Verkamo},
  editor       = {Jan M. Zytkow and
                  Jan Rauch},
  title        = {Association Rule Selection in a Data Mining Environment},
  booktitle    = {Principles of Data Mining and Knowledge Discovery, Third European
                  Conference, {PKDD} '99, Prague, Czech Republic, September 15-18, 1999,
                  Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1704},
  pages        = {372--377},
  publisher    = {Springer},
  year         = {1999},
  url          = {https://doi.org/10.1007/978-3-540-48247-5\_45},
  doi          = {10.1007/978-3-540-48247-5\_45},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/KlemettinenMV99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining methods easily produce large collections of rules, so that the usability of the methods is hampered by the sheer size of the rule set. One way of limiting the size of the result set is to provide the user with tools to help in finding the truly interesting rules. We use this approach in a case study where we search for association rules in NCHS health care data, and select interesting subsets of the result by using a simple query language implemented in the KESO data mining system. Our results emphasize the importance of the explorative approach supported by efficient selection tools.}
}

@inproceedings{DBLP:conf/compstat/EerolaMS98,
  author       = {Mervi Eerola and
                  Heikki Mannila and
                  Marko Salmenkivi},
  editor       = {Roger Payne and
                  Peter Green},
  title        = {Frailty Factors and Time-dependent Hazards in Modelling Ear Infections in Children Using {BASSIST}},
  booktitle    = {{COMPSTAT} 1998, Proceedings in Computational Statistics 13th Symposium
                  held in Bristol, Great Britain, 1998},
  pages        = {287--292},
  publisher    = {Springer},
  year         = {1998},
  url          = {https://doi.org/10.1007/978-3-662-01131-7\_36},
  doi          = {10.1007/978-3-662-01131-7\_36},
  timestamp    = {Wed, 29 Apr 2020 19:28:37 +0200},
  biburl       = {https://dblp.org/rec/conf/compstat/EerolaMS98.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The BASSIST system is a general purpose tool for MCMC sampling for intensity models. The system allows the user to specify an intensity model in a high-level language. The model is used to generate a simulation program that uses the Metropolis-Hastings algorithm to obtain the desired samples. In contrast to BUGS (Spiegelhalter et al., 1996), BASSIST contains several primitives that are suited for modelling event data, including piecewise constant functions etc.}
}

@inproceedings{DBLP:conf/dis/MannilaTKO98,
  author       = {Heikki Mannila and
                  Hannu Toivonen and
                  Atte Korhola and
                  Heikki Olander},
  editor       = {Setsuo Arikawa and
                  Hiroshi Motoda},
  title        = {Learning, Mining, or Modeling? {A} Case Study from Paleocology},
  booktitle    = {Discovery Science, First International Conference, {DS} '98, Fukuoka,
                  Japan, December 14-16, 1998, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1532},
  pages        = {12--24},
  publisher    = {Springer},
  year         = {1998},
  url          = {https://doi.org/10.1007/3-540-49292-5\_2},
  doi          = {10.1007/3-540-49292-5\_2},
  timestamp    = {Thu, 14 Oct 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/dis/MannilaTKO98.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Exploratory data mining, machine learning, and statistical modeling all have a role in discovery science. We describe a paleoecological reconstruction problem where Bayesian methods are useful and allow plausible inferences from the small and vague data sets available. Paleoecological reconstruction aims at estimating temperatures in the past. Knowledge about present day abundances of certain species are combined with data about the same species in fossil assemblages (e.g., lake sediments). Stated formally, the reconstruction task has the form of a typical machine learning problem. However, to obtain useful predictions, a lot of background knowledge about ecological variation is needed. In paleoecological literature the statistical methods are involved variations of regression. We compare these methods with regression trees, nearest neighbor methods, and Bayesian hierarchical models. All the methods achieve about the same prediction accuracy on modern specimens, but the Bayesian methods and the involved regression methods seem to yield the best reconstructions. The advantage of the Bayesian methods is that they also give good estimates on the variability of the reconstructions.}
}

@inproceedings{DBLP:conf/kdd/DasLMRS98,
  author       = {Gautam Das and
                  King{-}Ip Lin and
                  Heikki Mannila and
                  Gopal Renganathan and
                  Padhraic Smyth},
  editor       = {Rakesh Agrawal and
                  Paul E. Stolorz and
                  Gregory Piatetsky{-}Shapiro},
  title        = {Rule Discovery from Time Series},
  booktitle    = {Proceedings of the Fourth International Conference on Knowledge Discovery
                  and Data Mining (KDD-98), New York City, New York, USA, August 27-31,
                  1998},
  pages        = {16--22},
  publisher    = {{AAAI} Press},
  year         = {1998},
  url          = {http://www.aaai.org/Library/KDD/1998/kdd98-003.php},
  timestamp    = {Fri, 18 May 2018 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/DasLMRS98.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the problem of finding rules relating patterns in a time series to other patterns in that series, or patterns in one series to patterns in another series. A simple example is a rule such as "a period of low telephone call activity is usually followed by a sharp rise in call volume". Examples of rules relating two or more time series are "if the Microsoft stock price goes up and Intel falls, then IBM goes up the next day," and "if Microsoft goes up strongly for one day, then declines strongly on the next day, and on the same days Intel stays about level, then IBM stays about level." Our emphasis is in the discovery of local patterns in multivariate time series, in contrast to traditional time series analysis which largely focuses on global models. Thus, we search for rules whose conditions refer to patterns in time series. However, we do not want to define beforehand which patterns are to be used; rather, we want the patterns to be formed from the data in the context of rule discovery. We describe adaptive methods for finding rules of the above type from time-series data. The methods are based on discretizing the sequence by methods resembling vector quantization. We first form subsequences by sliding a window through the time series, and then cluster these subsequences by using a suitable measure of time-series similarity. The discretized version of the time series is obtained by taking the cluster identifiers corresponding to the subsequence. Once the time-series is discretized, we use simple rule finding methods to obtain rules from the sequence. We present empirical results on the behavior of the method.}
}

@inproceedings{DBLP:conf/kdd/DasMR98,
  author       = {Gautam Das and
                  Heikki Mannila and
                  Pirjo Ronkainen},
  editor       = {Rakesh Agrawal and
                  Paul E. Stolorz and
                  Gregory Piatetsky{-}Shapiro},
  title        = {Similarity of Attributes by External Probes},
  booktitle    = {Proceedings of the Fourth International Conference on Knowledge Discovery
                  and Data Mining (KDD-98), New York City, New York, USA, August 27-31,
                  1998},
  pages        = {23--29},
  publisher    = {{AAAI} Press},
  year         = {1998},
  url          = {http://www.aaai.org/Library/KDD/1998/kdd98-004.php},
  timestamp    = {Fri, 18 May 2018 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/DasMR98.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In data mining, similarity or distance between attributes is one of the central notions. Such a notion can be used to build attribute hierarchies etc. Similarity metrics can be user-defined, but an important problem is defining similarity on the basis of data. Several methods based on statistical techniques exist. For defining the similarity between two attributes A and B they typically consider only the values of A and B, not the other attributes. We describe how a similarity notion between attributes can be defined by considering the values of other attributes. The basic idea is that in a 0/1 relation r, two attributes A and B are similar if the subrelations aA= l (r) and aB=~(r) are similar. Similarity between the two relations is defined by considering the marginal frequencies of a selected subset of other attributes. We show that the framework produces natural notions of similarity. Empirical results on the Reuters-21578 document dataset show, for example, how natural classifications for countries can be discovered from keyword distributions in documents. The similarity notion is easily computable with scalable algorithms.}
}

@inproceedings{DBLP:conf/pkdd/BoulicautKM98,
  author       = {Jean{-}Fran{\c{c}}ois Boulicaut and
                  Mika Klemettinen and
                  Heikki Mannila},
  editor       = {Jan M. Zytkow and
                  Mohamed Quafafou},
  title        = {Querying Inductive Databases: {A} Case Study on the {MINE} {RULE} Operator},
  booktitle    = {Principles of Data Mining and Knowledge Discovery, Second European
                  Symposium, {PKDD} '98, Nantes, France, September 23-26, 1998, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1510},
  pages        = {194--202},
  publisher    = {Springer},
  year         = {1998},
  url          = {https://doi.org/10.1007/BFb0094820},
  doi          = {10.1007/BFB0094820},
  timestamp    = {Tue, 14 May 2019 10:00:47 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/BoulicautKM98.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Knowledge discovery in databases (KDD) is a process that can include steps like forming the data set, data transformations, discovery of patterns, searching for exceptions to a pattern, zooming on a subset of the data, and postprocessing some patterns. We describe a comprehensive framework in which all these steps can be carried out by means of queries over an inductive database. An inductive database is a database that in addition to data also contains intensionally defined generalizations about the data. We formalize this concept: an inductive database consists of a normal database together with a subset of patterns from a class of patterns, and an evaluation function that tells how the patterns occur in the data. Then, looking for potential query languages built on top of SQL, we consider the research on the MINE RULE operator by Meo, Psaila and Ceri. It is a serious step towards an implementation framework for inductive databases, though it addresses only the association rule mining problem. Perspectives are then discussed.}
}

@article{DBLP:journals/acta/EiterM97,
  author       = {Thomas Eiter and
                  Heikki Mannila},
  title        = {Distance Measures for Point Sets and their Computation},
  journal      = {Acta Informatica},
  volume       = {34},
  number       = {2},
  pages        = {109--133},
  year         = {1997},
  url          = {https://doi.org/10.1007/s002360050075},
  doi          = {10.1007/S002360050075},
  timestamp    = {Sun, 21 Jun 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/acta/EiterM97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the problem of measuring the similarity or distance between two finite sets of points in a metric space, and computing the measure. This problem has applications in, e.g., computational geometry, philosophy of science, updating or changing theories, and machine learning. We review some of the distance functions proposed in the literature, among them the minimum distance link measure, the surjection measure, and the fair surjection measure, and supply polynomial time algorithms for the computation of these measures. Furthermore, we introduce the minimum link measure, a new distance function which is more appealing than the other distance functions mentioned. We also present a polynomial time algorithm for computing this new measure. We further address the issue of defining a metric on point sets. We present the metric infimum method that constructs a metric from any distance functions on point sets. In particular, the metric infimum of the minimum link measure is a quite intuitive. The computation of this measure is shown to be in NP for a broad class of instances; it is NP-hard for a natural problem class.}
}

@article{DBLP:journals/datamine/MannilaT97,
  author       = {Heikki Mannila and
                  Hannu Toivonen},
  title        = {Levelwise Search and Borders of Theories in Knowledge Discovery},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {1},
  number       = {3},
  pages        = {241--258},
  year         = {1997},
  url          = {https://doi.org/10.1023/A:1009796218281},
  doi          = {10.1023/A:1009796218281},
  timestamp    = {Mon, 05 Jun 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/datamine/MannilaT97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {One of the basic problems in knowledge discovery in databases (KDD) is the following: given a data set r, a class L of sentences for defining subgroups of r, and a selection predicate, find all sentences of L deemed interesting by the selection predicate. We analyze the simple levelwise algorithm for finding all such descriptions. We give bounds for the number of database accesses that the algorithm makes. For this, we introduce the concept of the border of a theory, a notion that turns out to be surprisingly powerful in analyzing the algorithm. We also consider the verification problem of a KDD process: given r and a set of sentences S  L determine whether S is exactly the set of interesting statements about r. We show strong connections between the verification problem and the hypergraph transversal problem. The verification problem arises in a natural way when using sampling to speed up the pattern discovery step in KDD.}
}

@article{DBLP:journals/datamine/MannilaTV97,
  author       = {Heikki Mannila and
                  Hannu Toivonen and
                  A. Inkeri Verkamo},
  title        = {Discovery of Frequent Episodes in Event Sequences},
  journal      = {Data Min. Knowl. Discov.},
  volume       = {1},
  number       = {3},
  pages        = {259--289},
  year         = {1997},
  url          = {https://doi.org/10.1023/A:1009748302351},
  doi          = {10.1023/A:1009748302351},
  timestamp    = {Mon, 05 Jun 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/datamine/MannilaTV97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events describing the behavior and actions of users or systems can be collected in several domains. An episode is a collection of events that occur relatively close to each other in a given partial order. We consider the problem of discovering frequently occurring episodes in a sequence. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We give efficient algorithms for the discovery of all frequent episodes from a given class of episodes, and present detailed experimental results. The methods are in use in telecommunication alarm management.}
}

@article{DBLP:journals/tods/EiterGM97,
  author       = {Thomas Eiter and
                  Georg Gottlob and
                  Heikki Mannila},
  title        = {Disjunctive Datalog},
  journal      = {{ACM} Trans. Database Syst.},
  volume       = {22},
  number       = {3},
  pages        = {364--418},
  year         = {1997},
  url          = {https://doi.org/10.1145/261124.261126},
  doi          = {10.1145/261124.261126},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/tods/EiterGM97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider disjunctive Datalog, a powerful database query language based on disjunctive logic programming. Briefly, disjunctive Datalog is a variant of Datalog where disjunctions may appear in the rule heads; advanced versions also allow for negation in the bodies which can be handled according to a semantics for negation in disjunctive logic programming. In particular, we investigate three different semantics for disjunctive Datalog: the minimal model semantics the perfect model semantics, and the stable model semantics. For each of these semantics, the expressive power and complexity are studied. We show that the possibility variants of these semantics express the same set of queries. In fact, they precisely capture the complexity class ΣP2. Thus, unless the Polynomial Hierarchy collapses, disjunctive Datalog is more expressive that normal logic programming with negation. These results are not only of theoretical interest; we demonstrate that problems relevant in practice such as computing the optimal tour value in the Traveling Salesman Problem and eigenvector computations can be handled in disjunctive Datalog, but not Datalog with negation (unless the Polynomial Hierarchy collapses). In addition, we study modularity properties of disjunctive Datalog and investigate syntactic restrictions of the formalisms.}
}

@inproceedings{DBLP:conf/compgeom/BollobasDGM97,
  author       = {B{\'{e}}la Bollob{\'{a}}s and
                  Gautam Das and
                  Dimitrios Gunopulos and
                  Heikki Mannila},
  editor       = {Jean{-}Daniel Boissonnat},
  title        = {Time-Series Similarity Problems and Well-Separated Geometric Sets},
  booktitle    = {Proceedings of the Thirteenth Annual Symposium on Computational Geometry,
                  Nice, France, June 4-6, 1997},
  pages        = {454--456},
  publisher    = {{ACM}},
  year         = {1997},
  url          = {https://doi.org/10.1145/262839.263080},
  doi          = {10.1145/262839.263080},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/compgeom/BollobasDGM97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Given a pair of nonidentical complex objects, defusing (and determining g) how similar they are toeach other is a nontrivial problem. In data mining applications, one frequently needs todetermine the similaritybetween two time series. We analyze a model of time-series similarity that allows outliers, and different scaling functions. We presentdeterministic and randomized algorithms for computing this notion of similarity. The algorithms are based on nontrivial tools and methods from computational geometry. In particular, we use properties of families of well-separated geometric sets. The randomized algorithm has provably good performanceand also works extremely efficiently in practice.}
}

@inproceedings{DBLP:conf/dexaw/KlemettinenMT97,
  author       = {Mika Klemettinen and
                  Heikki Mannila and
                  Hannu Toivonen},
  editor       = {Roland R. Wagner},
  title        = {A Data Mining Methodology and Its Application to Semi-automatic Knowledge Acquisition},
  booktitle    = {Eighth International Workshop on Database and Expert Systems Applications,
                  {DEXA} '97, Toulouse, France, September 1-2, 1997, Proceedings},
  pages        = {670--677},
  publisher    = {{IEEE} Computer Society},
  year         = {1997},
  url          = {https://doi.org/10.1109/DEXA.1997.617410},
  doi          = {10.1109/DEXA.1997.617410},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/dexaw/KlemettinenMT97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We introduce a methodology for knowledge discovery in databases (KDD) where one first discovers large collections of patterns at once, and then performs interactively retrieves subsets of the collection of patterns. The proposed methodology suits such KDD formalisms as association and episode rules, where large collections of potentially interesting rules can be found efficiently. We present methods that support interactive exploration of large collections of rules. With these methods the user can flexibly specify the focus of interest, and also iteratively refine it. We have implemented our methodology in the TASA system which discovers patterns in telecommunication alarm databases. We give concrete examples of how to use frequent patterns in the construction of alarm correlation expert systems.}
}

@inproceedings{DBLP:conf/dmkd/FeldmanAAM97,
  author       = {Ronen Feldman and
                  Yonatan Aumann and
                  Amihood Amir and
                  Heikki Mannila},
  title        = {Efficient Algorithms for Discovering Frequent Sets in Incremental Databases},
  booktitle    = {Workshop on Research Issues on Data Mining and Knowledge Discovery,
                  {DMKD} 1997 in cooperation with {ACM} SIGMOD'97, Tucson, Arizona,
                  USA, May 11, 1997},
  year         = {1997},
  timestamp    = {Wed, 07 Aug 2019 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/dmkd/FeldmanAAM97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Efficient Algorithms for Discovering Frequent Sets in Incremental Databases}
}

@inproceedings{DBLP:conf/icdt/Mannila97,
  author       = {Heikki Mannila},
  editor       = {Foto N. Afrati and
                  Phokion G. Kolaitis},
  title        = {Methods and Problems in Data Mining},
  booktitle    = {Database Theory - {ICDT} '97, 6th International Conference, Delphi,
                  Greece, January 8-10, 1997, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1186},
  pages        = {41--55},
  publisher    = {Springer},
  year         = {1997},
  url          = {https://doi.org/10.1007/3-540-62222-5\_35},
  doi          = {10.1007/3-540-62222-5\_35},
  timestamp    = {Tue, 14 May 2019 10:00:54 +0200},
  biburl       = {https://dblp.org/rec/conf/icdt/Mannila97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. We consider some methods used in data mining, concentrating on levelwise search for all frequently occurring patterns. We show how this technique can be used in various applications. We also discuss possibilities for compiling data mining queries into algorithms, and look at the use of sampling in data mining. We conclude by listing several open research problems in data mining and knowledge discovery.}
}

@inproceedings{DBLP:conf/icdt/GunopoulosMS97,
  author       = {Dimitrios Gunopulos and
                  Heikki Mannila and
                  Sanjeev Saluja},
  editor       = {Foto N. Afrati and
                  Phokion G. Kolaitis},
  title        = {Discovering All Most Specific Sentences by Randomized Algorithms},
  booktitle    = {Database Theory - {ICDT} '97, 6th International Conference, Delphi,
                  Greece, January 8-10, 1997, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1186},
  pages        = {215--229},
  publisher    = {Springer},
  year         = {1997},
  url          = {https://doi.org/10.1007/3-540-62222-5\_47},
  doi          = {10.1007/3-540-62222-5\_47},
  timestamp    = {Thu, 14 Oct 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/icdt/GunopoulosMS97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Data mining can in many instances be viewed as the task of computing a representation of a theory of a model or a database. In this paper we present a randomized algorithm that can be used to compute the representation of a theory in terms of the most specific sentences of that theory. In addition to randomization, the algorithm uses a generalization of the concept of hypergraph transversal. We apply the general algorithm, for discovering maximal frequent sets in 0/1 data, and for computing minimal keys in relations. We present some empirical results on the performance of these methods on real data. We also show some complexity theoretic evidence of the hardness of these problems.}
}

@inproceedings{DBLP:conf/pkdd/DasGM97,
  author       = {Gautam Das and
                  Dimitrios Gunopulos and
                  Heikki Mannila},
  editor       = {Henryk Jan Komorowski and
                  Jan M. Zytkow},
  title        = {Finding Similar Time Series},
  booktitle    = {Principles of Data Mining and Knowledge Discovery, First European
                  Symposium, {PKDD} '97, Trondheim, Norway, June 24-27, 1997, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1263},
  pages        = {88--100},
  publisher    = {Springer},
  year         = {1997},
  url          = {https://doi.org/10.1007/3-540-63223-9\_109},
  doi          = {10.1007/3-540-63223-9\_109},
  timestamp    = {Mon, 05 Feb 2024 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pkdd/DasGM97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Similarity of objects is one of the crucial concepts in several applications, including data mining. For complex objects, similarity is nontrivial to define. In this paper we present an intuitive model for measuring the similarity between two time series. The model takes into account outliers, different scaling functions, and variable sampling rates. Using methods from computational geometry, we show that this notion of similarity can be computed in polynomial time. Using statistical approximation techniques, the algorithms can be speeded up considerably. We give preliminary experimental results that show the naturalness of the notion.}
}

@inproceedings{DBLP:conf/pods/GunopulosKMT97,
  author       = {Dimitrios Gunopulos and
                  Roni Khardon and
                  Heikki Mannila and
                  Hannu Toivonen},
  editor       = {Alberto O. Mendelzon and
                  Z. Meral {\"{O}}zsoyoglu},
  title        = {Data mining, Hypergraph Transversals, and Machine Learning},
  booktitle    = {Proceedings of the Sixteenth {ACM} {SIGACT-SIGMOD-SIGART} Symposium
                  on Principles of Database Systems, May 12-14, 1997, Tucson, Arizona,
                  {USA}},
  pages        = {209--216},
  publisher    = {{ACM} Press},
  year         = {1997},
  url          = {https://doi.org/10.1145/263661.263684},
  doi          = {10.1145/263661.263684},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pods/GunopulosKMT97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Several data mining problems can be formulated as problems of finding maximally specific sentences that are interesting in a database. We first show that this problem has a close relationship with the hypergraph transversal problem. We then analyze two algorithms that have been previously used in data mining, proving upper bounds on their complexity. The first algorithm is useful when the maximally specific interesting sentences are “small”. We show that this algorithm can also be used to efficiently solve a special case of the hypergraph transversal problem, improving on previous results. The second algorithm utilizes a subroutine for hypergraph transversals, and is applicable in more general situations, with complexity close to a lower bound for the problem. We also relate these problems to the model of exact learning in computational learning theory, and use the correspondence to derive some corollaries.}
}

@inproceedings{DBLP:conf/slp/Mannila97,
  author       = {Heikki Mannila},
  editor       = {Jan Maluszynski},
  title        = {Inductive Databases and Condensed Representations for Data Mining},
  booktitle    = {Logic Programming, Proceedings of the 1997 International Symposium,
                  Port Jefferson, Long Island, NY, USA, October 13-16, 1997},
  pages        = {21--30},
  publisher    = {{MIT} Press},
  year         = {1997},
  timestamp    = {Fri, 10 Jul 2015 12:20:33 +0200},
  biburl       = {https://dblp.org/rec/conf/slp/Mannila97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. It can be argued that several data mining tasks consist of locating interesting sentences from a given logic that are true in the database. Then the task of the user/analyst is to is to query this set, the theory of the database. This view gives rise to the concept of of inductive databases, ie, databases that in addition to the data contain also inductive generalizations about the data. We describe a rough framework for inductive databases, and consider also condensed representations, data structures that make it possible to answer queries about the inductive database approximately correctly and reasonably efficiently.}
}

@inproceedings{DBLP:conf/time/MannilaR97,
  author       = {Heikki Mannila and
                  Pirjo Ronkainen},
  title        = {Similarity of Event Sequences},
  booktitle    = {4th International Workshop on Temporal Representation and Reasoning,
                  {TIME} '97, Daytona Beach, Florida, USA, May 10-11, 1997},
  pages        = {136--139},
  publisher    = {{IEEE} Computer Society},
  year         = {1997},
  url          = {https://doi.org/10.1109/TIME.1997.600793},
  doi          = {10.1109/TIME.1997.600793},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/time/MannilaR97.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events are an important form of data that occurs in many application domains, such as telecommunications, biostatistics, user interface design, etc. We present a simple model for measuring the similarity of event sequences, and show that the resulting measure of distance can be efficiently computed using a form of dynamic programming.}
}

@article{DBLP:journals/cacm/ImielinskiM96,
  author       = {Tomasz Imielinski and
                  Heikki Mannila},
  title        = {A Database Perspective on Knowledge Discovery},
  journal      = {Commun. {ACM}},
  volume       = {39},
  number       = {11},
  pages        = {58--64},
  year         = {1996},
  url          = {https://doi.org/10.1145/240455.240472},
  doi          = {10.1145/240455.240472},
  timestamp    = {Mon, 03 Mar 2025 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/cacm/ImielinskiM96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The concept of data mining as a querying process and the first steps toward efficient development of knowledge discovery applications are discussed.}
}

@inproceedings{DBLP:conf/er/Mannila96,
  author       = {Heikki Mannila},
  editor       = {Bernhard Thalheim},
  title        = {Schema Design and Knowledge Discovery},
  booktitle    = {Conceptual Modeling - ER'96, 15th International Conference on Conceptual
                  Modeling, Cottbus, Germany, October 7-10, 1996, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {1157},
  pages        = {27},
  publisher    = {Springer},
  year         = {1996},
  url          = {https://doi.org/10.1007/BFb0019912},
  doi          = {10.1007/BFB0019912},
  timestamp    = {Tue, 14 May 2019 10:00:50 +0200},
  biburl       = {https://dblp.org/rec/conf/er/Mannila96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Knowledge discovery in databases (KDD), often also referred to as data mining, aims at the discovery of useful information from large masses of data. In this talk we review some issues in the intersection of schema design and knowledge discovery, both in the relational database design and in text databases.}
}

@inproceedings{DBLP:conf/icde/HatonenKMRT96,
  author       = {Kimmo H{\"{a}}t{\"{o}}nen and
                  Mika Klemettinen and
                  Heikki Mannila and
                  Pirjo Ronkainen and
                  Hannu Toivonen},
  editor       = {Stanley Y. W. Su},
  title        = {Knowledge Discovery from Telecommunication Network Alarm Databases},
  booktitle    = {Proceedings of the Twelfth International Conference on Data Engineering,
                  February 26 - March 1, 1996, New Orleans, Louisiana, {USA}},
  pages        = {115--122},
  publisher    = {{IEEE} Computer Society},
  year         = {1996},
  url          = {https://doi.org/10.1109/ICDE.1996.492095},
  doi          = {10.1109/ICDE.1996.492095},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icde/HatonenKMRT96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A telecommunication network produces daily large amounts of alarm data. The data contains hidden valuable knowledge about the behavior of the network. This knowledge can be used in filtering redundant alarms, locating problems in, the network, and possibly in predicting severe faults. We describe the TASA (Telecommunication Network Alarm Sequence Analyzer) system for discovering and browsing knowledge from large alarm databases. The system is built on the basis of viewing knowledge discovery as an interactive and iterative process, containing data collection, pattern discovery, rule postprocessing, etc. The system uses a novel framework for locating frequently occurring episodes from sequential data. The TASA system offers a variety of selection and ordering criteria for episodes, and supports iterative retrieval from the discovered knowledge. This means that a large part of the iterative nature of the KDD process can be replaced by iteration in the rule postprocessing stage. The user interface is based on dynamically generated HTML. The system is in experimental use, and the results are encouraging: some of the discovered knowledge is being integrated into the alarm handling software of telecommunication operators.}
}

@inproceedings{DBLP:conf/kdd/MannilaT96,
  author       = {Heikki Mannila and
                  Hannu Toivonen},
  editor       = {Evangelos Simoudis and
                  Jiawei Han and
                  Usama M. Fayyad},
  title        = {Discovering Generalized Episodes Using Minimal Occurrences},
  booktitle    = {Proceedings of the Second International Conference on Knowledge Discovery
                  and Data Mining (KDD-96), Portland, Oregon, {USA}},
  pages        = {146--151},
  publisher    = {{AAAI} Press},
  year         = {1996},
  url          = {http://www.aaai.org/Library/KDD/1996/kdd96-024.php},
  timestamp    = {Mon, 05 Jun 2017 13:20:21 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaT96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events are an important special form of data that arises in several contexts, including telecommunications, user interface studies, and epidemiology. We present a general and flexible framework of specifying classes of generalized episodes. These are recurrent combinations of events satisfying certain conditions. The framework can be instantiated to a wide variety of applications by selecting suitable primitive conditions. We present algorithms for discovering frequently occurring episodes and episode rules. The algorithms are based on the use of minimal occurrences of episodes; this makes it possible to evaluate confidences of a wide variety of rules using only a single analysis pass. We present empirical results on the behavior of the algorithms on events stemming from a WWW log.}
}

@inproceedings{DBLP:conf/kdd/MannilaT96a,
  author       = {Heikki Mannila and
                  Hannu Toivonen},
  editor       = {Evangelos Simoudis and
                  Jiawei Han and
                  Usama M. Fayyad},
  title        = {Multiple Uses of Frequent Sets and Condensed Representations},
  booktitle    = {Proceedings of the Second International Conference on Knowledge Discovery
                  and Data Mining (KDD-96), Portland, Oregon, {USA}},
  pages        = {189--194},
  publisher    = {{AAAI} Press},
  year         = {1996},
  url          = {http://www.aaai.org/Library/KDD/1996/kdd96-031.php},
  timestamp    = {Wed, 12 Dec 2012 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaT96a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In interactive data mining it is advantageous to have condensed representations of data that can be used to efficiently answer different queries. In this paper we show how frequent sets can be used as a condensed representation for answering various types of queries.
  Given a table r with O/i vaiues and a threshoid 6, a frequent set of r is a set X of columns of r such that at least a fraction u of the rows of r have a 1 in all the columns of X. Finding frequent sets is a first step in finding association rules, and there exists several efficient algorithms for finding the frequent sets. We show that frequent sets have wider applications than just finding association rules.}
}

@inproceedings{DBLP:conf/noms/HatonenKMRT96,
  author       = {Kimmo H{\"{a}}t{\"{o}}nen and
                  Mika Klemettinen and
                  Heikki Mannila and
                  Pirjo Ronkainen and
                  Hannu Toivonen},
  editor       = {Masayoshi Ejiri and
                  Shri K. Goyal},
  title        = {{TASA:} Telecommunication Alarm Sequence Analyzer or how to enjoy faults in your network},
  booktitle    = {1996 Network Operations and Management Symposium, {NOMS} 1996, Kyoto,
                  Japan, April 15-19, 1996. Proceedings},
  pages        = {520--529},
  publisher    = {{IEEE}},
  year         = {1996},
  url          = {https://doi.org/10.1109/NOMS.1996.539622},
  doi          = {10.1109/NOMS.1996.539622},
  timestamp    = {Wed, 16 Oct 2019 14:14:54 +0200},
  biburl       = {https://dblp.org/rec/conf/noms/HatonenKMRT96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Today's large and complex telecommunication networks produce large amounts of alarms daily. The sequence of alarms contains valuable knowledge about the behavior of the network, but much of the knowledge is fragmented and hidden in the vast amount of data. Regularities in the alarms can be used in fault management applications, e.g., for filtering redundant alarms, locating problems in the network, and possibly in predicting severe faults. In this paper we describe TASA (Telecommunication Alarm Sequence Analyzer), a novel system for discovering interesting regularities in the alarms. In the core of the system are algorithms for locating frequent alarm episodes from the alarm stream and presenting them as rules. Discovered rules can then be explored with flexible information retrieval tools that support iteration. The user interface is hypertext, based on HTML, and can be used with a standard WWW browser. TASA is in experimental use and has already discovered rules that have been integrated into the alarm handling software of an operator.}
}

@inproceedings{DBLP:conf/ssdbm/Mannila96,
  author       = {Heikki Mannila},
  editor       = {Per Svensson and
                  James C. French},
  title        = {Data Mining: Machine Learning, Statistics, and Databases},
  booktitle    = {Proceedings of the Eighth International Conference on Scientific and
                  Statistical Database Management, Stockholm, Sweden, June 18-20, 1996},
  pages        = {2--9},
  publisher    = {{IEEE} Computer Society},
  year         = {1996},
  url          = {https://doi.org/10.1109/SSDM.1996.505910},
  doi          = {10.1109/SSDM.1996.505910},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/ssdbm/Mannila96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. We give an overview of the area and present some of the research issues, especially from the database angle.}
}

@incollection{DBLP:books/mit/fayyadPSU96/AgrawalMSTV96,
  author       = {Rakesh Agrawal and
                  Heikki Mannila and
                  Ramakrishnan Srikant and
                  Hannu Toivonen and
                  A. Inkeri Verkamo},
  editor       = {Usama M. Fayyad and
                  Gregory Piatetsky{-}Shapiro and
                  Padhraic Smyth and
                  Ramasamy Uthurusamy},
  title        = {Fast Discovery of Association Rules},
  booktitle    = {Advances in Knowledge Discovery and Data Mining},
  pages        = {307--328},
  publisher    = {{AAAI/MIT} Press},
  year         = {1996},
  timestamp    = {Sat, 03 Aug 2019 19:21:51 +0200},
  biburl       = {https://dblp.org/rec/books/mit/fayyadPSU96/AgrawalMSTV96.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Association rules are statements of the form "98% of customers that purchase tires and automobile accessories also get automotive services." We consider the problem of discovering association rules between items in large databases. We present two new algorithms for solving this problem. Experiments with synthetic data show that these algorithms outperform previous algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We show how the best features of the two proposed algorithms can be combined into a hybrid algorithm. Scale-up experiments show that the hybrid algorithm scales linearly with the number of transactions and that it has excellent scale-up properties with respect to the transaction size and the number of items in the database. We also give simple information-theoretic lower bounds for the problem of finding association rules, and show that sampling can be in some cases an efficient way of finding such rules.}
}

@article{DBLP:journals/dam/EiterKM95,
  author       = {Thomas Eiter and
                  Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  title        = {Recognizing Renamable Generalized Propositional Horn Formulas Is NP-complete},
  journal      = {Discret. Appl. Math.},
  volume       = {59},
  number       = {1},
  pages        = {23--31},
  year         = {1995},
  url          = {https://doi.org/10.1016/0166-218X(93)E0152-O},
  doi          = {10.1016/0166-218X(93)E0152-O},
  timestamp    = {Thu, 11 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/dam/EiterKM95.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Yamasaki and Doshita (1983) have defined an extension of the class of propositional Horn formulas; later, Gallo and Scutellà (1988) generalized this class to a hierarchy Γo ⊂- Γ1 ⊂- … ⊂- Γk ⊂- …, where Γo is the set of Horn formulas and Γ1 is the class of Yamasaki and Doshita. For any fixed k, the propositional formulas in Γk can be recognized in polynomial time, and the satisfiability problem for Γk formulas can be solved in polynomial time. A possible way of extending these tractable subclasses of the satisfiability problem is to consider renamings: a renaming of a formula is obtained by replacing for some variables all their positive occurrences by negative occurrences and vice versa. The class of renamings of Horn formulas can be recognized in linear time. Chandru et al. (1990) have posed the problem of deciding whether the renamings of Γ1 formulas can be recognized efficiently. We show that this is probably not the case by proving the NP-completeness of recognizing the renamings of Γk formulas for any k ⩾ 1.}
}

@article{DBLP:journals/siamcomp/KilpelainenM95,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  title        = {Ordered and Unordered Tree Inclusion},
  journal      = {{SIAM} J. Comput.},
  volume       = {24},
  number       = {2},
  pages        = {340--356},
  year         = {1995},
  url          = {https://doi.org/10.1137/S0097539791218202},
  doi          = {10.1137/S0097539791218202},
  timestamp    = {Sat, 27 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/siamcomp/KilpelainenM95.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The following tree-matching problem is considered: Given labeled trees P and T, can P be obtained from T by deleting nodes? Deleting a node u entails removing all edges incident to u and, if u has a parent v, replacing the edge from v to u by edges from v to the children of u. The problem is motivated by the study of query languages for structured text databases. Simple solutions to this problem require exponential time. For ordered trees an algorithm is presented that requires  time and space. The corresponding problem for unordered trees is also considered and a proof of its NP-completeness is given. An algorithm is presented for the unordered problem. This algorithm works in  time if the out-degrees of the nodes in P are bounded by a constant, and in polynomial time if they are O(log |T|).}
}

@article{DBLP:journals/tcs/KivinenM95,
  author       = {Jyrki Kivinen and
                  Heikki Mannila},
  title        = {Approximate Inference of Functional Dependencies from Relations},
  journal      = {Theor. Comput. Sci.},
  volume       = {149},
  number       = {1},
  pages        = {129--149},
  year         = {1995},
  url          = {https://doi.org/10.1016/0304-3975(95)00028-U},
  doi          = {10.1016/0304-3975(95)00028-U},
  timestamp    = {Mon, 28 Aug 2023 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/tcs/KivinenM95.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The functional dependency inference problem is the following. Given a relation r, find a set of functional dependencies that is equivalent with the set of all functional dependencies holding in r. All known algorithms for this task have running times that can be in the worst case exponential in the size of the smallest cover of the dependency set. We consider approximate dependency inference. We define various measures for the error of a dependency in a relation. These error measures have the value 0 if the dependency holds and a value close to 1 if the dependency clearly does not hold. Depending on the measure used, all dependencies with error at least ϵ in r can be detected with high probability by considering only O ( 1 ε ) or O (|r| 1 2 /ε ) random tuples of r. We also show how a machine learning algorithm due to Angluin et al. can be applied to give in output-polynomial time an approximately correct cover for the set of dependencies holding in r.}
}

@inproceedings{DBLP:conf/eurocolt/KilpelainenMU95,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila and
                  Esko Ukkonen},
  editor       = {Paul M. B. Vit{\'{a}}nyi},
  title        = {{MDL} learning of unions of simple pattern languages from positive examples},
  booktitle    = {Computational Learning Theory, Second European Conference, EuroCOLT
                  '95, Barcelona, Spain, March 13-15, 1995, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {904},
  pages        = {252--260},
  publisher    = {Springer},
  year         = {1995},
  url          = {https://doi.org/10.1007/3-540-59119-2\_182},
  doi          = {10.1007/3-540-59119-2\_182},
  timestamp    = {Tue, 14 May 2019 10:00:53 +0200},
  biburl       = {https://dblp.org/rec/conf/eurocolt/KilpelainenMU95.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The following learning task is considered: Given a set S of strings consisting of basic symbols and a set C of patterns consisting of basic symbols and variables, compute a concise set  such that each string in S is obtained from some pattern in C by substituting basic symbols for the variables. We apply Rissanen's MDL principle to the selection of patterns to the result. This leads to a length-minimization problem that we approximately solve in polynomial time (in the length of S and C) with a logarithmic performance guarantee. Our algorithm is based on a greedy solution of a variant of the set covering problem.}
}

@inproceedings{DBLP:conf/kdd/HolsheimerKMT95,
  author       = {Marcel Holsheimer and
                  Martin L. Kersten and
                  Heikki Mannila and
                  Hannu Toivonen},
  editor       = {Usama M. Fayyad and
                  Ramasamy Uthurusamy},
  title        = {A Perspective on Databases and Data Mining},
  booktitle    = {Proceedings of the First International Conference on Knowledge Discovery
                  and Data Mining (KDD-95), Montreal, Canada, August 20-21, 1995},
  pages        = {150--155},
  publisher    = {{AAAI} Press},
  year         = {1995},
  url          = {http://www.aaai.org/Library/KDD/1995/kdd95-017.php},
  timestamp    = {Wed, 12 Dec 2012 15:08:19 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/HolsheimerKMT95.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We discuss the use of database met hods for data mining. Recently impressive results have been achieved for some data mining problems using highly specialized and clever data structures. We study how well one can manage by using general purpose database management systems. We illustrate our ideas by investigating the use of a dbms for a well-researched area: the discovery of association rules. We present a simple algorithm, consisting of only union and intersection operations, and show that it achieves quite good performance on an efficient dbms. Our method can incorporate inheritance hierarchies to the association rule algorithm easily. We also present a technique that effectively reduces the number of database operations when searching large search spaces that contain only few interesting items. Our work shows that database techniques are promising for data mining: general architectures can achieve reasonable results.}
}

@inproceedings{DBLP:conf/kdd/MannilaTV95,
  author       = {Heikki Mannila and
                  Hannu Toivonen and
                  A. Inkeri Verkamo},
  editor       = {Usama M. Fayyad and
                  Ramasamy Uthurusamy},
  title        = {Discovering Frequent Episodes in Sequences},
  booktitle    = {Proceedings of the First International Conference on Knowledge Discovery
                  and Data Mining (KDD-95), Montreal, Canada, August 20-21, 1995},
  pages        = {210--215},
  publisher    = {{AAAI} Press},
  year         = {1995},
  url          = {http://www.aaai.org/Library/KDD/1995/kdd95-024.php},
  timestamp    = {Wed, 12 Dec 2012 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaTV95.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Sequences of events describing the behavior and actions of users or systems can be collected in several domains. In this paper we consider the problem of recognizing frequent episodes in such sequences of events. An episode is defined to be a collection of events that occur within time intervals of a given size in a given partial order. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We describe an efficient algorithm for the discovery of all frequent episodes from a given class of episodes, and present experimental results.}
}

@article{DBLP:journals/dke/MannilaR94,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {Algorithms for Inferring Functional Dependencies from Relations},
  journal      = {Data Knowl. Eng.},
  volume       = {12},
  number       = {1},
  pages        = {83--99},
  year         = {1994},
  url          = {https://doi.org/10.1016/0169-023X(94)90023-X},
  doi          = {10.1016/0169-023X(94)90023-X},
  timestamp    = {Sun, 02 Jun 2019 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/dke/MannilaR94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The dependency inference problem is to find a cover of the set of functional dependencies that hold in a given relation. The problem has applications in relational database design, in query optimization, and in artificial intelligence. The problem is exponential in the number of attributes. We develop two algorithms with better best case behavior than the simple one. One algorithm reduces the problem to computing the transversal of a hypergraph. The other is based on repeatedly sorting the relation with respect to a set of attributes.}
}

@inproceedings{DBLP:conf/cikm/KlemettinenMRTV94,
  author       = {Mika Klemettinen and
                  Heikki Mannila and
                  Pirjo Ronkainen and
                  Hannu Toivonen and
                  A. Inkeri Verkamo},
  title        = {Finding Interesting Rules from Large Sets of Discovered Association Rules},
  booktitle    = {Proceedings of the Third International Conference on Information and
                  Knowledge Management (CIKM'94), Gaithersburg, Maryland, USA, November
                  29 - December 2, 1994},
  pages        = {401--407},
  publisher    = {{ACM}},
  year         = {1994},
  url          = {https://doi.org/10.1145/191246.191314},
  doi          = {10.1145/191246.191314},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/cikm/KlemettinenMRTV94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Association rules, introduced by Agrawal, Imielinski, and Swami, are rules of the form "for 90% of the rows of the relation, if the row has value 1 in the columns in set W, then it has 1 also in column B". Efficient methods exist for discovering association rules from large collections of data. The number of discovered rules can, however, be so large that browsing the rule set and finding interesting rules from it can be quite difficult for the user. We show how a simple formalism of rule templates makes it possible to easily describe the structure of interesting rules. We also give examples of visualization of rules, and show how a visualization tool interfaces with rule templates.}
}

@inproceedings{DBLP:conf/cpm/KilpelainenM94,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  editor       = {Maxime Crochemore and
                  Dan Gusfield},
  title        = {Query Primitives for Tree-Structured Data},
  booktitle    = {Combinatorial Pattern Matching, 5th Annual Symposium, {CPM} 94, Asilomar,
                  California, USA, June 5-8, 1994, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {807},
  pages        = {213--225},
  publisher    = {Springer},
  year         = {1994},
  url          = {https://doi.org/10.1007/3-540-58094-8\_19},
  doi          = {10.1007/3-540-58094-8\_19},
  timestamp    = {Tue, 14 May 2019 10:00:38 +0200},
  biburl       = {https://dblp.org/rec/conf/cpm/KilpelainenM94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider primitives for retrieving information from trees. We define a sequence of tree matching operations based on a classification of properties preserved in matching. We analyze the time complexity of the primitives. The addition of logical variables to the primitives is also considered, and its effects on the complexities is studied.}
}

@inproceedings{DBLP:conf/ecml/KivinenMUV94,
  author       = {Jyrki Kivinen and
                  Heikki Mannila and
                  Esko Ukkonen and
                  Jaak Vilo},
  editor       = {Francesco Bergadano and
                  Luc De Raedt},
  title        = {An Algorithm for Learning Hierarchical Classifiers},
  booktitle    = {Machine Learning: ECML-94, European Conference on Machine Learning,
                  Catania, Italy, April 6-8, 1994, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {784},
  pages        = {375--378},
  publisher    = {Springer},
  year         = {1994},
  url          = {https://doi.org/10.1007/3-540-57868-4\_77},
  doi          = {10.1007/3-540-57868-4\_77},
  timestamp    = {Sun, 25 Oct 2020 23:05:12 +0100},
  biburl       = {https://dblp.org/rec/conf/ecml/KivinenMUV94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = { In [J. Kivinen, H. Mannila, and E. Ukkonen.] an Occam algorithm ([A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth]) was introduced for PAC learning certain kind of decision lists from classified examples. Such decision lists, or hierarchical rules as we call them, are of the form shown in Fig. 1. The purpose of the present paper is to discuss the practical implementation of the algorithm, to present a linguistic application (hyphenation of Finnish), and compare the learning result with an earlier experiment in which Angluin's k-reversible automata were used. if xEC1 or xEC2 or... or xEC,~ then~ rt else if xEC,~+ I or xEC~+ 2or... orxEC,~+ m then~ 2 else else if x E Cn+ m+...+ l or x E C~+,~+...+ 2 or... or x E Cp then ak}
}

@inproceedings{DBLP:conf/gi/EiterGM94,
  author       = {Thomas Eiter and
                  Georg Gottlob and
                  Heikki Mannila},
  editor       = {Bernd E. Wolfinger},
  title        = {Disjunctive Logic Programming over Finite Structures},
  booktitle    = {Innovationen bei Rechen- und Kommunikationssystemen, Eine Herausforderung
                  f{\"{u}}r die Informatik, 24. GI-Jahrestagung im Rahmen des 13th World
                  Computer Congress, {IFIP} Congress '94, Hamburg, 28. August - 2. September
                  1994},
  series       = {Informatik Aktuell},
  pages        = {69--73},
  publisher    = {Springer},
  year         = {1994},
  url          = {https://doi.org/10.1007/978-3-642-51136-3\_10},
  doi          = {10.1007/978-3-642-51136-3\_10},
  timestamp    = {Tue, 23 May 2017 01:10:34 +0200},
  biburl       = {https://dblp.org/rec/conf/gi/EiterGM94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Disjunctive logic programming gained increasing interest recently, and several semantics for disjunctive logic programs, i.e., programs with clauses that allow for a disjunction of atoms in the head, have been developed; many of them are extensions of well-known semantics of logic programming. Natural questions arising with such extensions are the following:
  1 What is the gain of allowing disjunction measured by the increase of expressive power, i.e., the class of queries over all collections of ground facts (i.e., relations) that can be implemented by a logic program. Besides, is such an increase necessary to solve relvant queries in practice?
  2. How is the computational complexity affected by allowing disjunction ?
  3. How do different semantics for disjunctive logic programs compare with respect to 1. and 2., and what is the effect of allowing other constructs like inequality ≠ and negation ¬ ?}
}

@inproceedings{DBLP:conf/icgi/AhonenMN94,
  author       = {Helena Ahonen and
                  Heikki Mannila and
                  Erja Nikunen},
  editor       = {Rafael C. Carrasco and
                  Jos{\'{e}} Oncina},
  title        = {Forming Grammars for Structured Documents: an Application of Grammatical Inference},
  booktitle    = {Grammatical Inference and Applications, Second International Colloquium,
                  ICGI-94, Alicante, Spain, September 21-23, 1994, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {862},
  pages        = {153--167},
  publisher    = {Springer},
  year         = {1994},
  url          = {https://doi.org/10.1007/3-540-58473-0\_145},
  doi          = {10.1007/3-540-58473-0\_145},
  timestamp    = {Tue, 14 May 2019 10:00:52 +0200},
  biburl       = {https://dblp.org/rec/conf/icgi/AhonenMN94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the problem of generating grammars for classes of structured documents — dictionaries, encyclopedias, user manuals, and so on — from examples. The examples consist of structures of individual documents, and they can be collected either by converting typographical tagging of documents prepared for printing into structural tags, or by using document recognition techniques. Our method forms first finite-state automata describing the examples completely. These automata are modified by considering certain context conditions; the modifications correspond to generalizing the underlying language. Finally, the automata are converted into regular expressions, and they are used to construct the grammar. In addition to automata, an alternative representation, characteristic k-grams, is introduced. Some interactive operations are also described that are necessary for generating a grammar for a large and complicated document.}
}

@inproceedings{DBLP:conf/iski/EiterGM94,
  author       = {Thomas Eiter and
                  Georg Gottlob and
                  Heikki Mannila},
  editor       = {Kai von Luck and
                  Heinz Marburger},
  title        = {Expressive Power and Complexity of Disjunctive Datalog under the Stable
                  Model Semantics},
  booktitle    = {Management and Processing of Complex Data Structures, Third Workshop
                  on Information Systems and Artificial Intelligence, Hamburg, Germany,
                  February 28 - March 2, 1994, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {777},
  pages        = {83--103},
  publisher    = {Springer},
  year         = {1994},
  url          = {https://doi.org/10.1007/3-540-57802-1\_5},
  doi          = {10.1007/3-540-57802-1\_5},
  timestamp    = {Sun, 12 Nov 2023 02:16:15 +0100},
  biburl       = {https://dblp.org/rec/conf/iski/EiterGM94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {DATALOG¬ is a well-known logical query language, whose expressive power and data complexity under the stable model semantics has been recently determined. In this paper we consider the extension of DATALOG¬ to disjunctive DATALOG¬ (DDL¬), which allows disjunction in the head of program clauses, under the stable model semantics. We investigate and determine the expressiveness and the data complexity of DDL¬, as well as the expression complexity. The main findings of this paper are that disjunctive datalog captures precisely the class of all ∑ 2 p -recognizable queries under the brave version of reasoning, and symmetrically the class of all Π 2 p -recognizable queries under the cautious version; the data complexity is ∑ 2 p -completeness in the brave version, and Π 2 p -complete in the cautious version, while the expression complexity is NEXPTIMENP-complete in the brave version and co-NEXPTIMENPcomplete in the cautious version.}
}

@inproceedings{DBLP:conf/kdd/MannilaTV94,
  author       = {Heikki Mannila and
                  Hannu Toivonen and
                  A. Inkeri Verkamo},
  editor       = {Usama M. Fayyad and
                  Ramasamy Uthurusamy},
  title        = {Efficient Algorithms for Discovering Association Rules},
  booktitle    = {Knowledge Discovery in Databases: Papers from the 1994 {AAAI} Workshop,
                  Seattle, Washington, USA, July 1994. Technical Report {WS-94-03}},
  pages        = {181--192},
  publisher    = {{AAAI} Press},
  year         = {1994},
  timestamp    = {Fri, 22 Jun 2018 07:33:24 +0200},
  biburl       = {https://dblp.org/rec/conf/kdd/MannilaTV94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Efficient Algorithms for Discovering Association Rules}
}

@inproceedings{DBLP:conf/pods/KivinenM94,
  author       = {Jyrki Kivinen and
                  Heikki Mannila},
  editor       = {Victor Vianu},
  title        = {The Power of Sampling in Knowledge Discovery},
  booktitle    = {Proceedings of the Thirteenth {ACM} {SIGACT-SIGMOD-SIGART} Symposium
                  on Principles of Database Systems, May 24-26, 1994, Minneapolis, Minnesota,
                  {USA}},
  pages        = {77--85},
  publisher    = {{ACM} Press},
  year         = {1994},
  url          = {https://doi.org/10.1145/182591.182601},
  doi          = {10.1145/182591.182601},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pods/KivinenM94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the problem of approximately verifying the truth of sentences of tuple relational calculus in a given relation M by considering only a random sample of M. We define two different measures for the error of a universal sentence in a relation. For a set of n universal sentences each with at most k universal quantifiers, we give upper and lower bounds for the sample sizes required for having a high probability that all the sentences with error at least ε can be detected as false by considering the sample. The sample sizes are O((log n)/ε) or O((|M|1–1/k)log n/ε), depending on the error measure used. We also consider universal-existential sentences.}
}

@inproceedings{DBLP:conf/pods/EiterGM94,
  author       = {Thomas Eiter and
                  Georg Gottlob and
                  Heikki Mannila},
  editor       = {Victor Vianu},
  title        = {Adding Disjunction to Datalog},
  booktitle    = {Proceedings of the Thirteenth {ACM} {SIGACT-SIGMOD-SIGART} Symposium
                  on Principles of Database Systems, May 24-26, 1994, Minneapolis, Minnesota,
                  {USA}},
  pages        = {267--278},
  publisher    = {{ACM} Press},
  year         = {1994},
  url          = {https://doi.org/10.1145/182591.182639},
  doi          = {10.1145/182591.182639},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pods/EiterGM94.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We study the expressive power and complexity of disjunctive datalog, i.e., datalog with disjunctive rule heads, under three different semantics: the minimal model semantics, the perfect models semantics, and the stable model semantics. We show that the brave variants of these semantics express the same set of queries. In fact, they precisely capture the complexity of class ΣP/2. The combined complexity of disjunctive datalog is shown to be NEXPTIMENP-complete.}
}

@article{DBLP:journals/dam/Estivill-CastroMW93,
  author       = {Vladimir Estivill{-}Castro and
                  Heikki Mannila and
                  Derick Wood},
  title        = {Right Invariant Metrics and Measures of Presortedness},
  journal      = {Discret. Appl. Math.},
  volume       = {42},
  number       = {1},
  pages        = {1--16},
  year         = {1993},
  url          = {https://doi.org/10.1016/0166-218X(93)90175-N},
  doi          = {10.1016/0166-218X(93)90175-N},
  timestamp    = {Thu, 11 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/dam/Estivill-CastroMW93.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Right invariant metrics (ri-metrics) have several applications in the theory of rank correlation methods. For example, ranking models based on ri-metrics generalize Mallow's ranking models. We explore the relationship between right invariant metrics and measures of presortedness (mops). The latter have been used to evaluate the behavior of sorting algorithms on nearly-sorted inputs. We give necessary and sufficient conditions for a measure of presortedness to be extended to a ri-metric; we characterize those ri-metrics that can be used as mops; and we show that those mops that are extendible to ri-metrics can be constructed from sets of sorting operations. Our results provide a paradigm for the construction of mops and ri-metrics.}
}

@inproceedings{DBLP:conf/eurocolt/KivinenMU93,
  author       = {Jyrki Kivinen and
                  Heikki Mannila and
                  Esko Ukkonen},
  editor       = {John Shawe{-}Taylor and
                  Martin Anthony},
  title        = {Learning rules with local exceptions},
  booktitle    = {Proceedings of the First European Conference on Computational Learning
                  Theory, EuroCOLT 1993, London, UK, December 20-22, 1993},
  pages        = {35--46},
  publisher    = {Oxford University Press},
  year         = {1993},
  timestamp    = {Tue, 10 Aug 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/conf/eurocolt/KivinenMU93.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Learning rules with local exceptions}
}

@inproceedings{DBLP:conf/sigir/KilpelainenM93,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  editor       = {Robert R. Korfhage and
                  Edie M. Rasmussen and
                  Peter Willett},
  title        = {Retrieval from Hierarchical Texts by Partial Patterns},
  booktitle    = {Proceedings of the 16th Annual International {ACM-SIGIR} Conference
                  on Research and Development in Information Retrieval. Pittsburgh,
                  PA, USA, June 27 - July 1, 1993},
  pages        = {214--222},
  publisher    = {{ACM}},
  year         = {1993},
  url          = {https://doi.org/10.1145/160688.160722},
  doi          = {10.1145/160688.160722},
  timestamp    = {Tue, 06 Dec 2022 21:19:59 +0100},
  biburl       = {https://dblp.org/rec/conf/sigir/KilpelainenM93.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Structured texts (for example dictionaries and user manuals) typically have a heirarchical (tree-like) structure. We describe a query language for retrieving information from collections of hierarchical text. The language is based on a tree pattern matching notion called tree inclusion. Tree inclusion allows easy expression of queries that use the structure and the content of the document. In using it a user need not be aware of the whole structure of the database. Thus a language based on tree inclusion is data independent, a property made necessary because of the great variance in the structure of the texts.}
}

@book{DBLP:books/aw/MannilaR92,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {Design of Relational Databases},
  publisher    = {Addison-Wesley},
  year         = {1992},
  isbn         = {0-201-56523-4},
  timestamp    = {Thu, 03 Jan 2002 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/books/aw/MannilaR92.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Design of Relational Databases}
}

@article{DBLP:journals/dam/MannilaR92,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {On the Complexity of Inferring Functional Dependencies},
  journal      = {Discret. Appl. Math.},
  volume       = {40},
  number       = {2},
  pages        = {237--243},
  year         = {1992},
  url          = {https://doi.org/10.1016/0166-218X(92)90031-5},
  doi          = {10.1016/0166-218X(92)90031-5},
  timestamp    = {Thu, 11 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/dam/MannilaR92.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The dependency inference problem is to find a cover for the set of functional dependencies that hold in a given relation. The problem has applications in relational database design and in query optimization. We show that this problem is solvable by a brute-force algorithm in Θ(n22np log p) time for a relation with p rows and n attributes. We show that for fixed n, time Ω(p log p) is a lower bound. We also show that the exponentially of the time bound with respect to n is unavoidable. We prove this by showing that there are small relations where an exponential number of nontrivial dependencies hold. We also prove two exponential lower bounds that hold even for the case where no explicit representation of the dependency set is needed.}
}

@article{DBLP:journals/ijis/KantolaMRS92,
  author       = {Martti Kantola and
                  Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}} and
                  Harri Siirtola},
  title        = {Discovering functional and inclusion dependencies in relational databases},
  journal      = {Int. J. Intell. Syst.},
  volume       = {7},
  number       = {7},
  pages        = {591--607},
  year         = {1992},
  url          = {https://doi.org/10.1002/int.4550070703},
  doi          = {10.1002/INT.4550070703},
  timestamp    = {Mon, 28 Aug 2023 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ijis/KantolaMRS92.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the problem of discovering the functional and inclusion dependencies that a given database instance satisfies. This technique is used in a database design tool that uses example databases to give feedback to the designer. If the examples show deficiencies in the design, the designer can directly modify the examples. the tool then infers new dependencies and the database schema can be modified, if necessary. the discovery of the functional and inclusion dependencies can also be used in analyzing an existing database. the problem of inferring functional dependencies has several connections to other topics in knowledge discovery and machine learning. In this article we discuss the use of examples in the design of databases, and give an overview of the complexity results and algorithms that have been developed for this problem.}
}

@inproceedings{DBLP:conf/colt/KivinenMU92,
  author       = {Jyrki Kivinen and
                  Heikki Mannila and
                  Esko Ukkonen},
  editor       = {David Haussler},
  title        = {Learning Hierarchical Rule Sets},
  booktitle    = {Proceedings of the Fifth Annual {ACM} Conference on Computational
                  Learning Theory, {COLT} 1992, Pittsburgh, PA, USA, July 27-29, 1992},
  pages        = {37--44},
  publisher    = {{ACM}},
  year         = {1992},
  url          = {https://doi.org/10.1145/130385.130389},
  doi          = {10.1145/130385.130389},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/colt/KivinenMU92.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We present an algorithm for learning sets of rules that are organized into up to k levels. Each level can contain an arbitrary number of rules “if c then l” where l is the class associated to the level and c is a concept from a given class of basic concepts. The rules of higher levels have precedence over the rules of lower levels and can be used to represent exceptions. As basic concepts we can use Boolean attributes in the infinite attribute space model, or certain concepts defined in terms of substrings. Given a sample of m examples, the algorithm runs in polynomial time and produces a consistent representation of size O((log m)knk), where n is the size of the smallest consistent representation with k levels of rules. This implies that the algorithm learns in the PAC model. The algorithm repeatedly applies the greedy heuristics for weighted set cover. The weights are obtained from approximate solutions to previous set cover problems.}
}

@inproceedings{DBLP:conf/cpm/KilpelainenM92,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  editor       = {Alberto Apostolico and
                  Maxime Crochemore and
                  Zvi Galil and
                  Udi Manber},
  title        = {Grammatical Tree Matching},
  booktitle    = {Combinatorial Pattern Matching, Third Annual Symposium, {CPM} 92,
                  Tucson, Arizona, USA, April 29 - May 1, 1992, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {644},
  pages        = {162--174},
  publisher    = {Springer},
  year         = {1992},
  url          = {https://doi.org/10.1007/3-540-56024-6\_13},
  doi          = {10.1007/3-540-56024-6\_13},
  timestamp    = {Tue, 14 May 2019 10:00:38 +0200},
  biburl       = {https://dblp.org/rec/conf/cpm/KilpelainenM92.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In structured text databases documents are represented as parse trees, and different tree matching notions can be used as primitives for query languages. Two useful notions of tree matching, tree inclusion and tree pattern matching both seem to require superlinear time. In this paper we give a general sufficient condition for a tree matching problem to be solvable in linear time, and apply it to tree pattern matching and tree inclusion. The application is based on the notion of a nonperiodic parse tree. We argue that most text documents can be modeled in a natural way using grammars yielding nonperiodic parse trees. We show how the knowledge that the target tree is nonperiodic can be used to obtain linear time algorithms for the tree matching problems. We also discuss the preprocessing of patterns for grammatical tree matching.}
}

@inproceedings{DBLP:conf/icdt/KivinenM92,
  author       = {Jyrki Kivinen and
                  Heikki Mannila},
  editor       = {Joachim Biskup and
                  Richard Hull},
  title        = {Approximate Dependency Inference from Relations},
  booktitle    = {Database Theory - ICDT'92, 4th International Conference, Berlin, Germany,
                  October 14-16, 1992, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {646},
  pages        = {86--98},
  publisher    = {Springer},
  year         = {1992},
  url          = {https://doi.org/10.1007/3-540-56039-4\_34},
  doi          = {10.1007/3-540-56039-4\_34},
  timestamp    = {Tue, 14 May 2019 10:00:54 +0200},
  biburl       = {https://dblp.org/rec/conf/icdt/KivinenM92.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The functional dependency inference problem consists of finding a cover for the set dep(r) of functional dependencies that hold in a given relation r. All known algorithms for this task have running times that can be in the worst case exponential in the size of the smallest cover of the dependency set. We consider approximate dependency inference. We define various measures for the error of a dependency in a relation. These error measures have the value 0 if the dependency holds and a value close to 1 if the dependency clearly does not hold. Depending on the measure used, all dependencies with error at least ɛ in r can be detected with high probability by considering only O(1/ɛ) or O(¦r¦1/2/ɛ) random tuples of r. We also show how a machine learning algorithm due to Angluin, Frazier and Pitt can be applied to give in output-polynomial time an approximately correct cover for the set of dependencies holding in r.}
}

@inproceedings{DBLP:conf/tapsoft/KilpelainenM91,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  editor       = {Samson Abramsky and
                  T. S. E. Maibaum},
  title        = {The Tree Inclusion Problem},
  booktitle    = {TAPSOFT'91: Proceedings of the International Joint Conference on Theory
                  and Practice of Software Development, Brighton, UK, April 8-12, 1991,
                  Volume 1: Colloquium on Trees in Algebra and Programming (CAAP'91)},
  series       = {Lecture Notes in Computer Science},
  volume       = {493},
  pages        = {202--214},
  publisher    = {Springer},
  year         = {1991},
  url          = {https://doi.org/10.1007/3-540-53982-4\_12},
  doi          = {10.1007/3-540-53982-4\_12},
  timestamp    = {Tue, 14 May 2019 10:00:50 +0200},
  biburl       = {https://dblp.org/rec/conf/tapsoft/KilpelainenM91.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We consider the following problem: Given ordered albeled trees S and T, can S be obtained from T by deleting nodes? Deletion of the root node u of a subtree with children 〈T 1, ..., T n 〉 means replacing the subtree by the trees T 1, ..., T n . The problem is motivated by the study of query languages for structured text data bases. The simple solutions to this problem require exponential time. We give an algorithm based on dynamic programming requiring O(|S||T|) time and space.}
}

@article{DBLP:journals/actaC/KilpelainenM90,
  author       = {Pekka Kilpel{\"{a}}inen and
                  Heikki Mannila},
  title        = {Generation of test cases for simple prolog programs},
  journal      = {Acta Cybern.},
  volume       = {9},
  number       = {3},
  pages        = {235--246},
  year         = {1990},
  url          = {https://cyber.bibl.u-szeged.hu/index.php/actcybern/article/view/3370},
  timestamp    = {Wed, 16 Sep 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/actaC/KilpelainenM90.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {}
}

@article{DBLP:journals/bit/MannilaU90,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  title        = {Unifications, Deunifications, and Their Complexity},
  journal      = {{BIT}},
  volume       = {30},
  number       = {4},
  pages        = {599--619},
  year         = {1990},
  url          = {https://doi.org/10.1007/BF01933209},
  doi          = {10.1007/BF01933209},
  timestamp    = {Tue, 22 Jun 2021 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/bit/MannilaU90.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {We describe a general method for producing complete sets of test data for Prolog programs. The method is based on the classical competent programmer hypothesis from the theory of testing, which states that the program written by the programmer differs only slightly from the correct one. The nearness is expressed by postulating a class of possible errors, and by assuming that the written program contains only errors from this class. Under this assumption the test cases produced by the method are enough to ensure the correctness of the program. The method is based on a result showing that it is sufficient to consider programs from which the written one differs by a single error. Test cases are produced by forming a path condition consisting of equations and universally quantified inequations, and solving the condition. The method is particularly easy to implement for the class of iterative programs; for general programs it can be used as a component of an interactive tool.}
}

@article{DBLP:journals/jcss/MannilaR89,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {Automatic Generation of Test Data for Relational Queries},
  journal      = {J. Comput. Syst. Sci.},
  volume       = {38},
  number       = {2},
  pages        = {240--258},
  year         = {1989},
  url          = {https://doi.org/10.1016/0022-0000(89)90002-0},
  doi          = {10.1016/0022-0000(89)90002-0},
  timestamp    = {Tue, 16 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/jcss/MannilaR89.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Automatic techniques for generating a test fatabase for a given query are studied. The methods can be applied in the testing of queries. We define the concept of a complete test database, characterize the notion using Armstrong databases, and give two constructions for producing complete test databases. We also give a method for generating databases that illustrate the effect of each operation appearing in the query.}
}

@inproceedings{DBLP:conf/pods/MannilaR89,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  editor       = {Avi Silberschatz},
  title        = {Practical Algorithms for Finding Prime Attributes and Testing Normal Forms},
  booktitle    = {Proceedings of the Eighth {ACM} {SIGACT-SIGMOD-SIGART} Symposium on
                  Principles of Database Systems, March 29-31, 1989, Philadelphia, Pennsylvania,
                  {USA}},
  pages        = {128--133},
  publisher    = {{ACM} Press},
  year         = {1989},
  url          = {https://doi.org/10.1145/73721.73734},
  doi          = {10.1145/73721.73734},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pods/MannilaR89.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Several decision problems for relational schemas with functional dependencies are computationally hard. Such problems include determining whether an attribute is prime and testing if a schema is in normal form. Algorithms for these problems are needed in database design tools. The problems can be solved by trivial exponential algorithms. Although the size of the instance is usually given by the number of attributes and hence is fairly small, such exponential algorithms are not usable for all design tasks. We give algorithms for these problems whose running time is polynomial in the number of maximal sets not determining an attribute or, equivalently, the number of generators of the family of closed attribute sets. There is theoretical and practical evidence that this quantity is small for the schemas occurring in practice and exponential only for pathological schemas. The algorithms are simple to implement and fast in practice. They are in use in the relational database design tool Design-By-Example.}
}

@inproceedings{DBLP:conf/swat/MannilaU88,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  editor       = {Rolf G. Karlsson and
                  Andrzej Lingas},
  title        = {Time Parameter and Arbitrary Deunions in the Set Union Problem},
  booktitle    = {{SWAT} 88, 1st Scandinavian Workshop on Algorithm Theory, Halmstad,
                  Sweden, July 5-8, 1988, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {318},
  pages        = {34--42},
  publisher    = {Springer},
  year         = {1988},
  url          = {https://doi.org/10.1007/3-540-19487-8\_4},
  doi          = {10.1007/3-540-19487-8\_4},
  timestamp    = {Tue, 14 May 2019 10:00:39 +0200},
  biburl       = {https://dblp.org/rec/conf/swat/MannilaU88.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The classical set union problem is to manipulate a partition of {1,2,...,n} under the operations find and union. We study two variants of this problem. In the first variant the find operations contain a time parameter. We show that this extended problem can be solved and requires ϑ(nlogn) time for n operations on separable pointer machines. In the second variant find operations are the usual ones, but an arbitrary union operation can be cancelled by the deunion operation. The same result holds for this variant. These problems are motivated by questions arising in the tracing of Prolog executions and in the incremental execution of logic programs.}
}

@inproceedings{DBLP:conf/slp/MannilaU87,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  title        = {Flow Analysis of Prolog Programs},
  booktitle    = {Proceedings of the 1987 Symposium on Logic Programming, San Francisco,
                  California, USA, August 31 - September 4, 1987},
  pages        = {205--214},
  publisher    = {{IEEE-CS}},
  year         = {1987},
  timestamp    = {Wed, 04 Dec 2013 14:42:58 +0100},
  biburl       = {https://dblp.org/rec/conf/slp/MannilaU87.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Flow Analysis of Prolog Programs}
}

@inproceedings{DBLP:conf/vldb/MannilaR87,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  editor       = {Peter M. Stocker and
                  William Kent and
                  Peter Hammersley},
  title        = {Dependency Inference},
  booktitle    = {VLDB'87, Proceedings of 13th International Conference on Very Large
                  Data Bases, September 1-4, 1987, Brighton, England},
  pages        = {155--158},
  publisher    = {Morgan Kaufmann},
  year         = {1987},
  url          = {http://www.vldb.org/conf/1987/P155.PDF},
  timestamp    = {Wed, 29 Mar 2017 16:45:24 +0200},
  biburl       = {https://dblp.org/rec/conf/vldb/MannilaR87.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Dependency Inference}
}

@article{DBLP:journals/jcss/MannilaR86,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {Design by Example: An Application of Armstrong Relations},
  journal      = {J. Comput. Syst. Sci.},
  volume       = {33},
  number       = {2},
  pages        = {126--141},
  year         = {1986},
  url          = {https://doi.org/10.1016/0022-0000(86)90015-2},
  doi          = {10.1016/0022-0000(86)90015-2},
  timestamp    = {Tue, 16 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/jcss/MannilaR86.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Example relations, and especially Armstrong relations, can be used as user friendly representations of dependency sets. In this paper we analyze the use of Armstrong relations in database design with functional dependencies, and show how they and the usual representation of dependencies can be used together. Special attention is given to the size of Armstrong relations. We derive new bounds for the size of minimal Armstrong relations for normalized schemes. Specifically, any relation scheme in Boyce-Codd Normal Form has an Armstrong relation whose size is roughly the product of the lenghts of the keys for the scheme. New algorithms are also given for generating Armstrong relations and for inferring the functional dependencies holding in a relation.}
}

@inproceedings{DBLP:conf/icalp/MannilaU86,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  editor       = {Laurent Kott},
  title        = {The Set Union Problem with Backtracking},
  booktitle    = {Automata, Languages and Programming, 13th International Colloquium,
                  ICALP86, Rennes, France, July 15-19, 1986, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {226},
  pages        = {236--243},
  publisher    = {Springer},
  year         = {1986},
  url          = {https://doi.org/10.1007/3-540-16761-7\_73},
  doi          = {10.1007/3-540-16761-7\_73},
  timestamp    = {Tue, 14 May 2019 10:00:44 +0200},
  biburl       = {https://dblp.org/rec/conf/icalp/MannilaU86.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The classical set union problem is to manipulate a partition of U = {1,2,...,n} under the operations find and union. We study a variant of this problem, where backtracking over the union operations is possible with the deunion operation. An implementation is developed to on-line perform an intermixed sequence of m finds, k unions and at most k deunions in O((m+k) log log n) steps. An O(k + m log n) method is also given. The problem considered is motivated by questions arising in the implementation of the programming language Prolog.}
}

@inproceedings{DBLP:conf/icde/MannilaR86,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {Inclusion Dependencies in Database Design},
  booktitle    = {Proceedings of the Second International Conference on Data Engineering,
                  February 5-7, 1986, Los Angeles, California, {USA}},
  pages        = {713--718},
  publisher    = {{IEEE} Computer Society},
  year         = {1986},
  url          = {https://doi.org/10.1109/ICDE.1986.7266283},
  doi          = {10.1109/ICDE.1986.7266283},
  timestamp    = {Fri, 24 Mar 2023 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/icde/MannilaR86.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A design methodology for relational databases is developed. The main aspects of the methodology are the following. (a) It uses functional dependencies and inclusion dependencies. The latter are essential to model properly e.g. isa-relationships. (b) It is incremental: the database scheme evolves step by step as new information is considered. (c) It is based on an interaction between the designer and a tool that implements the methodology. The basis of the methodology is a normal form for schemes with functional dependencies and inclusion dependencies. Transformations for incrementally changing a scheme into normal form are given.}
}

@inproceedings{DBLP:conf/iclp/MannilaU86,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  editor       = {Ehud Shapiro},
  title        = {On the Complexity of Unification Sequences},
  booktitle    = {Third International Conference on Logic Programming, Imperial College
                  of Science and Technology, London, United Kingdom, July 14-18, 1986,
                  Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {225},
  pages        = {122--133},
  publisher    = {Springer},
  year         = {1986},
  url          = {https://doi.org/10.1007/3-540-16492-8\_69},
  doi          = {10.1007/3-540-16492-8\_69},
  timestamp    = {Tue, 14 May 2019 10:00:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclp/MannilaU86.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The execution of a Prolog program can be viewed as a sequence of unifications and backtracks over unifications. We study the time requirement of executing a sequence of such operations (the unify-deunify problem). It is shown that the well-known set union problem is reducible to this problem. As the set union problem requires nonlinear time on large class of algorithms, the same holds for the unify-deunify problem. Thus the linearity of single unifications does not give a complete picture of the time complexity of Prolog primitives. We discuss the methods for executing sequences of unifications used in Prolog interpreters and show that many of them require even quadratic time in the worst case. We also outline some theoretically better methods.}
}

@inproceedings{DBLP:conf/pods/MannilaR86,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  editor       = {Avi Silberschatz},
  title        = {Test Data for Relational Queries},
  booktitle    = {Proceedings of the Fifth {ACM} {SIGACT-SIGMOD} Symposium on Principles
                  of Database Systems, March 24-26, 1986, Cambridge, Massachusetts,
                  {USA}},
  pages        = {217--223},
  publisher    = {{ACM}},
  year         = {1986},
  url          = {https://doi.org/10.1145/6012.15415},
  doi          = {10.1145/6012.15415},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pods/MannilaR86.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {An automatic technique for generating a comprehensive test database for a given query is studied. The test database is large enough to cover all essentially different situations under the given set of dependencies, and also large enough to illustrate the effect of each operation appearing in the query. On the other hand, the database attempts to do this in a minimal way. The method can be applied in the testing of queries, e.g. as an aid in learning a new query language. The basis of the construction is the definition of an adequate test case. We characterize this concept using Armstrong relations and show that adequate examples have the desired properties. We also give a method for producing reasonably small example databases for select-project-join queries where each relation scheme appears at most once in the query.}
}

@inproceedings{DBLP:conf/slp/MannilaU86,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  title        = {Timestamped Term Representation for Implementing Prolog},
  booktitle    = {Proceedings of the 1986 Symposium on Logic Programming, Salt Lake
                  City, Utah, USA, September 22-25, 1986},
  pages        = {159--165},
  publisher    = {{IEEE-CS}},
  year         = {1986},
  timestamp    = {Wed, 04 Dec 2013 14:43:00 +0100},
  biburl       = {https://dblp.org/rec/conf/slp/MannilaU86.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Timestamped Term Representation for Implementing Prolog}
}

@article{DBLP:journals/ipl/MannilaM85,
  author       = {Heikki Mannila and
                  Kurt Mehlhorn},
  title        = {A Fast Algorithm for Renaming a Set of Clauses as a Horn Set},
  journal      = {Inf. Process. Lett.},
  volume       = {21},
  number       = {5},
  pages        = {269--272},
  year         = {1985},
  url          = {https://doi.org/10.1016/0020-0190(85)90096-1},
  doi          = {10.1016/0020-0190(85)90096-1},
  timestamp    = {Fri, 26 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ipl/MannilaM85.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A set of clauses is a Horn set if each clause contains at most one positive literal. Lewis (1978) has given a polynomial-time algorithm for testing whether a set of clauses can be renamed as a Horn set. His algorithm uses in the worst case O(c·v2) time, where c is the number of clauses and v the number of variables. We give an algorithm working in O(c·v·(log v)2) time. The algorithm is based on an efficient depth-first search on a dense graph with a short description.}
}

@article{DBLP:journals/tc/Mannila85,
  author       = {Heikki Mannila},
  title        = {Measures of Presortedness and Optimal Sorting Algorithms},
  journal      = {{IEEE} Trans. Computers},
  volume       = {34},
  number       = {4},
  pages        = {318--325},
  year         = {1985},
  url          = {https://doi.org/10.1109/TC.1985.5009382},
  doi          = {10.1109/TC.1985.5009382},
  timestamp    = {Sat, 20 May 2017 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/tc/Mannila85.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The concept of presortedness and its use in sorting are studied. Natural ways to measure presortedness are given and some general properties necessary for a measure are proposed. A concept of a sorting algorithm optimal with respect to a measure of presortedness is defined, and examples of such algorithms are given. An insertion sort is shown to be optimal with respect to three natural measures. The problem of finding an optimal algorithm for an arbitrary measure is studied and partial results are proven.}
}

@article{DBLP:journals/tcs/BackM85,
  author       = {Ralph{-}Johan Back and
                  Heikki Mannila},
  title        = {On the Suitability of Trace Semantics for Modular Proofs of Communicating Processes},
  journal      = {Theor. Comput. Sci.},
  volume       = {39},
  pages        = {47--68},
  year         = {1985},
  url          = {https://doi.org/10.1016/0304-3975(85)90130-6},
  doi          = {10.1016/0304-3975(85)90130-6},
  timestamp    = {Wed, 17 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/tcs/BackM85.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The question of whether a semantic model is suitable for the construction of a modular proof system is studied in detail. The notion of one semantic model being a (full) abstraction of another semantic model with respect to a given class of properties is introduced, and is used in analyzing different semantic models for communicating processes. A trace model for communicating processes is described and shown to be suitable for the construction of a modular proof system in which partial correctness assertions about communicating processes can be expressed.}
}

@inproceedings{DBLP:conf/pods/MannilaR85,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {Small Armstrong Relations for Database Design},
  booktitle    = {Proceedings of the Fourth {ACM} {SIGACT-SIGMOD} Symposium on Principles
                  of Database Systems, March 25-27, 1985, Portland, Oregon, {USA}},
  pages        = {245--250},
  publisher    = {{ACM}},
  year         = {1985},
  url          = {https://doi.org/10.1145/325405.325449},
  doi          = {10.1145/325405.325449},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/pods/MannilaR85.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Example relations, and especially Armstrong relations, can be used as user-friendly representations of dependency sets. In this paper we analyze the use of Armstrong relations in database design with functional dependencies, and show how they and the usual representation of dependencies can be used together. Special attention is given to the siae of Armstrong relations. We derive new bounds for the size of minimal Armstrong relations for normalized schemes. New alp rithms are also given for generating Armstrong relationa and for inferring the functional dependencies holding in a relation.}
}

@article{DBLP:journals/iandc/BackM84,
  author       = {Ralph{-}Johan Back and
                  Heikki Mannila},
  title        = {A Semantic Approach to Program Modularity},
  journal      = {Inf. Control.},
  volume       = {60},
  number       = {1-3},
  pages        = {138--167},
  year         = {1984},
  url          = {https://doi.org/10.1016/S0019-9958(84)80026-1},
  doi          = {10.1016/S0019-9958(84)80026-1},
  timestamp    = {Fri, 12 Feb 2021 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/iandc/BackM84.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Modularity in programs is studied from a semantic point of view. A simple model of modular systems and modularization mechanisms is presented, together with correctness criteria for modular systems. A concept of locality is defined for modular systems and modularization mechanisms. In a local modular system the correctness of each module can be established by only looking at the module specifications, i.e., without using any information about how the modules are implemented. Locality is thus a basic property of modular systems and justifies the use of Parnas' information hiding principle in the construction of modular systems. A characterization of locality is given, and the locality of hierarchical and recursive modular systems is studied. Hierarchical modular systems are shown always to be local, while recursive systems need not be local. A sufficient condition for the locality of recursive modular systems is given. Finally, the composition of modular systems to yield higher level modular systems is described and analyzed. It is shown that locality of modular systems is preserved in a hierarchical composition of modular systems. Correctness of hierarchically composed modular systems is analyzed and sufficient conditions for establishing it are given.}
}

@article{DBLP:journals/ipl/MannilaU84,
  author       = {Heikki Mannila and
                  Esko Ukkonen},
  title        = {A Simple Linear-Time Algorithm for in Situ Merging},
  journal      = {Inf. Process. Lett.},
  volume       = {18},
  number       = {4},
  pages        = {203--208},
  year         = {1984},
  url          = {https://doi.org/10.1016/0020-0190(84)90112-1},
  doi          = {10.1016/0020-0190(84)90112-1},
  timestamp    = {Sun, 02 Jun 2019 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/ipl/MannilaU84.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {A simple algorithm for merging two ordered lists in bounded workspace is given. The algorithm is developed by first finding an intuitively clear divide-and-conquer algorithm with sublinear workspace, and then removing the workspace.}
}

@article{DBLP:journals/acta/MannilaR83,
  author       = {Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  title        = {On the Relationship of Minimum and Optimum Covers for a Set of Functional Dependencies},
  journal      = {Acta Informatica},
  volume       = {20},
  pages        = {143--158},
  year         = {1983},
  url          = {https://doi.org/10.1007/BF00289412},
  doi          = {10.1007/BF00289412},
  timestamp    = {Sun, 21 Jun 2020 01:00:00 +0200},
  biburl       = {https://dblp.org/rec/journals/acta/MannilaR83.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Most algorithms in relational database theory use a set of functional dependencies as their input. The efficiency of the algorithms depends on the size of the set. The notions of a minimum set (with as few dependencies as possible) and an optimum set (which is as short as possible) were introduced by Maier. He showed that while a minimum cover for a given set of dependencies can be found in polynomial time, obtaining an optimum cover is an NP-complete problem. Here the relationship of these covers is explored further. It is shown that the length of a minimum set (i) cannot be bounded by a linear function on the length of an optimum cover, and (ii) is bounded by the square of the length of an optimum cover. It is also shown that the NP-completeness of the optimization problem is somewhat surprisingly caused solely by the difficulty of optimizing a single class of dependencies having equivalent left sides, not by the globality of the optimality condition. This result has some practical significance, since the equivalence classes appearing in practice are short. The problem of optimizing an equivalence class is studied and left and right sides of a dependency are shown to behave differently. A new representation for equivalence classes based on this observation is suggested. The optimization of single dependencies is shown to be NP-complete, and a method to produce good approximations is given.}
}

@article{DBLP:journals/apal/Mannila83,
  author       = {Heikki Mannila},
  title        = {A topological characterization of ({\(\lambda\)}, {\(\mu\)})\({}^{\mbox{*}}\)-compactness},
  journal      = {Ann. Pure Appl. Log.},
  volume       = {25},
  number       = {3},
  pages        = {301--305},
  year         = {1983},
  url          = {https://doi.org/10.1016/0168-0072(83)90022-2},
  doi          = {10.1016/0168-0072(83)90022-2},
  timestamp    = {Fri, 21 Feb 2020 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/journals/apal/Mannila83.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Makowsky and Shelah have defined the concept of (λ, μ)*-compactness of a logic. This notion is shown to be equivalent to the topological (λ, μ)-compactness of the model spaces of the logic. Combining this characterization with a theorem of Alexandroff and Urysohn gives a result about (λ, μ)*-compactness as a corollary.}
}

@inproceedings{DBLP:conf/popl/BackMR83,
  author       = {Ralph{-}Johan Back and
                  Heikki Mannila and
                  Kari{-}Jouko R{\"{a}}ih{\"{a}}},
  editor       = {John R. Wright and
                  Larry Landweber and
                  Alan J. Demers and
                  Tim Teitelbaum},
  title        = {Derivation of Efficient {DAG} Marking Algorithms},
  booktitle    = {Conference Record of the Tenth Annual {ACM} Symposium on Principles
                  of Programming Languages, Austin, Texas, USA, January 1983},
  pages        = {20--27},
  publisher    = {{ACM} Press},
  year         = {1983},
  url          = {https://doi.org/10.1145/567067.567071},
  doi          = {10.1145/567067.567071},
  timestamp    = {Tue, 06 Nov 2018 00:00:00 +0100},
  biburl       = {https://dblp.org/rec/conf/popl/BackMR83.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {The best known linear-time list marking algorithms also require a linear amount of workspace. Algorithms working in bounded workspace have been obtained only by allowing quadratic execution time or by restricting the list structures to trees. We improve on this here by deriving a new linear-time, bounded workspace marking algorithm that works for dags. The algorithm is derived using correctness-preserving program transformations, which prove the correctness of the algorithm. Our derivation of the marking algorithm provides an example where this method has actually been used to derive a new, more efficient algorithm, rather than just to establish the correctness of a previously known algorithm.}
}

@inproceedings{DBLP:conf/icalp/BackM82,
  author       = {Ralph{-}Johan Back and
                  Heikki Mannila},
  editor       = {Mogens Nielsen and
                  Erik Meineche Schmidt},
  title        = {Locality in Modular Systems},
  booktitle    = {Automata, Languages and Programming, 9th Colloquium, Aarhus, Denmark,
                  July 12-16, 1982, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {140},
  pages        = {1--13},
  publisher    = {Springer},
  year         = {1982},
  url          = {https://doi.org/10.1007/BFb0012752},
  doi          = {10.1007/BFB0012752},
  timestamp    = {Tue, 14 May 2019 10:00:44 +0200},
  biburl       = {https://dblp.org/rec/conf/icalp/BackM82.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Modularity of programs is studied from a semantic point of view. A simple model of modular systems and modularization mechanisms is presented, together with correctness criteria for modular systems. A concept of locality of modular systems is defined; it is a property which “good” modular decompositions should have. The locality of certain kinds of modularization mechanisms is studied, and the results are applied to parameterless procedures.}
}

@inproceedings{DBLP:conf/podc/BackM82,
  author       = {Ralph{-}Johan Back and
                  Heikki Mannila},
  editor       = {Robert L. Probert and
                  Michael J. Fischer and
                  Nicola Santoro},
  title        = {A Refinement of Kahn's Semantic to Handle Non-Determinism and Communication},
  booktitle    = {{ACM} {SIGACT-SIGOPS} Symposium on Principles of Distributed Computing,
                  Ottawa, Canada, August 18-20, 1982},
  pages        = {111--120},
  publisher    = {{ACM}},
  year         = {1982},
  url          = {https://doi.org/10.1145/800220.806688},
  doi          = {10.1145/800220.806688},
  timestamp    = {Tue, 30 Jul 2024 10:34:08 +0200},
  biburl       = {https://dblp.org/rec/conf/podc/BackM82.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {Systems of processes connected together by communication channels are studied semantically. A model for such systems, based on traces of communication events, is described. This semantic model is more general than the stream function model described by Kahn, in that it permits processes to have a nondeterministic behaviour. The relationship between these two semantics is discussed. It is shown that the merge anomaly of Brock and Ackermann does not arise in the trace semantics.}
}

